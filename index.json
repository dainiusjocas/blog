[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.jocas.lt/blog/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blog/authors/admin/","section":"authors","summary":"","tags":null,"title":"Dainius Jocas","type":"authors"},{"authors":["Dainius Jocas"],"categories":["AWS Lambda","babashka","Clojure","GraalVM"],"content":"TL;DR\nI've managed to package a simple babashka script to an AWS Lambda Custom Runtime. Here is the code, try for yourself.\nMotivation Wouldn't it be great to deploy little Clojure code snippets to Custom Lambda Runtime? The main benefits would be:\n you would not suffer from java cold-start problems; you wouldn't need to compile your project with GraalVM native-image tool which is time consuming and for anything more advanced is not likely to work anyway; babashka supports scripting with a subset of Clojure, which might do the work for you.  The plan I know what it takes to deploy to Lambda Custom Runtime. Last year I've created a Clojure project template for deploying GraalVM compiled AWS Lambda Custom Runtime. And babashka is just another self contained binary. It should be too hard to bring two things together and get it working? Challenge accepted.\nPackaging I like to build software inside Docker containers. In this experiment, for the first attempt I've used this Dockerfile:\nFROM borkdude/babashka:latest as BABASHKA FROM clojure:tools-deps-alpine as BUILDER RUN apk add --no-cache zip WORKDIR /var/task COPY --from=BABASHKA /usr/local/bin/bb bb ENV GITLIBS=\u0026quot;.gitlibs/\u0026quot; COPY lambda/bootstrap bootstrap COPY deps.edn deps.edn RUN clojure -Sdeps '{:mvn/local-repo \u0026quot;./.m2/repository\u0026quot;}' -Spath \u0026gt; cp COPY src/ src/ COPY resources/ resources/ RUN zip -q -r function.zip bb cp bootstrap .gitlibs/ .m2/ src/ resources/ deps.edn  Here:\n copy bb binary from babashka Docker image, download the dependencies for babashka script using clojure (both, maven and git dependencies are supported, like is described here), write a classpath to the cp file, copy all source code, zip the required contents to the function.zip.  Every line of this dockerfile is packed with details but I'll leave it for the future posts.\nI've packaged all dependencies for lambda into function.zip. The contents of the archive are:\n bb: babashka binary bootstrap: AWS Lambda entry point script cp: generated classpath text file deps.edn .gitlibs: directory with gitlibs .m2: directory with Maven dependencies resources: src: directory with babashka scripts  Custom runtime discoveries Finally, having all dependencies packaged up, I've deployed the function.zip to AWS Lambda. The first error message was not very encouraging:\nUtil_sun_misc_Signal.ensureInitialized: CSunMiscSignal.create() failed. errno: 38 Function not implemented Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed. JavaFrameAnchor dump: No anchors TopFrame info: TotalFrameSize in CodeInfoTable 32 VMThreads info: VMThread 0000000003042750 STATUS_IN_JAVA (safepoints disabled) java.lang.Thread@0x264fa98 VM Thread State for current thread 0000000003042750: 0 (8 bytes): com.oracle.svm.jni.JNIThreadLocalEnvironment.jniFunctions = (bytes) 0000000003042750: 0000000002293a88 8 (32 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.regularTLAB = (bytes) 0000000003042758: 00007f7809500000 00007f7809600000 0000000003042768: 00007f7809507160 0000000000000000 40 (8 bytes): com.oracle.svm.core.heap.NoAllocationVerifier.openVerifiers = (Object) null 48 (8 bytes): com.oracle.svm.core.jdk.IdentityHashCodeSupport.hashCodeGeneratorTL = (Object) null 56 (8 bytes): com.oracle.svm.core.snippets.SnippetRuntime.currentException = (Object) null 64 (8 bytes): com.oracle.svm.core.thread.JavaThreads.currentThread = (Object) java.lang.Thread 000000000264fa98 72 (8 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.activeTimer = (Object) null 80 (8 bytes): com.oracle.svm.jni.JNIObjectHandles.handles = (Object) com.oracle.svm.core.handles.ThreadLocalHandles 00007f7809501558 88 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPendingException.pendingException = (Object) null 96 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPinnedObjects.pinnedObjectsListHead = (Object) null 104 (8 bytes): com.oracle.svm.jni.JNIThreadOwnedMonitors.ownedMonitors = (Object) null 112 (8 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.freeList = (Word) 0 0000000000000000 120 (8 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.stackBoundaryTL = (Word) 1 0000000000000001 128 (8 bytes): com.oracle.svm.core.stack.JavaFrameAnchors.lastAnchor = (Word) 0 0000000000000000 136 (8 bytes): com.oracle.svm.core.thread.VMThreads.IsolateTL = (Word) 25636864 0000000001873000 144 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadHandleTL = (Word) 50477184 0000000003023880 152 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadIdTL = (Word) 50477184 0000000003023880 160 (8 bytes): com.oracle.svm.core.thread.VMThreads.nextTL = (Word) 0 0000000000000000 168 (4 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.yellowZoneStateTL = (int) -16843010 fefefefe 172 (4 bytes): com.oracle.svm.core.snippets.ImplicitExceptions.implicitExceptionsAreFatal = (int) 0 00000000 176 (4 bytes): com.oracle.svm.core.thread.Safepoint.safepointRequested = (int) 2147473200 7fffd730 180 (4 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.currentPauseDepth = (int) 0 00000000 184 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.safepointsDisabledTL = (int) 1 00000001 188 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.statusTL = (int) 1 00000001 VMOperation dump: No VMOperation in progress Dump Counters: Raw Stacktrace: 00007ffeb8e0a940: 000000000186e776 000000000207b9d0 00007ffeb8e0a950: 0000000001873000 000000000085b37c 00007ffeb8e0a960: 000000000084540a 00000000008454ca 00007ffeb8e0a970: 000000000264f128 000000000264ef58 00007ffeb8e0a980: 00007f78095018d8 0000000002650640 00007ffeb8e0a990: 000000000264f128 0000002602650c18 00007ffeb8e0a9a0: 0000000000845444 00007ffeb8e0a970 00007ffeb8e0a9b0: 0000000000000000 0000000000845f6e 00007ffeb8e0a9c0: 0000000002650e18 0000000002650c18 00007ffeb8e0a9d0: 0000000002650e18 0000000002070c60 00007ffeb8e0a9e0: 00000000021f48f8 00000000012b77e6 00007ffeb8e0a9f0: 0000000002650e18 0000000002650c18 00007ffeb8e0aa00: 0000001000000000 0000000002070c60 00007ffeb8e0aa10: 00007f7809507138 0000000000477f69 00007ffeb8e0aa20: 00007f7809503b88 00007f7809501910 00007ffeb8e0aa30: 00007f7809507138 00000000004831b4 00007ffeb8e0aa40: 0000000000000010 000000000085d16d 00007ffeb8e0aa50: 000000000000003b 00000000008b4bdb 00007ffeb8e0aa60: 000000000291e970 00007f7809504828 00007ffeb8e0aa70: 0000000100000007 0000000001079a70 00007ffeb8e0aa80: 00007f78095070b8 00007f7809507080 00007ffeb8e0aa90: 0000000001873000 000000000291e970 00007ffeb8e0aaa0: 00007f7809506f78 00007f78095070b8 00007ffeb8e0aab0: 0000000000000008 0000000000000010 00007ffeb8e0aac0: 0000000000000010 00000000008144a1 00007ffeb8e0aad0: 0000000000000007 0000000000cd7c2e 00007ffeb8e0aae0: 00007f7809504938 0000000001873000 00007ffeb8e0aaf0: 0000000002205088 00007f78095070b8 00007ffeb8e0ab00: 00007f7809507080 0000000cc0001000 00007ffeb8e0ab10: 0000000000000000 0000000000cd73eb 00007ffeb8e0ab20: 00007f7809503b58 00007f78095070b8 00007ffeb8e0ab30: 00007f7809507080 00007f78095038e0 00007ffeb8e0ab40: 00007f7807c8e388 000000000205e900 00007ffeb8e0ab50: 00007f7809501350 000000240000000c 00007ffeb8e0ab60: 000000000000000c 00007f78095038e0 00007ffeb8e0ab70: d15c483b00000000 00000000004830e5 00007ffeb8e0ab80: 0000000000000007 00007f78095038e0 00007ffeb8e0ab90: 00007f78095038e0 00000000006f2b33 00007ffeb8e0aba0: 000000000205e900 0000000002070448 00007ffeb8e0abb0: 00007f78095070b8 0000000000cd8b3d 00007ffeb8e0abc0: 00000000020864c8 0000000000cbffc1 00007ffeb8e0abd0: 0000000002070448 00007f78095070b8 00007ffeb8e0abe0: 0000000c00000000 00007f7809505ef8 00007ffeb8e0abf0: 00007f78095070d8 00007f7809504840 00007ffeb8e0ac00: 7cab467402070d98 0000000000fbfc08 00007ffeb8e0ac10: 0000000002634470 00007f7809507020 00007ffeb8e0ac20: 0000000001873000 00007f78095070d8 00007ffeb8e0ac30: 00007f7809504840 0000000000cc187e 00007ffeb8e0ac40: 0000000000000000 0000000000000000 00007ffeb8e0ac50: 00007f7807c91840 00007f7809504840 00007ffeb8e0ac60: 0000000002070d98 0000000000cc17b9 00007ffeb8e0ac70: 0000000000c848f0 00007f78095038e0 00007ffeb8e0ac80: 0000000002b33a78 0000000100cc4f83 00007ffeb8e0ac90: 0000000000483140 00000000004b5713 00007ffeb8e0aca0: 0000000002070d98 0000000000cdae9a 00007ffeb8e0acb0: 000000000209a600 00007f78095038e0 00007ffeb8e0acc0: 0000000002b33a78 000000000047c576 00007ffeb8e0acd0: 000000000209a600 000000000209a630 00007ffeb8e0ace0: 0000000002a1b8d8 0000000002a1b408 00007ffeb8e0acf0: 000000000209a600 00000000017acc23 00007ffeb8e0ad00: 0000000000000001 0000000000001000 00007ffeb8e0ad10: 0000000000000000 0000000000000000 00007ffeb8e0ad20: 0000000000000000 0000000000000000 00007ffeb8e0ad30: 0000000000000000 0000000000000000 Stacktrace Stage0: RSP 00007ffeb8e0a940 RIP 000000000085b3f6 FrameSize 32 RSP 00007ffeb8e0a960 RIP 000000000085b37c FrameSize 16 RSP 00007ffeb8e0a970 RIP 00000000008454ca FrameSize 80 RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e FrameSize 48 RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 FrameSize 48 RSP 00007ffeb8e0aa20 RIP 0000000000477f69 FrameSize 32 RSP 00007ffeb8e0aa40 RIP 00000000004831b4 FrameSize 320 RSP 00007ffeb8e0ab80 RIP 00000000004830e5 FrameSize 32 RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 FrameSize 256 RSP 00007ffeb8e0aca0 RIP 00000000004b5713 FrameSize 48 RSP 00007ffeb8e0acd0 RIP 000000000047c576 FrameSize 160 RSP 00007ffeb8e0ad70 RIP 000000000047c285 FrameSize 32 RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 FrameSize 256 RSP 00007ffeb8e0ae90 RIP 000000000048f162 FrameSize 32 RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c FrameSize 1 Stacktrace Stage1: RSP 00007ffeb8e0a940 RIP 000000000085b3f6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a960 RIP 000000000085b37c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a970 RIP 00000000008454ca com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aa20 RIP 0000000000477f69 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aa40 RIP 00000000004831b4 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ab80 RIP 00000000004830e5 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aca0 RIP 00000000004b5713 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0acd0 RIP 000000000047c576 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ad70 RIP 000000000047c285 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ae90 RIP 000000000048f162 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code Full Stacktrace: RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.jdk.VMErrorSubstitutions.shutdown(VMErrorSubstitutions.java:111) RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:74) RSP 00007ffeb8e0a960 RIP 000000000085b37c [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:59) RSP 00007ffeb8e0a970 RIP 00000000008454ca [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.ensureInitialized(SunMiscSubstitutions.java:176) RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.numberFromName(SunMiscSubstitutions.java:223) RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.findSignal(Signal.java:78) RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.\u0026lt;init\u0026gt;(Signal.java:140) RSP 00007ffeb8e0aa20 RIP 0000000000477f69 [image code] babashka.impl.pipe_signal_handler$handle_pipe_BANG_.invokeStatic(pipe_signal_handler.clj:11) RSP 00007ffeb8e0aa40 RIP 00000000004831b4 [image code] babashka.main$main.invokeStatic(main.clj:282) RSP 00007ffeb8e0ab80 RIP 00000000004830e5 [image code] babashka.main$main.doInvoke(main.clj:282) RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137) RSP 00007ffeb8e0aca0 RIP 00000000004b5713 [image code] clojure.core$apply.invokeStatic(core.clj:665) RSP 00007ffeb8e0acd0 RIP 000000000047c576 [image code] babashka.main$_main.invokeStatic(main.clj:442) RSP 00007ffeb8e0ad70 RIP 000000000047c285 [image code] babashka.main$_main.doInvoke(main.clj:437) RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137) RSP 00007ffeb8e0ae90 RIP 000000000048f162 [image code] babashka.main.main(Unknown Source) RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.runCore(JavaMainWrapper.java:151) RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:186) RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.code.IsolateEnterStub.JavaMainWrapper_run_5087f5482cc9a6abc971913ece43acb471d2631b(IsolateEnterStub.java:0) [Native image heap boundaries: ReadOnly Primitives: 0x1873008 .. 0x206f048 ReadOnly References: 0x206ff78 .. 0x24fc9f8 Writable Primitives: 0x24fd000 .. 0x26343e0 Writable References: 0x2634470 .. 0x2ba42c0] [Heap: [Young generation: [youngSpace: aligned: 0/0 unaligned: 0/0]] [Old generation: [fromSpace: aligned: 0/0 unaligned: 0/0] [toSpace: aligned: 0/0 unaligned: 0/0] ] [Unused: aligned: 0/0]] Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed. RequestId: 263ff1be-425d-4dcb-9ea5-67020dc3041b Error: Runtime exited with error: exit status 99 Runtime.ExitError  The fight After some Googling I've discovered several related clues here and here. They say that signals are not supported in AWS lambda. So, why not to disable signals for babashka and see what happens? I've forked the repo, made a flag that disables PIPE signal handling, deployed babashka to the docker hub and tried to deploy lambda once again.\nAnd? It worked:\nmake function-name=$(make get-function-name) invoke-function =\u0026gt; {\u0026quot;test\u0026quot;:\u0026quot;test914\u0026quot;}{ \u0026quot;StatusCode\u0026quot;: 200, \u0026quot;ExecutedVersion\u0026quot;: \u0026quot;$LATEST\u0026quot; }  Summary Here is the example of babashka script that can be deployed to AWS Lambda.\n The function.zip weights just 18MB. The cold startup of the Lambda that is given 128MB of RAM is ~400ms. Subsequent calls ranges from 4ms and 120ms. The more RAM you give the faster lambda gets. I can develop the code in Cursive as the structure is like of an ordinary Clojure deps.edn project (and it can be used on the JVM). I made a PR to babashka and I've got accepted.  Next Steps  Fix Problem building on macos (/tmp dir is not writable). Get rid of AWS CloudFormation part. Work a bit more to support AWS API Gateway. Create a template for such projects.  ","date":1584748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584748800,"objectID":"2d26caeb6ab5a7424a8d00ad7870fe3d","permalink":"https://www.jocas.lt/blog/post/babashka-aws-lambda/","publishdate":"2020-03-21T00:00:00Z","relpermalink":"/blog/post/babashka-aws-lambda/","section":"post","summary":"Adventures with babashka and AWS Lambda.","tags":["AWS Lambda","babashka","clojure","GraalVM"],"title":"Deploy babashka script to AWS Lambda","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch"],"content":"I want to take a look at Search Templates for Elasticsearch. Let's apply them to examples from previous post on Synonym Graphs.\nSetup I'm using Elasticsearch 7.5.1.\nIndex configuration:\nDELETE test_index-1 PUT /test_index-1 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;descrition\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;entity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;lowercase_keyword_analyzer\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;synonym_graph_analyzer\u0026quot; } } }, \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;synonym_graph_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_synonym_graph\u0026quot; ] }, \u0026quot;lowercase_keyword_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot; ], \u0026quot;char_filter\u0026quot;: [ \u0026quot;spaces_to_undescores_filter\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;spaces_to_undescores_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mapping\u0026quot;, \u0026quot;mappings\u0026quot;: [ \u0026quot; \\\\u0020 =\u0026gt; _\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;my_synonym_graph\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym_graph\u0026quot;, \u0026quot;lenient\u0026quot;: true, \u0026quot;synonyms\u0026quot;: [ \u0026quot;very important thing =\u0026gt; very_important_thing\u0026quot; ] } } } } } }  Index the document:\nPUT test_index-1/_doc/1 { \u0026quot;description\u0026quot;: \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot;: \u0026quot;Very Important Thing\u0026quot; }  Search queries:\n prefix very important thing suffix  Templates I'm very interested in one particular use of the search templates: how flexible is the management of stored seach templates? Can I update a search template while receiving queries?\nAdd a template:\nPOST _scripts/synonym-graph-search { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;mustache\u0026quot;, \u0026quot;source\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;{{query_string}}\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } } } }  Try to run the search:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: \u0026quot;suffix very important thing prefix\u0026quot; } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.5753642, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.5753642, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Exactly as expected.\nWhen using a stored search template the Elasticsearch client doesn't need to handle the complex query construction.\nTemplates are updateable Let's try to update the template with a higher boost value:\nPOST _scripts/synonym-graph-search { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;mustache\u0026quot;, \u0026quot;source\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;{{query_string}}\u0026quot;, \u0026quot;boost\u0026quot;: 5 } } } } } }  Works.\nNow let's run the same query:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: \u0026quot;suffix very important thing prefix\u0026quot; } }  This yields:\n{ \u0026quot;took\u0026quot; : 4, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.4384103, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.4384103, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  The scores are 0.5753642 and 1.4384103 that is ~2/5. Cool! This means that without changing (and redeploying) the Elasticsearch client we can change the querying logic, making the query an more dynamic.\nCorner Cases What if we run query has more attributes, e.g.:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: \u0026quot;suffix very important thing prefix\u0026quot;, \u0026quot;new_attr\u0026quot;: \u0026quot;123\u0026quot; } }  Works as expected!\nWhen query_string is null:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: null } }  Works!\nWhat if the param is not provided:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;new_attr\u0026quot;: \u0026quot;value\u0026quot; } }  No error!\nWhat if we provide a list instead of a string:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: [\u0026quot;this\u0026quot;, \u0026quot;Very Important Thing\u0026quot;] } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.4384103, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.4384103, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Instead of profiding one value we can replace it with a list. Good!\nMetadata of the search template It would be great to be able to store some metadata with the search template script, e.g. Git commit SHA of the query. I couldn't find a way to do this. A workaround might be to _name attribute of the query. E.g.:\nPOST _scripts/synonym-graph-search { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;mustache\u0026quot;, \u0026quot;source\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;_name\u0026quot;: \u0026quot;GIT COMMIT SHA\u0026quot;, \u0026quot;query\u0026quot;: \u0026quot;{{query_string}}\u0026quot;, \u0026quot;boost\u0026quot;: 5 } } } } } }  The response:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.4384103, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.4384103, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; }, \u0026quot;matched_queries\u0026quot; : [ \u0026quot;GIT COMMIT SHA\u0026quot; ] } ] } }  Not great but might be useful.\nDiscussion  Templates doesn't support search index specification. Field names can be parameterized, this feature alows to start/stop using a new/old field. Search template can be tested in (even in production cluster) independently. We can run our query against multiple search templates. Combine this with the Profile API and performance can be compared. Explain API also is supported.  ","date":1577059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577059200,"objectID":"dae7e72b85246f090d3d6b15c05e133d","permalink":"https://www.jocas.lt/blog/post/on-search-templates/","publishdate":"2019-12-23T00:00:00Z","relpermalink":"/blog/post/on-search-templates/","section":"post","summary":"A couple of examples and notes on using Elasticsearch search templates","tags":["elasticsearch"],"title":"Using Search Templates in Elasticsearch","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch"],"content":"I've written that if you google for How can you match a long query text to a short text field? you're advised to use Elasticsearch Percolator. Today I'll show an alternative way of solving the same problem with Elasticsearch.\nThe main idea is to use Synonym Graph Token Filter with some data preparation.\nProblem Statement Say that we learned how extract some entity from free form text with techniques such as NER, dictionary annotations, or some fancy Machine Learning. And when this entity is mentioned in the search query we want to boost documents that mention this entity. Also, say you've ruled out using Elasticsearch Percolator because it increases network latency because it requires additional call to Elasticsearch.\nFor further discussion our unstructured text is going to be This description is about a Very Important Thing and something else. and the extracted entity Very Important Thing. Our test document looks like :\n{ \u0026quot;description\u0026quot;: \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot;: \u0026quot;Very Important Thing\u0026quot; }  Search queries:\n prefix very important thing suffix prefix very important another thing suffix prefix thing suffix  All examples are tested on Elasticsearch 7.5.1.\nNaive Setup Let's create an index for our documents:\nPUT /test_index-2 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;entity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } }  Entity field is of type text because we want it to be searchable. keyword type won't work because it does only exact matches and out query most likely will be longer than our entity string.\nIndex our document:\nPUT test_index-2/_doc/1 { \u0026quot;description\u0026quot;: \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot;: \u0026quot;Very Important Thing\u0026quot; }  Search the index with the query that mentions our very important thing:\nGET test_index-2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.7260926, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-2\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.7260926, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Cool, we found what we we looking for.\nLet's try another query, this time with a mention of very important another thing:\nGET test_index-2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important another thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.7260926, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-2\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.7260926, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Oh, the results are the same as with the previous query despite the fact that we mention Another Thing here. But it still might be OK because we matched all the terms of the entity.\nLet's try another query:\nGET test_index-2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.5753642, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-2\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.5753642, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Oh no, we still matched our Very Important Thing while only thing term is present in the query. But at least this time the score is lower than with previous twoqueries, 0.5753642 vs. 1.7260926. Here we clearly see the problem: we are matching short strings with long strings and partial matches raises problems.\nProposed Solution Let's leverage Synonym Graph Token Filter to solve our problem.\nPUT /test_index-1 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;descrition\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;entity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;lowercase_keyword_analyzer\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;synonym_graph_analyzer\u0026quot; } } }, \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;synonym_graph_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_synonym_graph\u0026quot; ] }, \u0026quot;lowercase_keyword_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot; ], \u0026quot;char_filter\u0026quot;: [ \u0026quot;spaces_to_undescores_filter\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;spaces_to_undescores_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mapping\u0026quot;, \u0026quot;mappings\u0026quot;: [ \u0026quot; \\\\u0020 =\u0026gt; _\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;my_synonym_graph\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym_graph\u0026quot;, \u0026quot;lenient\u0026quot;: true, \u0026quot;synonyms\u0026quot;: [ \u0026quot;very important thing =\u0026gt; very_important_thing\u0026quot; ] } } } } } }  Let's decompose this large index configuration piece by piece:\n The entity attribute now has separate analyzers for both index and search phases. The lowercase_keyword_analyzer uses keyword tokenizer which means that tokenization will result in the sequence of token of size 1, then it normalizes tokens by lowercasing them and finally spaces_to_undescores_filter, replaces spaces to underscores. E.g. a string \u0026quot;Very Important Thing\u0026quot; is transformed into list of tokens [\u0026quot;very_important_thing\u0026quot;]. Or use out friend _analyze API:  POST test_index-1/_analyze { \u0026quot;text\u0026quot;: [\u0026quot;Very Important Thing\u0026quot;], \u0026quot;analyzer\u0026quot;: \u0026quot;lowercase_keyword_analyzer\u0026quot; }  This yields:\n{ \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;very_important_thing\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 20, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 0 } ] }  The synonym_graph_analyzer use standard tokenizer, which is followed by the lowercase filter, and then the my_synonym_graph token filter is applied. We've set up one synonym \u0026quot;very important thing =\u0026gt; very_important_thing\u0026quot;. E.g.  POST test_index-1/_analyze { \u0026quot;text\u0026quot;: [\u0026quot;prefix very important thing suffix\u0026quot;], \u0026quot;analyzer\u0026quot;: \u0026quot;synonym_graph_analyzer\u0026quot; }  This yields:\n{ \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;prefix\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 6, \u0026quot;type\u0026quot; : \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot; : 0 }, { \u0026quot;token\u0026quot; : \u0026quot;very_important_thing\u0026quot;, \u0026quot;start_offset\u0026quot; : 7, \u0026quot;end_offset\u0026quot; : 27, \u0026quot;type\u0026quot; : \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot; : 1 }, { \u0026quot;token\u0026quot; : \u0026quot;suffix\u0026quot;, \u0026quot;start_offset\u0026quot; : 28, \u0026quot;end_offset\u0026quot; : 34, \u0026quot;type\u0026quot; : \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot; : 2 } ] }  After analysis we have 3 tokens [\u0026quot;prefix\u0026quot;, \u0026quot;very_important_thing\u0026quot;, \u0026quot;suffix\u0026quot;]. Notice \u0026quot;very_important_thing\u0026quot; token: this is equal to the right-hand-side from our synonym definitions. Now let's run queries from the previous section:\nGET test_index-1/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.5753642, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.5753642, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  As expected: exact match -\u0026gt; hit.\nAnother query:\nGET test_index-1/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important another thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 0, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No hits! Good! The document is not going to be boosted despite the fact that all tokens match.\nAnd the last one:\nGET test_index-1/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No hits! Good. This means that also substring doesn't match.\nDiscussion Synonym Graph Token Filter can \u0026ldquo;replace\u0026rdquo; a sequence of tokens (e.g. a phrase) with another sequence of tokens. In this particular example: many tokens were replaced with one token.\n One field can have only one analyzer pair for index and search phases. If we want another analysis pipeline for the entity attribute we have to create another field with the analyzers specified, e.g. stemmed phrase with lower boost. The synonym list must be prepared before the index creation. Management of the synonym list might complicate index management, e.g. you use templates for your index management. The overal solution in general might look a bit too complicated.  ","date":1576972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576972800,"objectID":"bbf1ba4712ba08644677c25dbacd8b0b","permalink":"https://www.jocas.lt/blog/post/synonym-graph-phrase-search/","publishdate":"2019-12-22T00:00:00Z","relpermalink":"/blog/post/synonym-graph-phrase-search/","section":"post","summary":"An idea on how to search for short string with a long query string","tags":["elasticsearch"],"title":"Phrase Search with Synonym Graph Token Filter in Elasticsearch","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch","percolator"],"content":"This time I need to percolate texts with different analyzers for index and search analyzers.\nLet's elaborate a bit on previous article and explicitly declare analyzers to use.\nDefine index:\nPUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } }  Then define 2 slightly different percolator queries (notice the difference between \u0026quot;bonsai tree\u0026quot; and \u0026quot;bonsai, tree\u0026quot;).\nPUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai tree\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot; } } } } PUT /my-index/_doc/2?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai, tree\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot; } } } }  Let's percolate:\nGET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 80, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 2, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai, tree\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;standard\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } }, { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai tree\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;standard\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  As expected: 2 documents matched.\nBut now lets change the analyzer of the second percolation query to whitespace:\nPUT /my-index/_doc/2?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai, tree\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;whitespace\u0026quot; } } } }  Run the percolator:\n GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 5, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai tree\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;standard\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  As expected: only 1 percolator query matched our input.\nPhrases with Stopwords Say, we have a phrase \u0026quot;bonsai is tree\u0026quot; and we percolate text A new bonsai in tree in the office with the standard analyzer for indexing and english for search analyzer. There should be no matches. Let's try:\nDELETE my-index PUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;english\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } } PUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai is tree\u0026quot; } } } } GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai in tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  And, surprisingly, this yields:\n{ \u0026quot;took\u0026quot; : 2, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai is tree\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] } } ] } }  We have a match! Also notice that the highlighter is broken!\nThe problem that these two analyzers have different stopword lists (no stopwords for standard and several English stopwords for english analyzer) and the phrase contains a stopword that is not shared between analyzers.\nLet's fix this surprise with search_quote_analyzer.\nDELETE my-index PUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;english\u0026quot;, \u0026quot;search_quote_analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } } PUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai is tree\u0026quot; } } } } GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai in tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No hits, as expected.\nLet's check if the expected behaviour is still there:\nGET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai is tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 4, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.39229375, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.39229375, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai is tree\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai is tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  Good. Even the highlighting works.\n","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"e3f59a2e678c8500ecbac9f71472142b","permalink":"https://www.jocas.lt/blog/post/percolator-phrase-analyzers/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/blog/post/percolator-phrase-analyzers/","section":"post","summary":"Example on how to use analyzers with the","tags":["elasticsearch","percolator"],"title":"Elasticsearch Percolator and Text Analyzers","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch","percolator"],"content":"If you google How can you match a long query text to a short text field? it will point you to the Stack Overflow page or here where the answer is to use Elasticsearch Percolator.\nMy search items are phrases meaning that it should match all terms in order. Let's create a sample setup in Kibana (v7.5) Dev dashboard.\n Create an index for percolation:  PUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } }  Note on \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot;: this allows Fast Vector Highlighter to highlight combined phrase not just separate qeury terms.\nStore one phrase query:  PUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;bonsai tree\u0026quot; } } }  Percolate a document:  GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  Note on \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot;: this instructs Elasticsearch to use the Fast Vector Highlighter.\nThe query yields:\n{ \u0026quot;took\u0026quot; : 23, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : \u0026quot;bonsai tree\u0026quot; } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  As we see highlighter correctly marker the search phrase.\nStoring additional data with percolator queries Percolation result can be used to connect pieces of information in your system, e.g. store a subscriber_email attribute of the user that wants to be notified when the query matches along with the percolator query.\nPUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;bonsai tree\u0026quot; } }, \u0026quot;subscriber_email\u0026quot;: \u0026quot;subscriber_email@example.com\u0026quot; }  Then query:\nGET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This query yields:\n{ \u0026quot;took\u0026quot; : 10, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : \u0026quot;bonsai tree\u0026quot; } }, \u0026quot;subscriber_email\u0026quot; : \u0026quot;subscriber_email@example.com\u0026quot; }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  Now, take the email under the \u0026quot;subscriber_email\u0026quot; from the response and send an email with the highlight.\n","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"8e98378fbfbaafa1f97ba3a46fd35817","permalink":"https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/blog/post/es-percolator-phrase-highlight/","section":"post","summary":"Investigating the Elasticsearch percolator.","tags":["elasticsearch","search"],"title":"Phrase Highlighting with the Elasticsearch Percolator","type":"post"},{"authors":["Dainius Jocas"],"categories":["clojure","aws","lambda","devops"],"content":"I was writing a Clojure application and the plan was to deploy it as a AWS Lambda. The question I'm going to answer in this blog post is: how to build an uberjar for AWS Lambda with Uberdeps?\nTL;DR Add an alias to the deps.edn for uberjar building:\n{:aliases {:uberjar {:extra-deps {uberdeps {:mvn/version \u0026quot;0.1.6\u0026quot;}} :main-opts [\u0026quot;-m\u0026quot; \u0026quot;uberdeps.uberjar\u0026quot;]}}}  Create an executable file compile.clj in the project root folder:\ntouch compile.clj chmod +x compile.clj  Put this code in the compile.clj file:\n Run:\n(rm -rf classes \u0026amp;\u0026amp; \\ mkdir classes \u0026amp;\u0026amp; \\ ./compile.clj \u0026amp;\u0026amp; \\ clojure -A:uberjar --target target/UBERJAR_NAME.jar)  I'd advise put that last script into a Makefile ;)\n Introduction To deploy your Clojure code to AWS Lambda you need to package it as an uberjar. If your project is managed with deps.edn, basically you're on your own to find a suitable library to package your code.\nFor some time to build uberjars for deps.edn projects I was using Cambada. It did the job but I was not entirely happy with the library for a couple of reasons:\n the library seems to be no longer maintained; it has various bugs with transitive Git dependencies. I've found out that these bugs are fixed in a fork of the Cambada and I used it as a git dependency.  Because building an uberjar for deps.edn boils down to just finding a library there is always temptation to try something new.\nEnter Uberdeps For my toy project I wanted to try out Uberdeps. The introduction blog post got me interested and I really liked the main idea:\n Takes deps.edn and packs an uberjar out of it.\n Sounds like exactly what I need.\nTrouble I've written my application, added all the things needed to deploy it as an AWS Lambda, build an uberjar with Uberdeps, deployed the app with the AWS CloudFormation, but when I've invoked the Lambda I've received an error:\n{ \u0026quot;message\u0026quot; : \u0026quot;Internal server error\u0026quot; }  After searching through the AWS CloudWatch logs I've found:\nClass not found: my.Lambda: java.lang.ClassNotFoundException java.lang.ClassNotFoundException: my.Lambda at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348)  The my.Lambda class was not found.\nAfter taking a look at the contents of the uberjar I've noticed that the my.Lambda class is indeed not inside the Uberjar. Ah, it seems that AOT (Ahead-of-Time) is not done out of the box. After searching and not finding a flag or some parameter that I need to pass to force the AOT compilation in the Uberdeps README, I've discovered an already closed pull request: the AOT compilation functionality is not implemented.\nI was in trouble.\nSolution The solution was to manually perform AOT compilation of the relevant namespaces right before building an uberjar and then instruct Uberdeps to put the resulting class files into the uberjar.\nTo do AOT compilation I've written a Clojure script compile.clj:\n Inspiration on how to write the script was taken from here and here.\nTo instruct Uberdeps to put class files to the uberjar I've added classes directory to the :paths vector in deps.edn.\nJust for the convenience, in the Makefile I've put commands for AOT compilation right before the command to build an uberjar:\nuberjar: rm -rf classes mkdir classes ./compile.clj clojure -A:uberjar --target target/my-jar-name.jar  And that is it! I have an uberjar with my.Lambda class and the AWS Lambda runtime is happy.\nDiscussion The solution is not bullet proof because:\n it assumes that the main deps.end file is called deps.edn; compiled classes are put in the classes directory; the alias for which namespaces should be AOT compiled is the default alias.  I hope that when a more generic solution will be needed either the Uberdeps will have an option for AOT compilatoin or I'll be clever enough to deal with the situation and write a follow up blog post with the workaround.\n","date":1573776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573776000,"objectID":"bc08dd6e0a5cfada2a44f4a636a8db0f","permalink":"https://www.jocas.lt/blog/post/uberdeps-for-aws-lambda/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/blog/post/uberdeps-for-aws-lambda/","section":"post","summary":"A guide on how to build an Uberjar for AWS Lambda with `tonsky/uberdeps`","tags":["clojure","devops","aws","lambda"],"title":"Using Uberdeps to Build AWS Lambda Uberjar","type":"post"},{"authors":["Dainius Jocas"],"categories":["clojure","gitlab","ci","devops"],"content":"I want to share my hard-won lessons on how to setup the Gitlab CI for Clojure projects based on tools.deps. I think that the Gitlab CI is a wonderful tool for CI workloads. But when you're going a bit sideways from the documented ways of doing things you have to do a bit of discovery for yourself.\nGitlab CI CacheSetup Usually I want to cache dependencies between all build and all branches. To achieve this I hard-code the cache key at the root of the.gitlab-ci.yml file e.g.:\ncache: key: one-key-to-rule-them-all  When it comes to caching Clojure dependencies we have to be aware that there different types of dependencies. Two most common ones are: Maven and gitlibs.\nThe Gitlab CI cache works only with directories inside the project directory. While local repositories (i.e. cache) for Clojure dependencies by default are stored outside the project directory (~/.m2 and ~/.gitlibs). Therefore, we have to provide parameters for our build tool to change the default directories for storing the dependencies.\nTo specify Maven local repository we can provide:mvn/local-repo parameter e.g.:\nclojure -Sdeps '{:mvn/local-repo \u0026quot;./.m2/repository\u0026quot;}' -A:test  Having configured local maven repository in our gitlab-ci.yml we can specify:\ncache: key: one-key-to-rule-them-all paths: - ./.m2/repository  When it comes to gitlibs there is no public API for changing the default directory in tools.deps. But the underlying tools.gitlibs uses an environment variable to set where to store the gitlibs conveniently named GITLIBS. E.g.\n$ (export GITLIBS=\u0026quot;.gitlibs/\u0026quot; \u0026amp;\u0026amp; clojure -A:test)  Of course, we should not forget to configure the cache:\ncache: key: one-key-to-rule-them-all paths: - ./.gitlibs  To use caching for both types of dependencies:\n(export GITLIBS=\u0026quot;.gitlibs/\u0026quot; \u0026amp;\u0026amp; clojure -Sdeps '{:mvn/local-repo \u0026quot;./.m2/repository\u0026quot;}' -A:test)  And setup the cache:\ncache: key: one-key-to-rule-them-all paths: - ./.m2/repository - ./.gitlibs  If you want to disable cache for a particular job (e.g. you're linting with clj-kondo, which is delivered as a GraalVM compiled native image), just give an empty map for a job's cache setup, e.g.:\nlint: stage: test image: borkdude/clj-kondo cache: {} when: always script: - clj-kondo --lint src test  I've used the Gitlab CI cache while working on a streaming-text search library Beagle. A full.gitlab-ci.yml file example of the setup can be found here.\nHope this helps!\n","date":1573430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573430400,"objectID":"10dfc9aba13fb40f8ec0be74224e4205","permalink":"https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/","publishdate":"2019-11-11T00:00:00Z","relpermalink":"/blog/post/gitlab-ci-clojure-dependencies/","section":"post","summary":"A guide on how to use Gitlab CI Cache for Clojure Dependencies","tags":["clojure","devops"],"title":"Using Gitlab CI Cache for Clojure Dependencies","type":"post"},{"authors":["Dainius Jocas"],"categories":null,"content":"  Clojure workflow @ TokenMill by Dainius Jocas  The source code of the demo project can be found here.\n","date":1559242800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559242800,"objectID":"86663215daf06a1fb32613517d99226f","permalink":"https://www.jocas.lt/blog/talk/vilnius-clojure-meetup/","publishdate":"2019-11-11T19:00:00Z","relpermalink":"/blog/talk/vilnius-clojure-meetup/","section":"talk","summary":"Sharing Joy of Clojure Programming","tags":["meetup","clojure","lambda","aws","graalvm"],"title":"Clojure Workflow @ TokenMill","type":"talk"}]