<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Dainius Jocas</title><link>https://www.jocas.lt/blog/authors/dainius-jocas/</link><atom:link href="https://www.jocas.lt/blog/authors/dainius-jocas/index.xml" rel="self" type="application/rss+xml"/><description>Dainius Jocas</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2020 Dainius Jocas</copyright><lastBuildDate>Wed, 08 Jul 2020 00:00:00 +0000</lastBuildDate><image><url>https://www.jocas.lt/blog/images/icon_hu849715217c2cf577e44af3c34605d58b_27848_512x512_fill_lanczos_center_2.png</url><title>Dainius Jocas</title><link>https://www.jocas.lt/blog/authors/dainius-jocas/</link></image><item><title>A Neat Trick with Elasticsearch Normalizers</title><link>https://www.jocas.lt/blog/post/elasticsearch-normlizers/</link><pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/elasticsearch-normlizers/</guid><description>&lt;p>To analyze the textual data Elasticsearch uses &lt;strong>analyzers&lt;/strong> while for the keyword analysis there is a thing called a &lt;strong>normalizer&lt;/strong>. In this article I&amp;rsquo;ll explain what the normalizer is and show it&amp;rsquo;s use case for &lt;strong>normalizing&lt;/strong> URLs.&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>A neat use case for keyword normalizers is to extract a specific part of the URL with a char_filter of the &lt;code>pattern_replace&lt;/code> type.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In Elasticsearch the textual data is represented with two data types: &lt;code>text&lt;/code> and &lt;code>keyword&lt;/code>. The &lt;code>text&lt;/code> type is meant to be used for full-text search use cases while &lt;code>keyword&lt;/code> is mean for filtering, sorting, and aggregation.&lt;/p>
&lt;h3 id="tldr-about-analyzers">TL;DR About Analyzers&lt;/h3>
&lt;p>To make a better use of &lt;code>text&lt;/code> data you can setup the
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer-anatomy.html" target="_blank" rel="noopener">analyzer&lt;/a> which is a combination of three components:&lt;/p>
&lt;ul>
&lt;li>exactly one &lt;strong>tokenizer&lt;/strong>,&lt;/li>
&lt;li>zero or more &lt;strong>character filters&lt;/strong>,&lt;/li>
&lt;li>zero or more &lt;strong>token filters&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>Basically, an analyzer transforms a single &lt;em>string&lt;/em> into &lt;em>words&lt;/em>, e.g. &lt;code>&amp;quot;This is my text&amp;quot;&lt;/code> can be transformed into &lt;code>[&amp;quot;this&amp;quot;, &amp;quot;my&amp;quot;, &amp;quot;text&amp;quot;]&lt;/code> which you can read as:&lt;/p>
&lt;ul>
&lt;li>text is split into tokens by tokenizer,&lt;/li>
&lt;li>each token is lowercased with the a token filter,&lt;/li>
&lt;li>stopwords are removed with another token filter.&lt;/li>
&lt;/ul>
&lt;h3 id="normalizers">Normalizers&lt;/h3>
&lt;p>The
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-normalizers.html" target="_blank" rel="noopener">documentation&lt;/a> says that:&lt;/p>
&lt;blockquote>
&lt;p>Normalizers are similar to analyzers except that they may only emit a single token.&lt;/p>
&lt;/blockquote>
&lt;p>Normalizers can only be applied to the &lt;code>keyword&lt;/code> datatype. The cannonical use case is to lowercase structured content such as IDs, email addresses, e.g. a database stores emails in whatever case but searching for emails should be case insensitive. Note that only a subset of available filters can be used by a normalizer: all filters must work on a &lt;strong>per-character basis&lt;/strong>, i.e. no stopwords or stemmers.&lt;/p>
&lt;h3 id="normalizers-for-normalizing-url-data">Normalizers for Normalizing URL Data&lt;/h3>
&lt;p>Storing a URL in a &lt;code>keyword&lt;/code> field allows to filter, sort, and aggregate your data per URL. But what if you need to filter, sort, and aggregate by just one part of the URL and you have little to no control over the upstream data source? You have a couple of options:&lt;/p>
&lt;ul>
&lt;li>convince upstream to extract that one part in their code and send it to you,&lt;/li>
&lt;li>setup a &lt;code>text&lt;/code> field with an analyzer that produces just that one token and enable field data (not a default setup and can get expensive).&lt;/li>
&lt;li>setup a &lt;code>keyword&lt;/code> field with a normalizer with a &lt;code>char_filter&lt;/code>.&lt;/li>
&lt;li>give up.&lt;/li>
&lt;/ul>
&lt;p>I want to explore the &lt;code>keyword&lt;/code> option. In the next section I&amp;rsquo;ll show how to setup normalizers for Elasticsearch URLs.&lt;/p>
&lt;h3 id="the-not-so-synthetic-problem">The not so Synthetic Problem&lt;/h3>
&lt;p>We have a list URLs without a hostname that were used to query Elasticsearch, e.g.: &lt;code>/my_search_index/_search?q=elasticsearch&lt;/code> and we need to split URLs into parts such as: index, operation endpoint, e.g.: &lt;code>_search&lt;/code> or &lt;code>_count&lt;/code>, query filters, etc. In the following example I&amp;rsquo;ll focus on the extracting the index part of the URL.&lt;/p>
&lt;p>Let&amp;rsquo;s create an index:&lt;/p>
&lt;pre>&lt;code>PUT elasticsearch_url_index
{
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;index_extractor_normalizer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;char_filter&amp;quot;: [
&amp;quot;index_name_extractor&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;index_name_extractor&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;/(.+)/.*&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;$1&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot; : {
&amp;quot;properties&amp;quot;: {
&amp;quot;url&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;index_extractor_normalizer&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Here we setup the index with a normalizer &lt;code>index_extractor_normalizer&lt;/code> that has a char filter &lt;code>index_name_extractor&lt;/code> that uses a regex &lt;code>pattern_replace&lt;/code> to extract characters between the first and the second slashes. The mappings have a property &lt;code>url&lt;/code> which is of the &lt;code>keyword&lt;/code> type and have a field &lt;code>index&lt;/code> which is also of the &lt;code>keyword&lt;/code> type and is set up to use the normalizer &lt;code>index_extractor_normalizer&lt;/code>.&lt;/p>
&lt;p>Since the normalizer is basically a collection of filters we can use our good old friend &lt;code>_analyze&lt;/code> API to test how it works.&lt;/p>
&lt;pre>&lt;code>POST elasticsearch_url_index/_analyze
{
&amp;quot;char_filter&amp;quot;: [&amp;quot;index_name_extractor&amp;quot;],
&amp;quot;text&amp;quot;: [&amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;my_search_index&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 40,
&amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
&amp;quot;position&amp;quot; : 0
}
]
}
&lt;/code>&lt;/pre>
&lt;p>Good, exactly as we wanted: &lt;code>/my_search_index/_search?q=elasticsearch&lt;/code> =&amp;gt; &lt;code>my_search_index&lt;/code>.&lt;/p>
&lt;p>Let&amp;rsquo;s index some data:&lt;/p>
&lt;pre>&lt;code>PUT elasticsearch_url_index/_doc/0
{
&amp;quot;url&amp;quot;: &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s try to filter URLs by the index name:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url:my_search_index
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 0,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No results? What? Oh! Wrong field: &lt;code>url&lt;/code> was used instead of &lt;code>url.index&lt;/code>. Let&amp;rsquo;s try once again:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url.index:my_search_index
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.2876821,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;elasticsearch_url_index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;0&amp;quot;,
&amp;quot;_score&amp;quot; : 0.2876821,
&amp;quot;_source&amp;quot; : {
&amp;quot;url&amp;quot; : &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected. Cool.&lt;/p>
&lt;h3 id="bonus-a-trick-with-the-docvalue_fields">Bonus: a Trick with the &lt;code>docvalue_fields&lt;/code>&lt;/h3>
&lt;p>Another neat trick is that we can get out the &lt;code>index&lt;/code> part of the URL from an Elasticsearch index using the &lt;code>docvalue_fields&lt;/code> option in a request ,e.g.:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url.index:my_search_index
{
&amp;quot;docvalue_fields&amp;quot;: [&amp;quot;url.index&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.2876821,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;elasticsearch_url_index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;0&amp;quot;,
&amp;quot;_score&amp;quot; : 0.2876821,
&amp;quot;_source&amp;quot; : {
&amp;quot;url&amp;quot; : &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
},
&amp;quot;fields&amp;quot; : {
&amp;quot;url.index&amp;quot; : [
&amp;quot;my_search_index&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The important part is this one:&lt;/p>
&lt;pre>&lt;code>&amp;quot;fields&amp;quot; : {
&amp;quot;url.index&amp;quot; : [
&amp;quot;my_search_index&amp;quot;
]
}
&lt;/code>&lt;/pre>
&lt;p>A neat thing about the &lt;code>docvalue_fields&lt;/code> is that in the example above the &lt;code>my_search_index&lt;/code> value is not comming from the &lt;code>_source&lt;/code> of the document. This means that we can use &lt;code>keywords&lt;/code> and by extension normalized &lt;code>keywords&lt;/code> to fetch an exact value from the Elasticsearch index and not necessarily the one that was sent to Elasticsearch which somewhat solves our dependency from the upstream systems.&lt;/p>
&lt;h2 id="notes">Notes&lt;/h2>
&lt;p>The setup is done in the Kibana Dev Tools with the Elasticsearch 7.7.0.&lt;/p>
&lt;p>The pattern &lt;code>&amp;quot;/(.+)/.*&amp;quot;&lt;/code> is a bit simplified purely for presentation purposes and doesn&amp;rsquo;t work as expected for URLs with more than 2 slashes, e.g.: &lt;code>/index/type/_search&lt;/code> would produce &lt;code>index/type&lt;/code>. You need something a bit more involved like &lt;code>&amp;quot;/([^/]+)/.*&amp;quot;&lt;/code>.&lt;/p>
&lt;h2 id="fin">Fin&lt;/h2>
&lt;p>That is all I wanted to show you today. Hope it might be useful/interesting to someone down the line. Leave comments on the Github issue
&lt;a href="https://github.com/dainiusjocas/blog/issues/7" target="_blank" rel="noopener">here&lt;/a>. Cheers!&lt;/p></description></item><item><title>Deploy babashka script to AWS Lambda</title><link>https://www.jocas.lt/blog/post/babashka-aws-lambda/</link><pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/babashka-aws-lambda/</guid><description>&lt;p>TL;DR&lt;/p>
&lt;p>I&amp;rsquo;ve managed to package a simple
&lt;a href="https://github.com/borkdude/babashka" target="_blank" rel="noopener">babashka&lt;/a> script to an AWS Lambda Custom Runtime.
&lt;a href="https://github.com/dainiusjocas/babashka-lambda" target="_blank" rel="noopener">Here&lt;/a> is the code, try for yourself.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Wouldn&amp;rsquo;t it be great to deploy little Clojure code snippets to Custom Lambda Runtime? The main benefits would be:&lt;/p>
&lt;ul>
&lt;li>you would not suffer from java cold-start problems;&lt;/li>
&lt;li>you wouldn&amp;rsquo;t need to compile your project with GraalVM &lt;code>native-image&lt;/code> tool which is time consuming and for anything more advanced is not likely to work anyway;&lt;/li>
&lt;li>babashka supports scripting with a subset of Clojure, which might do the work for you.&lt;/li>
&lt;/ul>
&lt;h2 id="the-plan">The plan&lt;/h2>
&lt;p>I know what it takes to deploy to Lambda Custom Runtime. Last year I&amp;rsquo;ve created a Clojure project template for deploying
&lt;a href="https://github.com/tokenmill/clojure-graalvm-aws-lambda-template" target="_blank" rel="noopener">GraalVM compiled AWS Lambda Custom Runtime&lt;/a>. And babashka is just another self contained binary. It should be too hard to bring two things together and get it working? Challenge accepted.&lt;/p>
&lt;h2 id="packaging">Packaging&lt;/h2>
&lt;p>I like to build software inside Docker containers. In this experiment, for the first attempt I&amp;rsquo;ve used this Dockerfile:&lt;/p>
&lt;pre>&lt;code>FROM borkdude/babashka:latest as BABASHKA
FROM clojure:tools-deps-alpine as BUILDER
RUN apk add --no-cache zip
WORKDIR /var/task
COPY --from=BABASHKA /usr/local/bin/bb bb
ENV GITLIBS=&amp;quot;.gitlibs/&amp;quot;
COPY lambda/bootstrap bootstrap
COPY deps.edn deps.edn
RUN clojure -Sdeps '{:mvn/local-repo &amp;quot;./.m2/repository&amp;quot;}' -Spath &amp;gt; cp
COPY src/ src/
COPY resources/ resources/
RUN zip -q -r function.zip bb cp bootstrap .gitlibs/ .m2/ src/ resources/ deps.edn
&lt;/code>&lt;/pre>
&lt;p>Here:&lt;/p>
&lt;ul>
&lt;li>copy &lt;code>bb&lt;/code> binary from babashka Docker image,&lt;/li>
&lt;li>download the dependencies for babashka script using &lt;code>clojure&lt;/code> (both, maven and git dependencies are supported, like is described
&lt;a href="https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/" target="_blank" rel="noopener">here&lt;/a>),&lt;/li>
&lt;li>write a classpath to the &lt;code>cp&lt;/code> file,&lt;/li>
&lt;li>copy all source code,&lt;/li>
&lt;li>zip the required contents to the &lt;code>function.zip&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Every line of this dockerfile is packed with details but I&amp;rsquo;ll leave it for the future posts.&lt;/p>
&lt;p>I&amp;rsquo;ve packaged all dependencies for lambda into &lt;code>function.zip&lt;/code>. The contents of the archive are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>bb&lt;/code>: babashka binary&lt;/li>
&lt;li>&lt;code>bootstrap&lt;/code>: AWS Lambda entry point script&lt;/li>
&lt;li>&lt;code>cp&lt;/code>: generated classpath text file&lt;/li>
&lt;li>&lt;code>deps.edn&lt;/code>&lt;/li>
&lt;li>&lt;code>.gitlibs&lt;/code>: directory with gitlibs&lt;/li>
&lt;li>&lt;code>.m2&lt;/code>: directory with Maven dependencies&lt;/li>
&lt;li>&lt;code>resources&lt;/code>:&lt;/li>
&lt;li>&lt;code>src&lt;/code>: directory with babashka scripts&lt;/li>
&lt;/ul>
&lt;h2 id="custom-runtime-discoveries">Custom runtime discoveries&lt;/h2>
&lt;p>Finally, having all dependencies packaged up, I&amp;rsquo;ve deployed the &lt;code>function.zip&lt;/code> to AWS Lambda. The first error message was not very
&lt;a href="https://gist.github.com/dainiusjocas/feafeef5653ff2c6e8c7b2d9627a831d" target="_blank" rel="noopener">encouraging&lt;/a>:&lt;/p>
&lt;pre>&lt;code class="language-text">Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.create() failed. errno: 38 Function not implemented
Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed.
JavaFrameAnchor dump:
No anchors
TopFrame info:
TotalFrameSize in CodeInfoTable 32
VMThreads info:
VMThread 0000000003042750 STATUS_IN_JAVA (safepoints disabled) java.lang.Thread@0x264fa98
VM Thread State for current thread 0000000003042750:
0 (8 bytes): com.oracle.svm.jni.JNIThreadLocalEnvironment.jniFunctions = (bytes)
0000000003042750: 0000000002293a88
8 (32 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.regularTLAB = (bytes)
0000000003042758: 00007f7809500000 00007f7809600000
0000000003042768: 00007f7809507160 0000000000000000
40 (8 bytes): com.oracle.svm.core.heap.NoAllocationVerifier.openVerifiers = (Object) null
48 (8 bytes): com.oracle.svm.core.jdk.IdentityHashCodeSupport.hashCodeGeneratorTL = (Object) null
56 (8 bytes): com.oracle.svm.core.snippets.SnippetRuntime.currentException = (Object) null
64 (8 bytes): com.oracle.svm.core.thread.JavaThreads.currentThread = (Object) java.lang.Thread 000000000264fa98
72 (8 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.activeTimer = (Object) null
80 (8 bytes): com.oracle.svm.jni.JNIObjectHandles.handles = (Object) com.oracle.svm.core.handles.ThreadLocalHandles 00007f7809501558
88 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPendingException.pendingException = (Object) null
96 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPinnedObjects.pinnedObjectsListHead = (Object) null
104 (8 bytes): com.oracle.svm.jni.JNIThreadOwnedMonitors.ownedMonitors = (Object) null
112 (8 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.freeList = (Word) 0 0000000000000000
120 (8 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.stackBoundaryTL = (Word) 1 0000000000000001
128 (8 bytes): com.oracle.svm.core.stack.JavaFrameAnchors.lastAnchor = (Word) 0 0000000000000000
136 (8 bytes): com.oracle.svm.core.thread.VMThreads.IsolateTL = (Word) 25636864 0000000001873000
144 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadHandleTL = (Word) 50477184 0000000003023880
152 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadIdTL = (Word) 50477184 0000000003023880
160 (8 bytes): com.oracle.svm.core.thread.VMThreads.nextTL = (Word) 0 0000000000000000
168 (4 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.yellowZoneStateTL = (int) -16843010 fefefefe
172 (4 bytes): com.oracle.svm.core.snippets.ImplicitExceptions.implicitExceptionsAreFatal = (int) 0 00000000
176 (4 bytes): com.oracle.svm.core.thread.Safepoint.safepointRequested = (int) 2147473200 7fffd730
180 (4 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.currentPauseDepth = (int) 0 00000000
184 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.safepointsDisabledTL = (int) 1 00000001
188 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.statusTL = (int) 1 00000001
VMOperation dump:
No VMOperation in progress
Dump Counters:
Raw Stacktrace:
00007ffeb8e0a940: 000000000186e776 000000000207b9d0
00007ffeb8e0a950: 0000000001873000 000000000085b37c
00007ffeb8e0a960: 000000000084540a 00000000008454ca
00007ffeb8e0a970: 000000000264f128 000000000264ef58
00007ffeb8e0a980: 00007f78095018d8 0000000002650640
00007ffeb8e0a990: 000000000264f128 0000002602650c18
00007ffeb8e0a9a0: 0000000000845444 00007ffeb8e0a970
00007ffeb8e0a9b0: 0000000000000000 0000000000845f6e
00007ffeb8e0a9c0: 0000000002650e18 0000000002650c18
00007ffeb8e0a9d0: 0000000002650e18 0000000002070c60
00007ffeb8e0a9e0: 00000000021f48f8 00000000012b77e6
00007ffeb8e0a9f0: 0000000002650e18 0000000002650c18
00007ffeb8e0aa00: 0000001000000000 0000000002070c60
00007ffeb8e0aa10: 00007f7809507138 0000000000477f69
00007ffeb8e0aa20: 00007f7809503b88 00007f7809501910
00007ffeb8e0aa30: 00007f7809507138 00000000004831b4
00007ffeb8e0aa40: 0000000000000010 000000000085d16d
00007ffeb8e0aa50: 000000000000003b 00000000008b4bdb
00007ffeb8e0aa60: 000000000291e970 00007f7809504828
00007ffeb8e0aa70: 0000000100000007 0000000001079a70
00007ffeb8e0aa80: 00007f78095070b8 00007f7809507080
00007ffeb8e0aa90: 0000000001873000 000000000291e970
00007ffeb8e0aaa0: 00007f7809506f78 00007f78095070b8
00007ffeb8e0aab0: 0000000000000008 0000000000000010
00007ffeb8e0aac0: 0000000000000010 00000000008144a1
00007ffeb8e0aad0: 0000000000000007 0000000000cd7c2e
00007ffeb8e0aae0: 00007f7809504938 0000000001873000
00007ffeb8e0aaf0: 0000000002205088 00007f78095070b8
00007ffeb8e0ab00: 00007f7809507080 0000000cc0001000
00007ffeb8e0ab10: 0000000000000000 0000000000cd73eb
00007ffeb8e0ab20: 00007f7809503b58 00007f78095070b8
00007ffeb8e0ab30: 00007f7809507080 00007f78095038e0
00007ffeb8e0ab40: 00007f7807c8e388 000000000205e900
00007ffeb8e0ab50: 00007f7809501350 000000240000000c
00007ffeb8e0ab60: 000000000000000c 00007f78095038e0
00007ffeb8e0ab70: d15c483b00000000 00000000004830e5
00007ffeb8e0ab80: 0000000000000007 00007f78095038e0
00007ffeb8e0ab90: 00007f78095038e0 00000000006f2b33
00007ffeb8e0aba0: 000000000205e900 0000000002070448
00007ffeb8e0abb0: 00007f78095070b8 0000000000cd8b3d
00007ffeb8e0abc0: 00000000020864c8 0000000000cbffc1
00007ffeb8e0abd0: 0000000002070448 00007f78095070b8
00007ffeb8e0abe0: 0000000c00000000 00007f7809505ef8
00007ffeb8e0abf0: 00007f78095070d8 00007f7809504840
00007ffeb8e0ac00: 7cab467402070d98 0000000000fbfc08
00007ffeb8e0ac10: 0000000002634470 00007f7809507020
00007ffeb8e0ac20: 0000000001873000 00007f78095070d8
00007ffeb8e0ac30: 00007f7809504840 0000000000cc187e
00007ffeb8e0ac40: 0000000000000000 0000000000000000
00007ffeb8e0ac50: 00007f7807c91840 00007f7809504840
00007ffeb8e0ac60: 0000000002070d98 0000000000cc17b9
00007ffeb8e0ac70: 0000000000c848f0 00007f78095038e0
00007ffeb8e0ac80: 0000000002b33a78 0000000100cc4f83
00007ffeb8e0ac90: 0000000000483140 00000000004b5713
00007ffeb8e0aca0: 0000000002070d98 0000000000cdae9a
00007ffeb8e0acb0: 000000000209a600 00007f78095038e0
00007ffeb8e0acc0: 0000000002b33a78 000000000047c576
00007ffeb8e0acd0: 000000000209a600 000000000209a630
00007ffeb8e0ace0: 0000000002a1b8d8 0000000002a1b408
00007ffeb8e0acf0: 000000000209a600 00000000017acc23
00007ffeb8e0ad00: 0000000000000001 0000000000001000
00007ffeb8e0ad10: 0000000000000000 0000000000000000
00007ffeb8e0ad20: 0000000000000000 0000000000000000
00007ffeb8e0ad30: 0000000000000000 0000000000000000
Stacktrace Stage0:
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 FrameSize 32
RSP 00007ffeb8e0a960 RIP 000000000085b37c FrameSize 16
RSP 00007ffeb8e0a970 RIP 00000000008454ca FrameSize 80
RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e FrameSize 48
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 FrameSize 48
RSP 00007ffeb8e0aa20 RIP 0000000000477f69 FrameSize 32
RSP 00007ffeb8e0aa40 RIP 00000000004831b4 FrameSize 320
RSP 00007ffeb8e0ab80 RIP 00000000004830e5 FrameSize 32
RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 FrameSize 256
RSP 00007ffeb8e0aca0 RIP 00000000004b5713 FrameSize 48
RSP 00007ffeb8e0acd0 RIP 000000000047c576 FrameSize 160
RSP 00007ffeb8e0ad70 RIP 000000000047c285 FrameSize 32
RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 FrameSize 256
RSP 00007ffeb8e0ae90 RIP 000000000048f162 FrameSize 32
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c FrameSize 1
Stacktrace Stage1:
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a960 RIP 000000000085b37c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a970 RIP 00000000008454ca com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aa20 RIP 0000000000477f69 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aa40 RIP 00000000004831b4 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ab80 RIP 00000000004830e5 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aca0 RIP 00000000004b5713 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0acd0 RIP 000000000047c576 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ad70 RIP 000000000047c285 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ae90 RIP 000000000048f162 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
Full Stacktrace:
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.jdk.VMErrorSubstitutions.shutdown(VMErrorSubstitutions.java:111)
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:74)
RSP 00007ffeb8e0a960 RIP 000000000085b37c [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:59)
RSP 00007ffeb8e0a970 RIP 00000000008454ca [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.ensureInitialized(SunMiscSubstitutions.java:176)
RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.numberFromName(SunMiscSubstitutions.java:223)
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.findSignal(Signal.java:78)
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.&amp;lt;init&amp;gt;(Signal.java:140)
RSP 00007ffeb8e0aa20 RIP 0000000000477f69 [image code] babashka.impl.pipe_signal_handler$handle_pipe_BANG_.invokeStatic(pipe_signal_handler.clj:11)
RSP 00007ffeb8e0aa40 RIP 00000000004831b4 [image code] babashka.main$main.invokeStatic(main.clj:282)
RSP 00007ffeb8e0ab80 RIP 00000000004830e5 [image code] babashka.main$main.doInvoke(main.clj:282)
RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137)
RSP 00007ffeb8e0aca0 RIP 00000000004b5713 [image code] clojure.core$apply.invokeStatic(core.clj:665)
RSP 00007ffeb8e0acd0 RIP 000000000047c576 [image code] babashka.main$_main.invokeStatic(main.clj:442)
RSP 00007ffeb8e0ad70 RIP 000000000047c285 [image code] babashka.main$_main.doInvoke(main.clj:437)
RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137)
RSP 00007ffeb8e0ae90 RIP 000000000048f162 [image code] babashka.main.main(Unknown Source)
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.runCore(JavaMainWrapper.java:151)
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:186)
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.code.IsolateEnterStub.JavaMainWrapper_run_5087f5482cc9a6abc971913ece43acb471d2631b(IsolateEnterStub.java:0)
[Native image heap boundaries:
ReadOnly Primitives: 0x1873008 .. 0x206f048
ReadOnly References: 0x206ff78 .. 0x24fc9f8
Writable Primitives: 0x24fd000 .. 0x26343e0
Writable References: 0x2634470 .. 0x2ba42c0]
[Heap:
[Young generation:
[youngSpace:
aligned: 0/0 unaligned: 0/0]]
[Old generation:
[fromSpace:
aligned: 0/0 unaligned: 0/0]
[toSpace:
aligned: 0/0 unaligned: 0/0]
]
[Unused:
aligned: 0/0]]
Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed.
RequestId: 263ff1be-425d-4dcb-9ea5-67020dc3041b Error: Runtime exited with error: exit status 99
Runtime.ExitError
&lt;/code>&lt;/pre>
&lt;h2 id="the-fight">The fight&lt;/h2>
&lt;p>After some Googling I&amp;rsquo;ve discovered several related clues
&lt;a href="https://github.com/oracle/graal/issues/841" target="_blank" rel="noopener">here&lt;/a> and
&lt;a href="https://github.com/quarkusio/quarkus/issues/4262" target="_blank" rel="noopener">here&lt;/a>. They say that signals are not supported in AWS lambda. So, why not to disable signals for babashka and see what happens? I&amp;rsquo;ve forked the repo, made a flag that disables PIPE signal handling, deployed babashka to the
&lt;a href="https://hub.docker.com/r/dainiusjocas/babashka" target="_blank" rel="noopener">docker hub&lt;/a> and tried to deploy lambda once again.&lt;/p>
&lt;p>And? It worked:&lt;/p>
&lt;pre>&lt;code class="language-shell">make function-name=$(make get-function-name) invoke-function
=&amp;gt;
{&amp;quot;test&amp;quot;:&amp;quot;test914&amp;quot;}{
&amp;quot;StatusCode&amp;quot;: 200,
&amp;quot;ExecutedVersion&amp;quot;: &amp;quot;$LATEST&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>
&lt;a href="https://github.com/dainiusjocas/babashka-lambda" target="_blank" rel="noopener">Here&lt;/a> is the example of babashka script that can be deployed to AWS Lambda.&lt;/p>
&lt;ul>
&lt;li>The &lt;code>function.zip&lt;/code> weights just 18MB.&lt;/li>
&lt;li>The cold startup of the Lambda that is given 128MB of RAM is ~400ms. Subsequent calls ranges from 4ms and 120ms. The more RAM you give the faster lambda gets.&lt;/li>
&lt;li>I can develop the code in Cursive as the structure is like of an ordinary Clojure deps.edn project (and it can be used on the JVM).&lt;/li>
&lt;li>I made a
&lt;a href="https://github.com/borkdude/babashka/pull/305" target="_blank" rel="noopener">PR to babashka&lt;/a> and I&amp;rsquo;ve got accepted.&lt;/li>
&lt;/ul>
&lt;h2 id="next-steps">Next Steps&lt;/h2>
&lt;ul>
&lt;li>Fix Problem building on macos (&lt;code>/tmp&lt;/code> dir is not writable).&lt;/li>
&lt;li>Get rid of AWS CloudFormation part.&lt;/li>
&lt;li>Work a bit more to support AWS API Gateway.&lt;/li>
&lt;li>Create a template for such projects.&lt;/li>
&lt;/ul></description></item><item><title>Using Search Templates in Elasticsearch</title><link>https://www.jocas.lt/blog/post/on-search-templates/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/on-search-templates/</guid><description>&lt;p>I want to take a look at
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html" target="_blank" rel="noopener">Search Templates&lt;/a> for Elasticsearch. Let&amp;rsquo;s apply them to examples from
&lt;a href="https://www.jocas.lt/blog/post/synonym-graph-phrase-search/" target="_blank" rel="noopener">previous post on Synonym Graphs&lt;/a>.&lt;/p>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>I&amp;rsquo;m using Elasticsearch 7.5.1.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code>DELETE test_index-1
PUT /test_index-1
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;descrition&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
}
},
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonym_graph_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;my_synonym_graph&amp;quot;
]
},
&amp;quot;lowercase_keyword_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
],
&amp;quot;char_filter&amp;quot;: [
&amp;quot;spaces_to_undescores_filter&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;spaces_to_undescores_filter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;,
&amp;quot;mappings&amp;quot;: [
&amp;quot; \\u0020 =&amp;gt; _&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;my_synonym_graph&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym_graph&amp;quot;,
&amp;quot;lenient&amp;quot;: true,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;
]
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Index the document:&lt;/p>
&lt;pre>&lt;code>PUT test_index-1/_doc/1
{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>prefix very important thing suffix&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="templates">Templates&lt;/h2>
&lt;p>I&amp;rsquo;m very interested in one particular use of the search templates: how flexible is the management of stored seach templates? Can I update a search template while receiving queries?&lt;/p>
&lt;p>Add a template:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Try to run the search:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Exactly as expected.&lt;/p>
&lt;p>When using a stored search template the Elasticsearch client doesn&amp;rsquo;t need to handle the complex query construction.&lt;/p>
&lt;h2 id="templates-are-updateable">Templates are updateable&lt;/h2>
&lt;p>Let&amp;rsquo;s try to update the template with a higher boost value:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 5
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Works.&lt;/p>
&lt;p>Now let&amp;rsquo;s run the same query:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 4,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The scores are 0.5753642 and 1.4384103 that is ~2/5. Cool! This means that without changing (and redeploying) the Elasticsearch client we can change the querying logic, making the query an more dynamic.&lt;/p>
&lt;h2 id="corner-cases">Corner Cases&lt;/h2>
&lt;p>What if we run query has more attributes, e.g.:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;,
&amp;quot;new_attr&amp;quot;: &amp;quot;123&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>Works as expected!&lt;/p>
&lt;p>When &lt;code>query_string&lt;/code> is &lt;code>null&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: null
}
}
&lt;/code>&lt;/pre>
&lt;p>Works!&lt;/p>
&lt;p>What if the param is not provided:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;new_attr&amp;quot;: &amp;quot;value&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>No error!&lt;/p>
&lt;p>What if we provide a list instead of a string:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: [&amp;quot;this&amp;quot;, &amp;quot;Very Important Thing&amp;quot;]
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Instead of profiding one value we can replace it with a list. Good!&lt;/p>
&lt;h2 id="metadata-of-the-search-template">Metadata of the search template&lt;/h2>
&lt;p>It would be great to be able to store some metadata with the search template script, e.g. Git commit SHA of the query. I couldn&amp;rsquo;t find a way to do this. A workaround might be to &lt;code>_name&lt;/code> attribute of the query. E.g.:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;_name&amp;quot;: &amp;quot;GIT COMMIT SHA&amp;quot;,
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 5
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The response:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;GIT COMMIT SHA&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Not great but might be useful.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;ul>
&lt;li>Templates doesn&amp;rsquo;t support search index specification.&lt;/li>
&lt;li>Field names can be parameterized, this feature alows to start/stop using a new/old field.&lt;/li>
&lt;li>Search template can be tested in (even in production cluster) independently.&lt;/li>
&lt;li>We can run our query against
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-search-template.html" target="_blank" rel="noopener">multiple search templates&lt;/a>. Combine this with the Profile API and performance can be compared. Explain API also is supported.&lt;/li>
&lt;/ul></description></item><item><title>Phrase Search with Synonym Graph Token Filter in Elasticsearch</title><link>https://www.jocas.lt/blog/post/synonym-graph-phrase-search/</link><pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/synonym-graph-phrase-search/</guid><description>&lt;p>I&amp;rsquo;ve
&lt;a href="https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/" target="_blank" rel="noopener">written&lt;/a> that if you google for &lt;code>How can you match a long query text to a short text field?&lt;/code> you&amp;rsquo;re advised to use Elasticsearch Percolator. Today I&amp;rsquo;ll show an alternative way of solving the same problem with Elasticsearch.&lt;/p>
&lt;p>The main idea is to use
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/analysis-synonym-graph-tokenfilter.html" target="_blank" rel="noopener">Synonym Graph Token Filter&lt;/a> with some data preparation.&lt;/p>
&lt;h2 id="problem-statement">Problem Statement&lt;/h2>
&lt;p>Say that we learned how extract some entity from free form text with techniques such as NER, dictionary annotations, or some fancy Machine Learning. And when this entity is mentioned in the search query we want to boost documents that mention this entity. Also, say you&amp;rsquo;ve ruled out using Elasticsearch Percolator because it increases network latency because it requires additional call to Elasticsearch.&lt;/p>
&lt;p>For further discussion our unstructured text is going to be &lt;code>This description is about a Very Important Thing and something else.&lt;/code> and the extracted entity &lt;code>Very Important Thing&lt;/code>. Our test document looks like :&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>prefix very important thing suffix&lt;/code>&lt;/li>
&lt;li>&lt;code>prefix very important another thing suffix&lt;/code>&lt;/li>
&lt;li>&lt;code>prefix thing suffix&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>All examples are tested on Elasticsearch 7.5.1.&lt;/p>
&lt;h3 id="naive-setup">Naive Setup&lt;/h3>
&lt;p>Let&amp;rsquo;s create an index for our documents:&lt;/p>
&lt;pre>&lt;code>PUT /test_index-2
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;description&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Entity field is of type &lt;code>text&lt;/code> because we want it to be searchable. &lt;code>keyword&lt;/code> type won&amp;rsquo;t work because it does only exact matches and out query most likely will be longer than our entity string.&lt;/p>
&lt;p>Index our document:&lt;/p>
&lt;pre>&lt;code>PUT test_index-2/_doc/1
{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search the index with the query that mentions our &lt;code>very important thing&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.7260926,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.7260926,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Cool, we found what we we looking for.&lt;/p>
&lt;p>Let&amp;rsquo;s try another query, this time with a mention of &lt;code>very important another thing&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important another thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.7260926,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.7260926,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Oh, the results are the same as with the previous query despite the fact that we mention &lt;code>Another Thing&lt;/code> here. But it still might be OK because we matched all the terms of the entity.&lt;/p>
&lt;p>Let&amp;rsquo;s try another query:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Oh no, we still matched our &lt;code>Very Important Thing&lt;/code> while only &lt;code>thing&lt;/code> term is present in the query. But at least this time the score is lower than with previous twoqueries, 0.5753642 vs. 1.7260926. Here we clearly see the problem: we are matching short strings with long strings and partial matches raises problems.&lt;/p>
&lt;h2 id="proposed-solution">Proposed Solution&lt;/h2>
&lt;p>Let&amp;rsquo;s leverage Synonym Graph Token Filter to solve our problem.&lt;/p>
&lt;pre>&lt;code>PUT /test_index-1
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;descrition&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
}
},
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonym_graph_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;my_synonym_graph&amp;quot;
]
},
&amp;quot;lowercase_keyword_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
],
&amp;quot;char_filter&amp;quot;: [
&amp;quot;spaces_to_undescores_filter&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;spaces_to_undescores_filter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;,
&amp;quot;mappings&amp;quot;: [
&amp;quot; \\u0020 =&amp;gt; _&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;my_synonym_graph&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym_graph&amp;quot;,
&amp;quot;lenient&amp;quot;: true,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;
]
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s decompose this large index configuration piece by piece:&lt;/p>
&lt;ol>
&lt;li>The &lt;code>entity&lt;/code> attribute now has separate analyzers for both index and search phases.&lt;/li>
&lt;li>The &lt;code>lowercase_keyword_analyzer&lt;/code> uses keyword tokenizer which means that tokenization will result in the sequence of token of size 1, then it normalizes tokens by lowercasing them and finally &lt;code>spaces_to_undescores_filter&lt;/code>, replaces spaces to underscores. E.g. a string &lt;code>&amp;quot;Very Important Thing&amp;quot;&lt;/code> is transformed into list of tokens &lt;code>[&amp;quot;very_important_thing&amp;quot;]&lt;/code>. Or use out friend &lt;code>_analyze&lt;/code> API:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>POST test_index-1/_analyze
{
&amp;quot;text&amp;quot;: [&amp;quot;Very Important Thing&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;very_important_thing&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 20,
&amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
&amp;quot;position&amp;quot; : 0
}
]
}
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>The &lt;code>synonym_graph_analyzer&lt;/code> use standard tokenizer, which is followed by the &lt;code>lowercase&lt;/code> filter, and then the &lt;code>my_synonym_graph&lt;/code> token filter is applied. We&amp;rsquo;ve set up one synonym &lt;code>&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;&lt;/code>. E.g.&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>POST test_index-1/_analyze
{
&amp;quot;text&amp;quot;: [&amp;quot;prefix very important thing suffix&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;prefix&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 6,
&amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot; : 0
},
{
&amp;quot;token&amp;quot; : &amp;quot;very_important_thing&amp;quot;,
&amp;quot;start_offset&amp;quot; : 7,
&amp;quot;end_offset&amp;quot; : 27,
&amp;quot;type&amp;quot; : &amp;quot;SYNONYM&amp;quot;,
&amp;quot;position&amp;quot; : 1
},
{
&amp;quot;token&amp;quot; : &amp;quot;suffix&amp;quot;,
&amp;quot;start_offset&amp;quot; : 28,
&amp;quot;end_offset&amp;quot; : 34,
&amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot; : 2
}
]
}
&lt;/code>&lt;/pre>
&lt;p>After analysis we have 3 tokens &lt;code>[&amp;quot;prefix&amp;quot;, &amp;quot;very_important_thing&amp;quot;, &amp;quot;suffix&amp;quot;]&lt;/code>. Notice &lt;code>&amp;quot;very_important_thing&amp;quot;&lt;/code> token: this is equal to the right-hand-side from our synonym definitions. Now let&amp;rsquo;s run queries from the previous section:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: exact match -&amp;gt; hit.&lt;/p>
&lt;p>Another query:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important another thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 0,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits! Good! The document is not going to be boosted despite the fact that all tokens match.&lt;/p>
&lt;p>And the last one:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits! Good. This means that also substring doesn&amp;rsquo;t match.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>Synonym Graph Token Filter can &amp;ldquo;replace&amp;rdquo; a sequence of tokens (e.g. a phrase) with another sequence of tokens. In this particular example: many tokens were replaced with one token.&lt;/p>
&lt;ol>
&lt;li>One field can have only one analyzer pair for index and search phases. If we want another analysis pipeline for the &lt;code>entity&lt;/code> attribute we have to create another field with the analyzers specified, e.g. stemmed phrase with lower boost.&lt;/li>
&lt;li>The synonym list must be prepared before the index creation.&lt;/li>
&lt;li>Management of the synonym list might complicate index management, e.g. you use templates for your index management.&lt;/li>
&lt;li>The overal solution in general might look a bit too complicated.&lt;/li>
&lt;/ol></description></item><item><title>Elasticsearch Percolator and Text Analyzers</title><link>https://www.jocas.lt/blog/post/percolator-phrase-analyzers/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/percolator-phrase-analyzers/</guid><description>&lt;p>This time I need to percolate texts with different analyzers for index and search analyzers.&lt;/p>
&lt;p>Let&amp;rsquo;s elaborate a bit on
&lt;a href="https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/" target="_blank" rel="noopener">previous article&lt;/a> and explicitly declare analyzers to use.&lt;/p>
&lt;p>Define index:&lt;/p>
&lt;pre>&lt;code>PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Then define 2 slightly different percolator queries (notice the difference between &lt;code>&amp;quot;bonsai tree&amp;quot;&lt;/code> and &lt;code>&amp;quot;bonsai, tree&amp;quot;&lt;/code>).&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
PUT /my-index/_doc/2?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s percolate:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 80,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
},
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: 2 documents matched.&lt;/p>
&lt;p>But now lets change the analyzer of the second percolation query to &lt;code>whitespace&lt;/code>:&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/2?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;whitespace&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Run the percolator:&lt;/p>
&lt;pre>&lt;code>
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 5,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: only 1 percolator query matched our input.&lt;/p>
&lt;h2 id="phrases-with-stopwords">Phrases with Stopwords&lt;/h2>
&lt;p>Say, we have a phrase &lt;code>&amp;quot;bonsai is tree&amp;quot;&lt;/code> and we percolate text &lt;code>A new bonsai in tree in the office&lt;/code> with the &lt;code>standard&lt;/code> analyzer for indexing and &lt;code>english&lt;/code> for search analyzer. There should be no matches. Let&amp;rsquo;s try:&lt;/p>
&lt;pre>&lt;code>DELETE my-index
PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;english&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai is tree&amp;quot;
}
}
}
}
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai in tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>And, surprisingly, this yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 2,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai is tree&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>We have a match! Also notice that the highlighter is broken!&lt;/p>
&lt;p>The problem that these two analyzers have different stopword lists (no stopwords for &lt;code>standard&lt;/code> and several English stopwords for &lt;code>english&lt;/code> analyzer) and the phrase contains a stopword that is not shared between analyzers.&lt;/p>
&lt;p>Let&amp;rsquo;s fix this surprise with
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html#search-quote-analyzer" target="_blank" rel="noopener">&lt;code>search_quote_analyzer&lt;/code>&lt;/a>.&lt;/p>
&lt;pre>&lt;code>DELETE my-index
PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;english&amp;quot;,
&amp;quot;search_quote_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai is tree&amp;quot;
}
}
}
}
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai in tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits, as expected.&lt;/p>
&lt;p>Let&amp;rsquo;s check if the expected behaviour is still there:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai is tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 4,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.39229375,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.39229375,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai is tree&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai is tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Good. Even the highlighting works.&lt;/p></description></item><item><title>Phrase Highlighting with the Elasticsearch Percolator</title><link>https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/</guid><description>&lt;p>If you google &lt;code>How can you match a long query text to a short text field?&lt;/code> it will point you to the
&lt;a href="https://stackoverflow.com/questions/51865747/elasticsearch-match-long-query-text-to-short-field" target="_blank" rel="noopener">Stack Overflow page&lt;/a>
&lt;a href="https://discuss.elastic.co/t/match-long-query-text-to-short-field/144584/3" target="_blank" rel="noopener">or here&lt;/a> where the answer is to use
&lt;a href="">Elasticsearch Percolator&lt;/a>.&lt;/p>
&lt;p>My search items are phrases meaning that it should match all terms in order. Let&amp;rsquo;s create a sample setup in Kibana (v7.5) Dev dashboard.&lt;/p>
&lt;ol>
&lt;li>Create an index for percolation:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note on &lt;code>&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;&lt;/code>: this allows
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.6/search-request-highlighting.html#fast-vector-highlighter" target="_blank" rel="noopener">Fast Vector Highlighter&lt;/a> to highlight combined phrase not just separate qeury terms.&lt;/p>
&lt;ol start="2">
&lt;li>Store one phrase query:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;bonsai tree&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>Percolate a document:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note on &lt;code>&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;&lt;/code>: this instructs Elasticsearch to use the Fast Vector Highlighter.&lt;/p>
&lt;p>The query yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 23,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : &amp;quot;bonsai tree&amp;quot;
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As we see highlighter correctly marker the search phrase.&lt;/p>
&lt;h2 id="storing-additional-data-with-percolator-queries">Storing additional data with percolator queries&lt;/h2>
&lt;p>Percolation result can be used to connect pieces of information in your system, e.g. store a &lt;code>subscriber_email&lt;/code> attribute of the user that wants to be notified when the query matches along with the percolator query.&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;bonsai tree&amp;quot;
}
},
&amp;quot;subscriber_email&amp;quot;: &amp;quot;subscriber_email@example.com&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Then query:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This query yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 10,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : &amp;quot;bonsai tree&amp;quot;
}
},
&amp;quot;subscriber_email&amp;quot; : &amp;quot;subscriber_email@example.com&amp;quot;
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Now, take the email under the &lt;code>&amp;quot;subscriber_email&amp;quot;&lt;/code> from the response and send an email with the highlight.&lt;/p></description></item><item><title>Using Uberdeps to Build AWS Lambda Uberjar</title><link>https://www.jocas.lt/blog/post/uberdeps-for-aws-lambda/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/uberdeps-for-aws-lambda/</guid><description>&lt;p>I was writing a Clojure application and the plan was to deploy it as a AWS Lambda. The question I&amp;rsquo;m going to answer in this blog post is: how to build an uberjar for AWS Lambda with
&lt;a href="https://github.com/tonsky/uberdeps" target="_blank" rel="noopener">Uberdeps&lt;/a>?&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>Add an alias to the &lt;code>deps.edn&lt;/code> for uberjar building:&lt;/p>
&lt;pre>&lt;code>{:aliases {:uberjar
{:extra-deps {uberdeps {:mvn/version &amp;quot;0.1.6&amp;quot;}}
:main-opts [&amp;quot;-m&amp;quot; &amp;quot;uberdeps.uberjar&amp;quot;]}}}
&lt;/code>&lt;/pre>
&lt;p>Create an executable file &lt;code>compile.clj&lt;/code> in the project root folder:&lt;/p>
&lt;pre>&lt;code class="language-bash">touch compile.clj
chmod +x compile.clj
&lt;/code>&lt;/pre>
&lt;p>Put this code in the &lt;code>compile.clj&lt;/code> file:&lt;/p>
&lt;script src="https://gist.github.com/dainiusjocas/e9b154d7a1cbdca8558cd7c5d730d5d0.js">&lt;/script>
&lt;p>Run:&lt;/p>
&lt;pre>&lt;code class="language-bash">(rm -rf classes &amp;amp;&amp;amp; \
mkdir classes &amp;amp;&amp;amp; \
./compile.clj &amp;amp;&amp;amp; \
clojure -A:uberjar --target target/UBERJAR_NAME.jar)
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;d advise put that last script into a &lt;code>Makefile&lt;/code> ;)&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>To deploy your Clojure code to AWS Lambda you need to package it as an uberjar. If your project is managed with &lt;code>deps.edn&lt;/code>, basically you&amp;rsquo;re on your own to find a suitable library to package your code.&lt;/p>
&lt;p>For some time to build uberjars for &lt;code>deps.edn&lt;/code> projects I was using
&lt;a href="https://github.com/luchiniatwork/cambada" target="_blank" rel="noopener">Cambada&lt;/a>. It did the job but I was not entirely happy with the library for a couple of reasons:&lt;/p>
&lt;ul>
&lt;li>the library seems to be no longer maintained;&lt;/li>
&lt;li>it has various
&lt;a href="https://github.com/luchiniatwork/cambada/issues" target="_blank" rel="noopener">bugs&lt;/a> with transitive Git dependencies. I&amp;rsquo;ve found out that these bugs are fixed in a
&lt;a href="https://github.com/xfthhxk/cambada" target="_blank" rel="noopener">fork&lt;/a> of the Cambada and I used it as a git dependency.&lt;/li>
&lt;/ul>
&lt;p>Because building an uberjar for &lt;code>deps.edn&lt;/code> boils down to just finding a library there is always temptation to try something new.&lt;/p>
&lt;h2 id="enter-uberdeps">Enter Uberdeps&lt;/h2>
&lt;p>For my toy project I wanted to try out
&lt;a href="https://github.com/tonsky/uberdeps" target="_blank" rel="noopener">Uberdeps&lt;/a>. The introduction
&lt;a href="https://tonsky.me/blog/uberdeps/" target="_blank" rel="noopener">blog post&lt;/a> got me interested and I really liked the main idea:&lt;/p>
&lt;blockquote>
&lt;p>Takes deps.edn and packs an uberjar out of it.&lt;/p>
&lt;/blockquote>
&lt;p>Sounds like exactly what I need.&lt;/p>
&lt;h2 id="trouble">Trouble&lt;/h2>
&lt;p>I&amp;rsquo;ve written my application, added all the things needed to deploy it as an AWS Lambda, build an uberjar with Uberdeps, deployed the app with the AWS CloudFormation, but when I&amp;rsquo;ve invoked the Lambda I&amp;rsquo;ve received an error:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;message&amp;quot; : &amp;quot;Internal server error&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>After searching through the AWS CloudWatch logs I&amp;rsquo;ve found:&lt;/p>
&lt;pre>&lt;code>Class not found: my.Lambda: java.lang.ClassNotFoundException
java.lang.ClassNotFoundException: my.Lambda
at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>my.Lambda&lt;/code> class was not found.&lt;/p>
&lt;p>After taking a look at the contents of the uberjar I&amp;rsquo;ve noticed that the &lt;code>my.Lambda&lt;/code> class is indeed not inside the Uberjar. Ah, it seems that AOT (Ahead-of-Time) is not done out of the box. After searching and not finding a flag or some parameter that I need to pass to force the AOT compilation in the Uberdeps README, I&amp;rsquo;ve discovered an already closed
&lt;a href="https://github.com/tonsky/uberdeps/pull/11" target="_blank" rel="noopener">pull request&lt;/a>: the AOT compilation functionality is not implemented.&lt;/p>
&lt;p>I was in trouble.&lt;/p>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;p>The solution was to manually perform AOT compilation of the relevant namespaces right before building an uberjar and then instruct Uberdeps to put the resulting class files into the uberjar.&lt;/p>
&lt;p>To do AOT compilation I&amp;rsquo;ve written a Clojure script &lt;code>compile.clj&lt;/code>:&lt;/p>
&lt;script src="https://gist.github.com/dainiusjocas/e9b154d7a1cbdca8558cd7c5d730d5d0.js">&lt;/script>
&lt;p>Inspiration on how to write the script was taken from
&lt;a href="https://www.reddit.com/r/Clojure/comments/8ltsrs/standalone_script_with_clj_including_dependencies/" target="_blank" rel="noopener">here&lt;/a> and
&lt;a href="https://github.com/tonsky/datascript/blob/master/release.clj" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>To instruct Uberdeps to put class files to the uberjar I&amp;rsquo;ve added &lt;code>classes&lt;/code> directory to the &lt;code>:paths&lt;/code> vector in &lt;code>deps.edn&lt;/code>.&lt;/p>
&lt;p>Just for the convenience, in the Makefile I&amp;rsquo;ve put commands for AOT compilation right before the command to build an uberjar:&lt;/p>
&lt;pre>&lt;code>uberjar:
rm -rf classes
mkdir classes
./compile.clj
clojure -A:uberjar --target target/my-jar-name.jar
&lt;/code>&lt;/pre>
&lt;p>And that is it! I have an uberjar with &lt;code>my.Lambda&lt;/code> class and the AWS Lambda runtime is happy.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>The solution is not bullet proof because:&lt;/p>
&lt;ul>
&lt;li>it assumes that the main &lt;code>deps.end&lt;/code> file is called &lt;code>deps.edn&lt;/code>;&lt;/li>
&lt;li>compiled classes are put in the &lt;code>classes&lt;/code> directory;&lt;/li>
&lt;li>the alias for which namespaces should be AOT compiled is the default alias.&lt;/li>
&lt;/ul>
&lt;p>I hope that when a more generic solution will be needed either the Uberdeps will have an option for AOT compilatoin or I&amp;rsquo;ll be clever enough to deal with the situation and write a follow up blog post with the workaround.&lt;/p></description></item><item><title>Using Gitlab CI Cache for Clojure Dependencies</title><link>https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/</link><pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/</guid><description>&lt;p>I want to share my hard-won lessons on how to setup the Gitlab CI for Clojure projects based on tools.deps. I think that the Gitlab CI is a wonderful tool for CI workloads. But when you&amp;rsquo;re going a bit sideways from the documented ways of doing things you have to do a bit of discovery for yourself.&lt;/p>
&lt;h2 id="gitlab-ci-cachesetup">Gitlab CI CacheÂ Setup&lt;/h2>
&lt;p>Usually I want to cache dependencies between all build and all branches. To achieve this I hard-code the cache key at the root of theÂ &lt;code>.gitlab-ci.yml&lt;/code> file e.g.:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
&lt;/code>&lt;/pre>
&lt;p>When it comes to caching Clojure dependencies we have to be aware that there different types of dependencies. Two most common ones are: Maven and gitlibs.&lt;/p>
&lt;p>The Gitlab CI cache works &lt;strong>only&lt;/strong> with directories &lt;strong>inside the project directory&lt;/strong>. While local repositories (i.e. cache) for Clojure dependencies &lt;strong>by default&lt;/strong> are stored &lt;strong>outside the project directory&lt;/strong> (&lt;code>~/.m2&lt;/code> and &lt;code>~/.gitlibs&lt;/code>). Therefore, we have to provide parameters for our build tool to change the default directories for storing the dependencies.&lt;/p>
&lt;p>To specify Maven local repository we can provideÂ &lt;code>:mvn/local-repo&lt;/code> parameter e.g.:&lt;/p>
&lt;pre>&lt;code class="language-yaml">clojure -Sdeps '{:mvn/local-repo &amp;quot;./.m2/repository&amp;quot;}' -A:test
&lt;/code>&lt;/pre>
&lt;p>Having configured local maven repository in our &lt;code>gitlab-ci.yml&lt;/code> we can specify:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
paths:
- ./.m2/repository
&lt;/code>&lt;/pre>
&lt;p>When it comes to gitlibs there is no public API for changing the default directory in &lt;code>tools.deps&lt;/code>. But the underlying &lt;code>tools.gitlibs&lt;/code> uses an environment variable to set where to store the
&lt;a href="https://github.com/clojure/tools.gitlibs/blob/b7acb151b97952409103094794f5fc6f4d7d3840/src/main/clojure/clojure/tools/gitlibs.clj#L23" target="_blank" rel="noopener">gitlibs conveniently named &lt;strong>GITLIBS&lt;/strong>&lt;/a>. E.g.&lt;/p>
&lt;pre>&lt;code class="language-bash">$ (export GITLIBS=&amp;quot;.gitlibs/&amp;quot; &amp;amp;&amp;amp; clojure -A:test)
&lt;/code>&lt;/pre>
&lt;p>Of course, we should not forget to configure the cache:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
paths:
- ./.gitlibs
&lt;/code>&lt;/pre>
&lt;p>To use caching for both types of dependencies:&lt;/p>
&lt;pre>&lt;code class="language-bash">(export GITLIBS=&amp;quot;.gitlibs/&amp;quot; &amp;amp;&amp;amp; clojure -Sdeps '{:mvn/local-repo &amp;quot;./.m2/repository&amp;quot;}' -A:test)
&lt;/code>&lt;/pre>
&lt;p>And setup the cache:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
paths:
- ./.m2/repository
- ./.gitlibs
&lt;/code>&lt;/pre>
&lt;p>If you want to disable cache for a particular job (e.g. you&amp;rsquo;re linting with
&lt;a href="https://github.com/borkdude/clj-kondo" target="_blank" rel="noopener">clj-kondo&lt;/a>, which is delivered as a
&lt;a href="https://www.graalvm.org/" target="_blank" rel="noopener">GraalVM&lt;/a> compiled
&lt;a href="https://www.graalvm.org/docs/reference-manual/native-image/" target="_blank" rel="noopener">native image&lt;/a>), just give an empty map for a job&amp;rsquo;s cache setup, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-yaml">lint:
stage: test
image: borkdude/clj-kondo
cache: {}
when: always
script:
- clj-kondo --lint src test
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;ve used the Gitlab CI cache while working on a streaming-text search library
&lt;a href="https://github.com/tokenmill/beagle" target="_blank" rel="noopener">Beagle&lt;/a>. A fullÂ .gitlab-ci.yml file example of the setup can be found
&lt;a href="https://github.com/tokenmill/beagle/blob/master/.gitlab-ci.yml" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>Hope this helps!&lt;/p></description></item><item><title>Clojure Workflow @ TokenMill</title><link>https://www.jocas.lt/blog/talk/vilnius-clojure-meetup/</link><pubDate>Thu, 30 May 2019 19:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/talk/vilnius-clojure-meetup/</guid><description>&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/1Qto0UWEglVdpB" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> &lt;/iframe> &lt;div style="margin-bottom:5px"> &lt;strong> &lt;a href="//www.slideshare.net/Dainius/clojure-workflow-tokenmill-by-dainius-jocas" title="Clojure workflow @ TokenMill by Dainius Jocas" target="_blank">Clojure workflow @ TokenMill by Dainius Jocas&lt;/a> &lt;/strong>&lt;/div>
&lt;p>The source code of the demo project can be found
&lt;a href="https://github.com/dainiusjocas/clojure-meetup-vilnius-2019-05-30" target="_blank" rel="noopener">here&lt;/a>.&lt;/p></description></item></channel></rss>