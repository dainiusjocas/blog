<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Elasticsearch | Dainius Jocas</title><link>https://www.jocas.lt/blog/categories/elasticsearch/</link><atom:link href="https://www.jocas.lt/blog/categories/elasticsearch/index.xml" rel="self" type="application/rss+xml"/><description>Elasticsearch</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2023 Dainius Jocas</copyright><lastBuildDate>Sun, 29 Jan 2023 00:00:00 +0000</lastBuildDate><image><url>img/map[gravatar:%!s(bool=false) shape:circle]</url><title>Elasticsearch</title><link>https://www.jocas.lt/blog/categories/elasticsearch/</link></image><item><title>Match the Entire Text with a Slop in Elasticsearch</title><link>https://www.jocas.lt/blog/post/elasticsearch-full-phrase-with-slop/</link><pubDate>Sun, 29 Jan 2023 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/elasticsearch-full-phrase-with-slop/</guid><description>&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>Recently I&amp;rsquo;ve done a little Elasticsearch-based demo where there was one requirement I believe is worth sharing: the entire text should match even if two words are swapped.
This post is divided into two parts: requirements for a demo, and several implementation options.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>A simplified version of relevant requirements goes something like this:&lt;/p>
&lt;ol>
&lt;li>Documents are chat app message bodies,&lt;/li>
&lt;li>Match a message body with other messages even if any two words were swapped.&lt;/li>
&lt;/ol>
&lt;p>E.g. a message &lt;code>this is my message&lt;/code> should match &lt;code>is this my message&lt;/code> and vice versa.&lt;/p>
&lt;p>At first, this looks like nothing fancy, just set up an &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html">analyzer&lt;/a> with the required &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html%60">token filters&lt;/a> and do a &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query-phrase.html">phrase_match query&lt;/a> with a &lt;code>slop=2&lt;/code>.&lt;/p>
&lt;p>But then comes the tricky bit: we also need to take into account the &lt;strong>length of the message&lt;/strong>.
This is because a &lt;code>phrase_match&lt;/code> with a short message will match longer messages.
E.g. We don&amp;rsquo;t want that the query &lt;code>is this my message&lt;/code> would match &lt;code>this is my message which is longer&lt;/code>.&lt;/p>
&lt;h2 id="implementation-strategies">Implementation strategies&lt;/h2>
&lt;p>We&amp;rsquo;ll discuss two implementation strategies: fingerprinting based on the token count and a clever trick leveraging the text analysis pipeline.&lt;/p>
&lt;h3 id="fingerprint">Fingerprint&lt;/h3>
&lt;p>An instinctive approach is to &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/fingerprint-processor.html">fingerprint&lt;/a> the message text.
The simplest fingerprint could just be the count of tokens.
Elasticsearch conveniently offers a &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/token-count.html">token count field type&lt;/a>.&lt;/p>
&lt;p>One serious downside of this approach is that at the query time we&amp;rsquo;d have to get the count of tokens of the query.
This means either one additional round-trip to Elasticsearch to get the token count, e.g. using the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html">&lt;code>_analyze&lt;/code> API&lt;/a> or use of a &lt;code>script&lt;/code> in the query.
Let&amp;rsquo;s rule out the round trip to the &lt;code>_analyze&lt;/code> API approach for the demo.
Then how to implement the &lt;code>script&lt;/code> to get query token count?
The &lt;a href="https://chat.openai.com/chat#">Chat GPT&lt;/a> gave me a hint at a possible &lt;code>script&lt;/code> based solution :)&lt;/p>
&lt;p>&lt;img src="query-token-count.png" alt="Script">&lt;/p>
&lt;p>A significant problem with the above approach is that the text is split by a whitespace character (or something that &lt;a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#split%2Djava.lang.CharSequence%2D">&lt;code>split&lt;/code>&lt;/a> accepts).
Any seasoned search engineer immediately notices that the text is not tokenized by the same analyzer that was used to tokenize the text during the index-time for the token counting.
Of, course we could set up both &lt;code>tokenizers&lt;/code> to work the same way, but that approach seems somewhat fragile because part of the logic is in the index mapping and the other part is in the script that lives in the query constructor which is probably inside your application.&lt;/p>
&lt;h3 id="clever-text-analysis">Clever Text Analysis&lt;/h3>
&lt;p>We could also be more clever about how to ensure that the length of the matched texts is equal.
My strategy is to require both conditions to be true:&lt;/p>
&lt;ol>
&lt;li>Match the text &lt;strong>from the beginning of the string&lt;/strong> with a &lt;code>phrase_match&lt;/code> query with a slop=2.&lt;/li>
&lt;li>Match the &lt;strong>reversed&lt;/strong> text from the beginning of the string with a &lt;code>phrase_match&lt;/code> query with a slop=2.&lt;/li>
&lt;/ol>
&lt;p>The strategy requires us to solve 2 puzzles:&lt;/p>
&lt;ol>
&lt;li>How to match exactly from the beginning of the text?&lt;/li>
&lt;li>How to reverse the text?&lt;/li>
&lt;/ol>
&lt;p>To ensure the matching from the beginning we could insert a synthetic &lt;code>PREFIX&lt;/code> token at the position 0.&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;p>For the text reversal the idea is to:&lt;/p>
&lt;ol>
&lt;li>Use the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-keyword-tokenizer.html">keyword tokenizer&lt;/a>&lt;/li>
&lt;li>Use the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-reverse-tokenfilter.html">reverse&lt;/a> token filter on the entire text,&lt;/li>
&lt;li>Split the text into tokens using the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-word-delimiter-graph-tokenfilter.html#analysis-word-delimiter-graph-tokenfilter">word delimiter graph token filter&lt;/a>.&lt;/li>
&lt;li>Add a synthetic token at the position 0.&lt;/li>
&lt;/ol>
&lt;p>Let&amp;rsquo;s work out an example from requirements: &lt;code>this is my message&lt;/code> should match &lt;code>is this my message&lt;/code> but should not match &lt;code>this is my message which is longer&lt;/code>.&lt;/p>
&lt;p>The &lt;code>messages&lt;/code> index config&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;settings&amp;quot;: {
&amp;quot;number_of_replicas&amp;quot;: 0,
&amp;quot;number_of_shards&amp;quot;: 1,
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;normal_direction_analyzer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;token_splitter&amp;quot;,
&amp;quot;prefixer&amp;quot;
]
},
&amp;quot;backwards_direction_analyzer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;reverse&amp;quot;,
&amp;quot;token_splitter&amp;quot;,
&amp;quot;prefixer&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;token_splitter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;word_delimiter_graph&amp;quot;,
&amp;quot;split_on_case_change&amp;quot;: false,
&amp;quot;split_on_numerics&amp;quot;: false
},
&amp;quot;inject_prefix&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;^(.*)$&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;PREFIX $1&amp;quot;
},
&amp;quot;prefixer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;condition&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;inject_prefix&amp;quot;,
&amp;quot;token_splitter&amp;quot;
],
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.getPosition() == 0&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;body&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;normal_direction&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;normal_direction_analyzer&amp;quot;
},
&amp;quot;backwards_direction&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;backwards_direction_analyzer&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s index 2 documents:&lt;/p>
&lt;pre>&lt;code class="language-text">POST _bulk?refresh=true
{ &amp;quot;index&amp;quot; : { &amp;quot;_index&amp;quot; : &amp;quot;messages&amp;quot;, &amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot; } }
{ &amp;quot;body&amp;quot;: &amp;quot;this is my message&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_index&amp;quot; : &amp;quot;messages&amp;quot;, &amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot; } }
{ &amp;quot;body&amp;quot;: &amp;quot;this is my message which is longer&amp;quot; }
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s query the &lt;code>messages&lt;/code> index:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;query&amp;quot;: {
&amp;quot;bool&amp;quot;: {
&amp;quot;must&amp;quot;: [
{
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;body.normal_direction&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;is this my message&amp;quot;,
&amp;quot;slop&amp;quot;: 2
}
}
},
{
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;body.backwards_direction&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;is this my message&amp;quot;,
&amp;quot;slop&amp;quot;: 2
}
}
}
],
&amp;quot;_name&amp;quot;: &amp;quot;full phrase with accounted length&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The response:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot;: 2,
&amp;quot;timed_out&amp;quot;: false,
&amp;quot;_shards&amp;quot;: {
&amp;quot;total&amp;quot;: 1,
&amp;quot;successful&amp;quot;: 1,
&amp;quot;skipped&amp;quot;: 0,
&amp;quot;failed&amp;quot;: 0
},
&amp;quot;hits&amp;quot;: {
&amp;quot;total&amp;quot;: {
&amp;quot;value&amp;quot;: 1,
&amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot;: 0.46947598,
&amp;quot;hits&amp;quot;: [
{
&amp;quot;_index&amp;quot;: &amp;quot;messages&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 0.46947598,
&amp;quot;_source&amp;quot;: {
&amp;quot;body&amp;quot;: &amp;quot;this is my message&amp;quot;
},
&amp;quot;matched_queries&amp;quot;: [
&amp;quot;full phrase with accounted length&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The response is exactly as expected: it does match the shorter but not the longer message body.&lt;/p>
&lt;p>Downsides of the approach:&lt;/p>
&lt;ul>
&lt;li>The text is indexed 2 times. But it is acceptable to index and analyze the text in multiple ways, e.g. stemming.&lt;/li>
&lt;li>Analyzers got somewhat complicated. But the complicated bit is isolated only for dealing with the first token. Also, a couple of examples with the &lt;code>_analyze&lt;/code> API should make the analysis pipeline understandable.&lt;/li>
&lt;li>The total text length is limited to the &lt;code>Integer.MAX_VALUE&lt;/code> which is 2147483647 characters. But ~2 GB per message should be enough.&lt;/li>
&lt;li>&amp;ldquo;Tokenization&amp;rdquo; is done with the &lt;code>word_delimiter_graph&lt;/code>. But it is a standard &lt;a href="https://lucene.apache.org">Lucene&lt;/a> feature that you should learn anyway.&lt;/li>
&lt;li>Also, all the gotchas of the &lt;code>slop&lt;/code> are relevant, e.g. if one token was dropped/added from/to the query, then there still would be a match.&lt;/li>
&lt;li>I&amp;rsquo;d suggest you also should take extra care in handling the non-alphanumeric symbols.&lt;/li>
&lt;li>Also, the setup might produce surprising results for shorter (e.g. 1-5 words) strings.&lt;/li>
&lt;/ul>
&lt;p>A nice thing is that this solution is contained within the text analysis pipeline: no ingest pipelines, no scripting in queries, etc.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>In this post we&amp;rsquo;ve defined a problem of matching entire text with some flexibility in terms of token position changes.
We&amp;rsquo;ve discussed 2 approaches: fingerprinting and leveraging text analysis pipeline to account for the text length.
IMO, both approaches are somewhat hacky and have their downsides.
I&amp;rsquo;ve picked to work out the text analysis approach, and I&amp;rsquo;ve got my demo done.&lt;/p>
&lt;p>Let me know how you would approach this problem in the comments below.&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Of course, I&amp;rsquo;ve been thinking about leveraging &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-span-first-query.html">&lt;code>span_first&lt;/code>&lt;/a> query, but it requires another inner term level span query.
Which would make the query construction complicated because tokenization should be done inside your application. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>lowercasing and ascii-folding are omitted for brevity. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Tricks with Elasticsearch Completions Suggesters</title><link>https://www.jocas.lt/blog/post/tricks-with-elasticsearch-completion-suggesters/</link><pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/tricks-with-elasticsearch-completion-suggesters/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>Elasticsearch text analyzers can supercharge search suggesters.&lt;/p>
&lt;h2>Table of Contents&lt;/h2>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#tldr">TL;DR&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#requirements">Requirements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#implementation">Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#baseline-suggestions-setup">Baseline suggestions setup&lt;/a>&lt;/li>
&lt;li>&lt;a href="#suggest-from-the-second-word">Suggest from the second word&lt;/a>&lt;/li>
&lt;li>&lt;a href="#suggest-from-the-second-entity">Suggest from the second entity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#suggest-from-the-last-word">Suggest from the last word&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bonus-1-synonyms-for-suggestions">Bonus 1: Synonyms for suggestions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bonus-2-shingles-for-suggestions">Bonus 2: shingles for suggestions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#fin">Fin&lt;/a>&lt;/li>
&lt;li>&lt;a href="#footnotes">Footnotes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>So, you are a &lt;a href="https://opensourceconnections.com/blog/2020/07/16/what-is-a-relevance-engineer/">search engineer&lt;/a> that happily uses &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html#completion-suggester">Elasticsearch Completion Suggester&lt;/a> feature: lightning speed prefix suggestions works just like a charm&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>But one day the product manager comes to you with a requirement: &lt;code>could we also suggest if users start typing a word from the middle of the suggested string?&lt;/code>. Of course, the deadline is yesterday, as always. On top he adds that he doesn&amp;rsquo;t care whether that makes sense from search engineers perspective, it must be done&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. You try to argue that it will take forever to change upstream indexing pipeline because it is owned by another department of your company. PM doesn&amp;rsquo;t blink.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>Your app suggests artist names. The problematic artist currently rocking in the charts and attracting a lot of attention is &lt;code>Britney Spears &amp;amp; Elton John&lt;/code>. The PM specifies that it is needed that artists name and surname should be the source of suggestions. This means and that all &lt;code>b&lt;/code>, &lt;code>s&lt;/code>, &lt;code>e&lt;/code>, &lt;code>j&lt;/code> should suggest that artist.&lt;/p>
&lt;p>The actual song
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8hLtlzkoGPk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
.&lt;/p>
&lt;h2 id="implementation">Implementation&lt;/h2>
&lt;p>Generating multiple strings upstream is not an option due to the time constraints (because in a week the song will not be in the charts anymore). Also, the team discards generation of multiple strings using an &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest-processors.html">ingest processor&lt;/a> because nobody in our team likes to work with them. In the anemic Elasticsearch &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html#_parameters_for_completion_fields_2">documentation&lt;/a> there is a hint that using text analyzers (stopwords are mentioned) it is possible to achieve different entry points for suggestions which sounds like the trick that might work for our case. You decide to go to the rabbit hole of the analyzers.&lt;/p>
&lt;p>After the technical refinement meeting the notes on implementation of use cases look like:&lt;/p>
&lt;ul>
&lt;li>&lt;code>b&lt;/code> -&amp;gt; the classic use case which already works;&lt;/li>
&lt;li>&lt;code>s&lt;/code> -&amp;gt; from the second word; trick is to use a different analyser at the search time;&lt;/li>
&lt;li>&lt;code>e&lt;/code> -&amp;gt; from the second entity;&lt;/li>
&lt;li>&lt;code>j&lt;/code> -&amp;gt; the last word.&lt;/li>
&lt;/ul>
&lt;p>Cool, it&amp;rsquo;s time to open the Kibana dev tools and hack. All examples are worked out and tested with version &lt;code>8.4.1&lt;/code>.&lt;/p>
&lt;h3 id="baseline-suggestions-setup">Baseline suggestions setup&lt;/h3>
&lt;p>Nothing fancy here, copy-paste from the documentation verbatim, for the sake of completeness.&lt;/p>
&lt;pre>&lt;code class="language-text">DELETE music
PUT music
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;b&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>And it returns:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;b&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="suggest-from-the-second-word">Suggest from the second word&lt;/h3>
&lt;p>The idea here is to have a subfield for suggestions that uses different analyzers for indexing and searching. For indexing we want an analyzer that drops the first token. For search time analyzer we want to use a standard analyzers because we should not drop first token from the search string. Do not forget to set &lt;code>preserve_position_increments&lt;/code> as &lt;code>false&lt;/code> for the new field.&lt;/p>
&lt;pre>&lt;code class="language-text">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;remove_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position != 0&amp;quot;
}
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;drop_first_word&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;from_second_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;drop_first_word&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;preserve_position_increments&amp;quot;: false
}
}
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;s&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest.from_second_word&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>It returns:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;s&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Great start!&lt;/p>
&lt;p>What about starting from the 3rd and 4th word? Could we just create a new subfield and change the script for the &lt;code>predicate_token_filter&lt;/code> from &lt;code>token.position != 0&lt;/code> to &lt;code>token.position != 0 &amp;amp;&amp;amp; token.position != 1&lt;/code> and so on? IMO, could work but there should be a &amp;ldquo;better&amp;rdquo; way.&lt;/p>
&lt;h3 id="suggest-from-the-second-entity">Suggest from the second entity&lt;/h3>
&lt;p>In other words we need to suggest text that starts after the separator. The implementation assumes that you have 2 entities. Once again, we will leverage different index and search time analyzers. For the indexing to achieve the required functionality with token filters would be complicated. The hack would be to use &lt;code>char_filter&lt;/code> to get rid of the first &amp;ldquo;entity&amp;rdquo;.&lt;/p>
&lt;pre>&lt;code class="language-text">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;char_filter&amp;quot;: {
&amp;quot;remove_until_separator&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;(.*and )|(.*&amp;amp; )&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;&amp;quot;
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;drop_first_entity&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;char_filter&amp;quot;: [&amp;quot;remove_until_separator&amp;quot;],
&amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;from_second_entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;drop_first_entity&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;preserve_position_increments&amp;quot;: false
}
}
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;e&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest.from_second_entity&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Returns:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;e&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="suggest-from-the-last-word">Suggest from the last word&lt;/h3>
&lt;p>The idea is not to tokenize the string, reverse the string, tokenize the reversed string, and reverse the tokens once again.&lt;/p>
&lt;pre>&lt;code class="language-text">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;last_word&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;word_delimiter_graph&amp;quot;,
&amp;quot;reverse&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;from_last_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;last_word&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;preserve_position_increments&amp;quot;: false
}
}
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;j&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest.from_last_word&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>returns&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;j&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Note that the suggestion for &amp;ldquo;john e&amp;rdquo; also works. Which might be a bit unexpected.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Every separate use case is covered with a dedicated field. To support all of them at once you just need to add all the subfields into index and query all of them with multiple &lt;code>suggest&lt;/code> clauses. How to combine those suggestions is out of scope for this post, but it should be implemented in your app.&lt;/p>
&lt;h2 id="bonus-1-synonyms-for-suggestions">Bonus 1: Synonyms for suggestions&lt;/h2>
&lt;p>After the &amp;ldquo;successful&amp;rdquo; release of the new functionality the very next day PM (under the usual influence of some exotic and probably illegal substances) once again came up with new idea: &amp;ldquo;in search suggestions we need to support variants of artist names&amp;rdquo;. His example was: &amp;ldquo;I&amp;rsquo;ve heard that most babies can&amp;rsquo;t pronounce &lt;code>britney&lt;/code> and she say something like &lt;a href="https://mom.com/baby-names/girl/19714/britney">&lt;code>ditney&lt;/code>&lt;/a>, so to make our product more successful among the baby searchers segment we must support this use case&amp;rdquo;.&lt;/p>
&lt;p>Somewhat reasonable :)&lt;/p>
&lt;p>The idea is to add synonym token filter for the artist names so that the synonym token would be on the 0 position in the token stream.&lt;/p>
&lt;pre>&lt;code class="language-kibana">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;name_synonyms&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym&amp;quot;,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;britney, ditney&amp;quot;
]
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonyms&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;name_synonyms&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;synonyms&amp;quot;
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;d&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Returns:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;d&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>It works, amazing! Baby searchers are happy.&lt;/p>
&lt;h2 id="bonus-2-shingles-for-suggestions">Bonus 2: shingles for suggestions&lt;/h2>
&lt;p>After some sleeping on the mind-bending development experience a colleague asked: &amp;ldquo;can&amp;rsquo;t we just use one field for all the requirements?&amp;rdquo;. His reasoning was that we index the same string multiple times and our clusters doesn&amp;rsquo;t have infinite capacity. Also, we&amp;rsquo;ve seen what and how PM is thinking, and it is only a matter of time when we will have to support suggestions from everywhere in the string&amp;quot;. We&amp;rsquo;ve already seen that using synonyms it is doable.&lt;/p>
&lt;p>His idea was to analyze text in such a way that it would produce multiple tokens with the position=0 for all the potential suggestion &amp;ldquo;entry points&amp;rdquo;. To achieve it we could &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-shingle-tokenfilter.html">shingle&lt;/a> the string, take only shingles that has position=0 and take the last word from every shingle. Let&amp;rsquo;s try.&lt;/p>
&lt;pre>&lt;code class="language-kibana">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;index.max_shingle_diff&amp;quot;: 10,
&amp;quot;analysis&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;after_last_space&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;(.* )&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;&amp;quot;
},
&amp;quot;preserve_only_first&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position == 0&amp;quot;
}
},
&amp;quot;big_shingling&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;shingle&amp;quot;,
&amp;quot;min_shingle_size&amp;quot;: 2,
&amp;quot;max_shingle_size&amp;quot;: 10,
&amp;quot;output_unigrams&amp;quot;: true
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;dark_magic&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;big_shingling&amp;quot;,
&amp;quot;preserve_only_first&amp;quot;,
&amp;quot;after_last_space&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;dark_magic&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s test how this analyzer works:&lt;/p>
&lt;pre>&lt;code class="language-kibana">POST music/_analyze
{
&amp;quot;explain&amp;quot;: false,
&amp;quot;text&amp;quot;: [&amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;dark_magic&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Returns&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;tokens&amp;quot;: [
{
&amp;quot;token&amp;quot;: &amp;quot;britney&amp;quot;,
&amp;quot;start_offset&amp;quot;: 0,
&amp;quot;end_offset&amp;quot;: 7,
&amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot;: 0
},
{
&amp;quot;token&amp;quot;: &amp;quot;spears&amp;quot;,
&amp;quot;start_offset&amp;quot;: 0,
&amp;quot;end_offset&amp;quot;: 14,
&amp;quot;type&amp;quot;: &amp;quot;shingle&amp;quot;,
&amp;quot;position&amp;quot;: 0,
&amp;quot;positionLength&amp;quot;: 2
},
{
&amp;quot;token&amp;quot;: &amp;quot;elton&amp;quot;,
&amp;quot;start_offset&amp;quot;: 0,
&amp;quot;end_offset&amp;quot;: 22,
&amp;quot;type&amp;quot;: &amp;quot;shingle&amp;quot;,
&amp;quot;position&amp;quot;: 0,
&amp;quot;positionLength&amp;quot;: 3
},
{
&amp;quot;token&amp;quot;: &amp;quot;john&amp;quot;,
&amp;quot;start_offset&amp;quot;: 0,
&amp;quot;end_offset&amp;quot;: 27,
&amp;quot;type&amp;quot;: &amp;quot;shingle&amp;quot;,
&amp;quot;position&amp;quot;: 0,
&amp;quot;positionLength&amp;quot;: 4
}
]
}
&lt;/code>&lt;/pre>
&lt;p>Positions of all the tokens are 0. Good start.&lt;/p>
&lt;p>Let&amp;rsquo;s test the suggestions with all the potential queries in the same request:&lt;/p>
&lt;pre>&lt;code class="language-text">
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;britney&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;b&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
},
&amp;quot;spears&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;s&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
},
&amp;quot;elton&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;e&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
},
&amp;quot;john&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;j&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>It returns:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;britney&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;b&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
],
&amp;quot;elton&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;e&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
],
&amp;quot;john&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;j&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
],
&amp;quot;spears&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;s&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Wow! Note, that a query (that spans several tokens) e.g. &lt;code>britney sp&lt;/code> doesn&amp;rsquo;t match anything. Fixable, but let&amp;rsquo;s leave the fix out of scope for now.&lt;/p>
&lt;h2 id="fin">Fin&lt;/h2>
&lt;p>Thank you and congratulations: You got to the very end of the blog post. Tell me about your craziest adventures with the search suggestions in the comments below?&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Have a look at the impressive engineering of &lt;a href="https://github.com/apache/lucene/blob/main/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java">Lucene&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>All the characters in this story are completely fictional. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Entity Resolution with Opensearch/Elasticsearch</title><link>https://www.jocas.lt/blog/post/entity-resolution/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/entity-resolution/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Record_linkage">entity resolution&lt;/a> can be implemented as a search application and if the requirements are not too crazy then Opensearch/Elasticsearch is good enough.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Say that our company have a curated registry of organizations (only organization names with the number of employees) conveniently indexed in the OpenSearch/Elasticsearch.
Our employer acquired a direct competitor with their own nice little registry (only organization names with their addresses), and we were tasked to integrate the new registry with our old registry, i.e. to perform an entity resolution.
Our task is to iterate through the new registry record by record and try to map them to our &amp;ldquo;Golden registry&amp;rdquo;.
Unfortunately, the only overlapping data is organization names and our matching needs to be based mostly on the organization name.
Let&amp;rsquo;s implement organization name matching by text similarity directly with Opensearch/Elasticsearch.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>Say that we were given these organization name similarity rules in the descending order of importance.
Let&amp;rsquo;s have an example query &amp;ldquo;Apple&amp;rdquo; in mind as we go:&lt;/p>
&lt;ol>
&lt;li>Exact match, e.g. &amp;ldquo;Apple&amp;rdquo;&lt;/li>
&lt;li>Exact first word match, e.g. &amp;ldquo;&lt;strong>Apple&lt;/strong> Computers&amp;rdquo;&lt;/li>
&lt;li>Exact not first word match, e.g. &amp;ldquo;Big &lt;strong>Apple&lt;/strong> Company&amp;rdquo;&lt;/li>
&lt;li>Partial first word match - start, e.g. &amp;ldquo;&lt;strong>Apple&lt;/strong>sauce Company&amp;rdquo;&lt;/li>
&lt;li>Partial first word match - end, e.g. &amp;ldquo;Pine&lt;strong>apple&lt;/strong> Manufacturing&amp;rdquo;&lt;/li>
&lt;li>Partial not first word match - end, e.g. &amp;ldquo;Canadian Bake&lt;strong>apple&lt;/strong>&amp;rdquo;&lt;/li>
&lt;li>Fuzzy, e.g. &amp;ldquo;&lt;strong>Apply&lt;/strong>&amp;rdquo;&lt;/li>
&lt;/ol>
&lt;p>On top of that, we should be able to add other signals that could change the ordering,
e.g. the number of employees: the more employees the matching organization has the higher its matching score.&lt;/p>
&lt;p>&amp;ldquo;Applesauce Company&amp;rdquo; with 100 employees should be lower than &amp;ldquo;Pineapple Manufacturing&amp;rdquo; with 100000 employees despite that
the rule of a &lt;em>partial first word match at the start&lt;/em> has a higher name similarity than the &lt;em>partial not first word match at the end&lt;/em>.&lt;/p>
&lt;p>And yes, the entity resolution should be implemented only with features provided by Opensearch/Elasticsearch.&lt;/p>
&lt;p>NOTE: the proposed solution is not production ready, use the examples wisely.&lt;/p>
&lt;h3 id="additional-notes-for-development">Additional notes for development&lt;/h3>
&lt;p>To make the development easier we also want:&lt;/p>
&lt;ul>
&lt;li>to know which rule matched for each hit we can leverage &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-request-named-queries-and-filters.html">named queries&lt;/a> feature (or try the highlighting&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>),&lt;/li>
&lt;li>give each rule a normalized score (&lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF&lt;/a> is hard to normalize),&lt;/li>
&lt;li>matching of strings should be case-insensitive,&lt;/li>
&lt;li>Index (conveniently names &lt;code>organizations&lt;/code>) contains the sample organizations list (extracted from the requirements examples),&lt;/li>
&lt;li>Work out the examples in Kibana/Dashboards,&lt;/li>
&lt;/ul>
&lt;p>The indexing of organizations can be done with this simple command:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations/_bulk
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;3&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Big Apple Company&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;4&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Applesauce Company&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;5&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Pineapple Manufacturing&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;6&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Canadian Bakeapple&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;7&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Apply&amp;quot; }
&lt;/code>&lt;/pre>
&lt;h2 id="implementation-strategy">Implementation Strategy&lt;/h2>
&lt;p>Let&amp;rsquo;s split the implementation into 3 parts:&lt;/p>
&lt;ul>
&lt;li>implement the organization name matching requirements,&lt;/li>
&lt;li>implement the scoring signal based on the number of employees,&lt;/li>
&lt;li>fine-tune the scoring function.&lt;/li>
&lt;/ul>
&lt;h2 id="string-matching-implementation">String Matching Implementation&lt;/h2>
&lt;p>In the following sections we&amp;rsquo;ll implement all string matching signals with the OpenSearch/Elasticsearch text analysis features.
I&amp;rsquo;ll work out each requirement in isolation.
For each rule we will define a subfield of the &lt;code>name&lt;/code> attribute with an analyzer and a corresponding query clause.&lt;/p>
&lt;h3 id="exact-match">Exact match&lt;/h3>
&lt;p>Exact matching is rather simple to implement, just use &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/keyword.html#keyword-field-type">&lt;code>keyword&lt;/code>&lt;/a> datatype with a &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/normalizer.html">normalizer&lt;/a> that &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenfilter.html">lowercases&lt;/a> the string.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-kibana">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;lowercased_keyword&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;keyword_lowercased&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;lowercased_keyword&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-kibana">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;term&amp;quot;: {
&amp;quot;name.keyword_lowercased&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;exact_match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 7
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Matches exactly one organization:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 3,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 7.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 7.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;exact_match&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="exact-first-word-match">Exact first word match&lt;/h3>
&lt;p>With this requirement we want the query to match the first word of the organization name&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We can implement the requirement by simply indexing only the first token.
To achieve it we could use the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-limit-token-count-tokenfilter.html">&lt;code>limit&lt;/code>&lt;/a> token filter.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_one_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;first_token&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_one_token_limit&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>We leverage the fact that by default the &lt;code>limit&lt;/code> token filter defaults &lt;code>max_token_count&lt;/code> parameters to 1 token.
And we don&amp;rsquo;t need to define a new token filter (key damage to the keyboard, yay).&lt;/p>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;term&amp;quot;: {
&amp;quot;name.first_token&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Exact first word match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 6
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 6.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 6.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Exact first word match&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 6.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Exact first word match&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Note that not only &amp;ldquo;Apple Computers&amp;rdquo; matched but also &amp;ldquo;Apple&amp;rdquo; matched.
This makes sense because &amp;ldquo;Apple&amp;rdquo; has only one word it is &lt;em>the first word&lt;/em>.&lt;/p>
&lt;p>If we wanted to exclude the &amp;ldquo;Apple&amp;rdquo; match we could combine the &lt;code>Exact first word match&lt;/code> with the &lt;code>Exact match&lt;/code> and construct a bool query, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;bool&amp;quot;: {
&amp;quot;_name&amp;quot;: &amp;quot;Exact first word match&amp;quot;,
&amp;quot;must&amp;quot;: [
{
&amp;quot;term&amp;quot;: {
&amp;quot;name.first_token&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;
}
}
}
],
&amp;quot;must_not&amp;quot;: [
{
&amp;quot;term&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;
}
}
}
]
}
},
&amp;quot;boost&amp;quot;: 6
}
}
}
&lt;/code>&lt;/pre>
&lt;p>For similar situations we could act similarly but let&amp;rsquo;s keep it simple.&lt;/p>
&lt;h3 id="exact-not-first-word-match">Exact not first word match&lt;/h3>
&lt;p>E.g.: query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Big Apple Company&amp;rdquo;.
The interesting bit is how to differentiate this rule from the &lt;code>Exact first word match&lt;/code>?
As an implementation we could create a boolean query that filters out the first word match.
Another way to implement the is not to index the first word of the organization name.
A slight complication with this approach is that the query time analyzer should be different from
the index time analyzer to prevent the removal of the first (and probably the only) word from the query.
Let&amp;rsquo;s proceed with the second approach because I think it is more interesting.&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;tokenized&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;tokenized_without_first_word&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;remove_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position != 0&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;tokenized_without_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;tokenized_without_first_word&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;tokenized&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note, for the sake of simplicity for examples we assume only one word queries.&lt;/p>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.tokenized_without_first_word&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Exact not first word match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 5
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 6.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;3&amp;quot;,
&amp;quot;_score&amp;quot; : 6.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Big Apple Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Exact not first word match&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="partial-first-word-match---start">Partial first word match - start&lt;/h3>
&lt;p>E.g. Query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Applesauce Company&amp;rdquo;.
Probably we could get away with a simple prefix query, but if we want to make it fast for larger indices we should do &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenfilter.html">edge n-grams token filter&lt;/a>.
The idea is to keep only this first word and then apply the edge n-gram token filter on it.
Of course, query analyzer should be specified without edge n-gram token filter to prevent false hits from matching a couple of first characters from the query, e.g. query &amp;ldquo;Apps&amp;rdquo; would match &amp;ldquo;Apple&amp;rdquo; which we don&amp;rsquo;t want.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_first_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_first_token_limit_edge_ngrams&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;1_10_edgegrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;,
&amp;quot;min_gram&amp;quot;: 1,
&amp;quot;max_gram&amp;quot;: 10
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;first_token_edge_ngram&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_first_token_limit_edge_ngrams&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Here the &lt;code>max_ngram&lt;/code> is set to 10 for no big reason, it really depends on your data.&lt;/p>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_token_edge_ngram&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial first word match - start&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 4
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 3,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 4.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 4.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - start&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 4.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - start&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;4&amp;quot;,
&amp;quot;_score&amp;quot; : 4.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Applesauce Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - start&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Note that &amp;ldquo;Apple&amp;rdquo; and &amp;ldquo;Apple Computers&amp;rdquo; also matched along the &amp;ldquo;Applesauce Company&amp;rdquo;.
We could prevent matching the first two by doing the bool query with must_not match on the whole first token.&lt;/p>
&lt;h3 id="partial-first-word-match---end">Partial first word match - end&lt;/h3>
&lt;p>E.g. Query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Pineapple Manufacturing&amp;rdquo;.
In other words, the query string should be the ending of the first word of the organization name.
The idea is to take the first word, reverse it, apply edge n-grams, reverse those n-grams.
The search time analyzer should not have an edge n-grams token filter.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_first_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_one_token_limit_endings&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;reverse&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;1_10_edgegrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;,
&amp;quot;min_gram&amp;quot;: 1,
&amp;quot;max_gram&amp;quot;: 10
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;first_word_end_ngrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_one_token_limit_endings&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_word_end_ngrams&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial first word match - end&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 3
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 3,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 3.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 3.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 3.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;5&amp;quot;,
&amp;quot;_score&amp;quot; : 3.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Pineapple Manufacturing&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="partial-not-first-word-match---end">Partial not first word match - end&lt;/h3>
&lt;p>E.g. query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Canadian Bakeapple&amp;rdquo;.
The idea is to tokenize, lowercase, remove first word, reverse tokens, create edge n-grams, reverse back.
Search time analyzer should not include edge n-grams.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_first_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_remaining_token_endings&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;reverse&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;1_10_edgegrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;,
&amp;quot;min_gram&amp;quot;: 1,
&amp;quot;max_gram&amp;quot;: 10
},
&amp;quot;remove_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position != 0&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;remaining_words_end_ngrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_remaining_token_endings&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.remaining_words_end_ngrams&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial not first word match - end&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 2
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 2.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;3&amp;quot;,
&amp;quot;_score&amp;quot; : 2.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Big Apple Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial not first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;6&amp;quot;,
&amp;quot;_score&amp;quot; : 2.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Canadian Bakeapple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial not first word match - end&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="fuzzy">Fuzzy&lt;/h3>
&lt;p>E.g. query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Apply&amp;rdquo;.
It is pretty much the exact match but with some &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html#query-dsl-match-query-fuzziness">fuzziness&lt;/a> allowed.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;lowercased_keyword&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;keyword_lowercased&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;lowercased_keyword&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;fuzziness&amp;quot;: 1,
&amp;quot;_name&amp;quot;: &amp;quot;fuzzy_1&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 1
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;fuzzy_1&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;7&amp;quot;,
&amp;quot;_score&amp;quot; : 1.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apply&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;fuzzy_1&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>In some circumstances we might allow fuzziness set to more than 1.&lt;/p>
&lt;h2 id="lets-combine-the-pieces-together">Let&amp;rsquo;s combine the pieces together&lt;/h2>
&lt;p>Phew, that was pretty involved.
Let&amp;rsquo;s combine the previous examples into one, so our requirements are satisfied.&lt;/p>
&lt;p>Index configuration is constructed by merging analysis components and adding fields from all the examples:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;lowercased_keyword&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;1_10_edgegrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;,
&amp;quot;min_gram&amp;quot;: 1,
&amp;quot;max_gram&amp;quot;: 10
},
&amp;quot;remove_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position != 0&amp;quot;
}
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_one_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;tokenized&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;tokenized_without_first_word&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_first_token_limit_edge_ngrams&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;
]
},
&amp;quot;standard_first_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_remaining_token_endings&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;reverse&amp;quot;
]
},
&amp;quot;standard_one_token_limit_endings&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;reverse&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;first_token&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_one_token_limit&amp;quot;
},
&amp;quot;tokenized_without_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;tokenized_without_first_word&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;tokenized&amp;quot;
},
&amp;quot;first_token_edge_ngram&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_first_token_limit_edge_ngrams&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
},
&amp;quot;first_word_end_ngrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_one_token_limit_endings&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
},
&amp;quot;remaining_words_end_ngrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_remaining_token_endings&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
},
&amp;quot;keyword_lowercased&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;lowercased_keyword&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Here&lt;/p>
&lt;p>The query is constructed from all the previous examples simply by adding constant score queries to the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-dis-max-query.html">dis_max&lt;/a> query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;dis_max&amp;quot;: {
&amp;quot;queries&amp;quot;: [
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;term&amp;quot;: {
&amp;quot;name.keyword_lowercased&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;exact_match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 7
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_token&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Exact first word match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 6
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.tokenized_without_first_word&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Exact not first word match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 5
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_token_edge_ngram&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial first word match - start&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 4
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_word_end_ngrams&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial first word match - end&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 3
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.remaining_words_end_ngrams&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial not first word match - end&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 2
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;fuzziness&amp;quot;: 1,
&amp;quot;_name&amp;quot;: &amp;quot;fuzzy_1&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 1
}
}
]
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Finally, the list of organizations ranked by the name similarity:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 2,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 7,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 7.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 7.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;exact_match&amp;quot;,
&amp;quot;Partial first word match - end&amp;quot;,
&amp;quot;Partial first word match - start&amp;quot;,
&amp;quot;Exact first word match&amp;quot;,
&amp;quot;fuzzy_1&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 6.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;,
&amp;quot;Partial first word match - start&amp;quot;,
&amp;quot;Exact first word match&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;3&amp;quot;,
&amp;quot;_score&amp;quot; : 5.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Big Apple Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Exact not first word match&amp;quot;,
&amp;quot;Partial not first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;4&amp;quot;,
&amp;quot;_score&amp;quot; : 4.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Applesauce Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - start&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;5&amp;quot;,
&amp;quot;_score&amp;quot; : 3.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Pineapple Manufacturing&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;6&amp;quot;,
&amp;quot;_score&amp;quot; : 2.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Canadian Bakeapple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial not first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;7&amp;quot;,
&amp;quot;_score&amp;quot; : 1.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apply&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;fuzzy_1&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Nice, from the most similar &amp;ldquo;Exact match&amp;rdquo; all the way down to the &amp;ldquo;fuzzy&amp;rdquo; matches.&lt;/p>
&lt;p>I believe this post got a bit too long to continue working out the remaining requirements.
I&amp;rsquo;ll finish the developments in the future post.
Also, it shouldn&amp;rsquo;t be too hard to load bigger dataset to test our entity resolution algorithm, something like &lt;a href="http://download.companieshouse.gov.uk/en_output.html">Companies House registry&lt;/a> with some real world data.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>I hope this exercise was interesting.
To sum up:&lt;/p>
&lt;ul>
&lt;li>I&amp;rsquo;ve demonstrated how to do the text analysis tricks that would shape the organization name string according to the requirements.&lt;/li>
&lt;li>As a way to normalize search scores I&amp;rsquo;ve used the &lt;code>constant_score&lt;/code> query. To come up with this trick is not very obvious because Elasticsearch scoring by default is unbounded due to text statistics and therefore not very well suited
to satisfy the requirements of matching semi structured short strings such ar organization names.&lt;/li>
&lt;li>Another neat trick was to use the &lt;code>dis_max&lt;/code> query to calculate the score by taking only the most significant similarity signal.
In this way we&amp;rsquo;ve prevented the situations where less similar matches would go up because they matched several less significant
similarity rules and their score were added up.&lt;/li>
&lt;li>Also, by using &lt;code>_name&lt;/code> parameters for each query clause we get which similarity rules matched despite the fact that we&amp;rsquo;ve took the score
of the highest scoring similarity rule. These flags might be used for further rescoring in your application.&lt;/li>
&lt;/ul>
&lt;p>In case of any comments or questions don&amp;rsquo;t hesitate to leave comments under this Github &lt;a href="https://github.com/dainiusjocas/blog/issues/25">issue&lt;/a>.&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Another approach would be to use hits &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/highlighting.html">highlighting&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Of course, this is a bit unrealistic because organization name might very well contain more than one word.
A hint on how to implement the same requrements for two word organization names would be to use &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-shingle-tokenfilter.html">shingles&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Make Elasticsearch Query Profiling Faster in Kibana</title><link>https://www.jocas.lt/blog/post/kibana-profile-many-shards-hack/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/kibana-profile-many-shards-hack/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;pre>&lt;code>curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}' | jq ' .profile.shards = [.profile.shards[0]]' | pbcopy
&lt;/code>&lt;/pre>
&lt;p>and paste into the Kibana&amp;rsquo;s Search Profile panel.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>When you profile a complex Elasticsearch query that targets many shards then Kibana might need a very long time (think, minutes) to visualize the profiling data.
It might be due to some bug in the Kibana or maybe you just throw too much data in there and since Javascript is single threaded it just takes time.
Anyway, you want to see the profile data visualization because reading the raw JSON is not your thing.&lt;/p>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;p>A trick you can try is to visualize only a part of the profile data.
What part?
Let&amp;rsquo;s say the profile data from only one shard.&lt;/p>
&lt;p>The Elasticsearch response with the profile data has this shape:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot;: 3,
&amp;quot;timed_out&amp;quot;: false,
&amp;quot;_shards&amp;quot;: {
&amp;quot;total&amp;quot;: 66,
&amp;quot;successful&amp;quot;: 66,
&amp;quot;skipped&amp;quot;: 0,
&amp;quot;failed&amp;quot;: 0
},
&amp;quot;hits&amp;quot;: {
&amp;quot;total&amp;quot;: {
&amp;quot;value&amp;quot;: 10000,
&amp;quot;relation&amp;quot;: &amp;quot;gte&amp;quot;
},
&amp;quot;max_score&amp;quot;: null,
&amp;quot;hits&amp;quot;: []
},
&amp;quot;profile&amp;quot;: {
&amp;quot;shards&amp;quot;: [
]
}
}
&lt;/code>&lt;/pre>
&lt;p>And the important bits are under &lt;code>.profile.shards&lt;/code> array (&lt;code>jq&lt;/code> syntax).
Let&amp;rsquo;s create a little &lt;code>jq&lt;/code> script that would transform the response body to include the profile data from only one shard:&lt;/p>
&lt;pre>&lt;code class="language-shell">jq '.profile.shards = [.profile.shards[0]]'
&lt;/code>&lt;/pre>
&lt;p>The output is now much smaller.&lt;/p>
&lt;p>For this script to work its input must be the full body of the Elasticsearch response in JSON.
Luckily for us, a simple &lt;code>curl&lt;/code> (that can be copied directly from Kibana Dev Tools) does the job well, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-shell">curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}'
&lt;/code>&lt;/pre>
&lt;p>Pipe the output of that &lt;code>curl&lt;/code> command to the &lt;code>jq&lt;/code> script:&lt;/p>
&lt;pre>&lt;code class="language-shell">curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}' | jq '.profile.shards = [.profile.shards[0]]'
&lt;/code>&lt;/pre>
&lt;p>Now just select the output with you mouse, copy, and then paste it in Kibana&amp;rsquo;s Search Profile panel and investigate the query profile.&lt;/p>
&lt;h2 id="but-using-the-mouse-in-terminal-is-not-sleek">But using the mouse in terminal is not sleek&lt;/h2>
&lt;p>One aditional trick to make the process sleeker is to send the profile data to the clipboard directly from your terminal and then paste in Kibana.
This can be done by simply piping the output to your clipboard.
Unfortunately, the script is different for different operating systems.&lt;/p>
&lt;p>Example in macOS:&lt;/p>
&lt;pre>&lt;code class="language-shell">curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}' | jq '.profile.shards = [.profile.shards[0]]' | pbcopy
&lt;/code>&lt;/pre>
&lt;p>Example in Kubuntu 21.04:&lt;/p>
&lt;pre>&lt;code class="language-shell">curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}' | jq '.profile.shards = [.profile.shards[0]]' | xclip -selection clip
&lt;/code>&lt;/pre></description></item><item><title>How to Prevent Data Corruption in Elasticsearch When Using Kafka Connect Elasticsearch Sink Connector</title><link>https://www.jocas.lt/blog/post/kc_es_data_consistency/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/kc_es_data_consistency/</guid><description>&lt;h3 id="tldr">TL;DR&lt;/h3>
&lt;p>When the &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch">Elasticsearch indexer&lt;/a> is highly concurrent, Kafka record keys are used as Elasticsearch document IDs, and indexer is set to delete records on &lt;code>null&lt;/code> values, then Kafka Connect Elasticsearch Sink Connector might corrupt your data: documents that should not be deleted end up being deleted, or documents that should be deleted end up still being present in the index. The fix is to use external versioning for deletes in bulk requests as it is proposed in this &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/pull/422">Github Pull Request&lt;/a>.&lt;/p>
&lt;h3 id="the-problem">The problem&lt;/h3>
&lt;p>NOTE: as of version 6.0.0 of the Confluent Platform (last checked on 2020-10-02) the bug that might lead to data corruption is still present.&lt;/p>
&lt;p>Let&amp;rsquo;s focus on a use case where Kafka record key is used as an Elasticsearch document ID&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. I would consider this to be a proper practice when the documents represent a catalog of things.&lt;/p>
&lt;p>Elasticsearch uses &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/optimistic-concurrency-control.html">optimistic concurrency control&lt;/a>. The job of this concurrency mechanism is to ensure that older version of the document doesn&amp;rsquo;t override a newer version. By default, order of arrival of the operation is applied, but the behaviour can be overriden in &lt;a href="https://www.elastic.co/blog/elasticsearch-versioning-support">several ways&lt;/a> depending on the version of Elasticsearch. In this post we focus on concurrent bulk requests, and with a concurrency that involves a network, requests will sometimes arrive out of order.&lt;/p>
&lt;p>To help Elasticsearch resolve the out-of-order indexing requests Kafka Connect Elasticsearch Sink Connector (from here on Kafka Connect for short) leverages the &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/blob/7256e9473cea690c373058b88fffd1111870cfe6/src/main/java/io/confluent/connect/elasticsearch/jest/JestElasticsearchClient.java#L564">&lt;code>external&lt;/code> document&lt;/a> versioning&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Using external versions in Kafka Connect makes sense because we already have versioning in place: Kafka topic partition offsets. If Kafka Connect applies changes to Elasticsearch indices in order of the topic offset, then any update ordering problems would be problems in the upstream system. This is a good guarantee to have.&lt;/p>
&lt;p>Let&amp;rsquo;s add to the mix delete operations. Kafka Connect supports a setting &lt;code>BEHAVIOR_ON_NULL_VALUES_CONFIG&lt;/code> to &lt;code>&amp;quot;delete&amp;quot;&lt;/code>. This setting instructs the Kafka Connect that a document in Elasticsearch with an ID of the kafka record key with &lt;code>null&lt;/code> value (a tombstone message) is going to be deleted. But for some strange reason the deletes &lt;strong>does not use external versioning&lt;/strong>! The line responsible for the described behaviour&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> can be found &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/blob/7256e9473cea690c373058b88fffd1111870cfe6/src/main/java/io/confluent/connect/elasticsearch/jest/JestElasticsearchClient.java#L554">here&lt;/a>. This means that for deletes the order-of-arrival wins. Let&amp;rsquo;s increase the concurrency of bulk requests with the param &lt;code>MAX_IN_FLIGHT_REQUESTS_CONFIG&lt;/code> to a largish number, and the data consistency problems is just round the corner for data that has some largish update ratio.&lt;/p>
&lt;p>The issue is even more pronounced when you re-index data into Elasticsearch and you want to do it as fast as possible, which means doing the indexing concurrently.&lt;/p>
&lt;h3 id="the-example">The Example&lt;/h3>
&lt;p>The code that demonstrated the faulty behaviour can be found in this &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/pull/422">Pull Request&lt;/a>.&lt;/p>
&lt;p>The test case is for testing the case when document should be present in Elasticsearch gets deleted.&lt;/p>
&lt;p>Let&amp;rsquo;s have a little walk over the code snippet:&lt;/p>
&lt;pre>&lt;code class="language-java">Collection&amp;lt;SinkRecord&amp;gt; records = new ArrayList&amp;lt;&amp;gt;();
for (int i = 0; i &amp;lt; numOfRecords - 1 ; i++) {
if (i % 2 == 0) {
SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, null, i);
records.add(sinkRecord);
} else {
record.put(&amp;quot;message&amp;quot;, Integer.toString(i));
SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, i);
records.add(sinkRecord);
}
}
record.put(&amp;quot;message&amp;quot;, Integer.toString(numOfRecords));
SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, numOfRecords);
records.add(sinkRecord);
task.put(records);
task.flush(null);
&lt;/code>&lt;/pre>
&lt;p>Here we send &lt;code>numOfRecords&lt;/code> (which larger than 2) to a Kafka topic. Every second record has &lt;code>null&lt;/code> body (delete operation), and the rest of the records have a sequence number as a &lt;code>message&lt;/code> value. The very last record is &lt;strong>always&lt;/strong> a non-null record with a &lt;code>message&lt;/code> value of &lt;code>numOfRecords&lt;/code>.&lt;/p>
&lt;p>Let&amp;rsquo;s setup a connector:&lt;/p>
&lt;pre>&lt;code class="language-java">KEY_IGNORE_CONFIG = &amp;quot;false&amp;quot;;
MAX_IN_FLIGHT_REQUESTS_CONFIG = Integer.toString(numOfRecords)
BATCH_SIZE_CONFIG = &amp;quot;1&amp;quot;
LINGER_MS_CONFIG = &amp;quot;1&amp;quot;
BEHAVIOR_ON_NULL_VALUES_CONFIG = &amp;quot;delete&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Here we set a connector to use Kafka record key as id &lt;code>KEY_IGNORE_CONFIG = &amp;quot;false&amp;quot;&lt;/code>, set the indexer concurrency to the &lt;code>numOfRecords&lt;/code>; set the indexing batch size to 1 (this creates as many requests to Elasticsearch as there are records in the Kafka topic); set indexer to send requests immediately with &lt;code>LINGER_MS_CONFIG = &amp;quot;1&amp;quot;&lt;/code>; and record with a &lt;code>null&lt;/code> value represents a delete operation.&lt;/p>
&lt;p>With this setup after the indexing is done we expect that in the index we have a document with ID and whose &lt;code>message&lt;/code> value is &lt;code>numOfRecords&lt;/code>. But when ordering of bulk requests is out-of-order then at the end we might have a situation where there is no document in the index at all: the bulk index request with &lt;code>message = numOfRecords&lt;/code> arrived before one of the bulk requests with a delete operation!&lt;/p>
&lt;p>The situation might seem to be a bit far-fetched but for applications like e-commerce where you have a catalog that is frequently updated (e.g. the catalog item should be available in search or not) and updates are modelled as document deletes it happens a bit more often than it might be expected.&lt;/p>
&lt;h3 id="the-fix">The fix&lt;/h3>
&lt;p>The fix is simple: use the same external versioning that is already being used by the indexing requests also for delete requests:&lt;/p>
&lt;pre>&lt;code class="language-java">if (record.version != null) {
req.setParameter(&amp;quot;version_type&amp;quot;, &amp;quot;external&amp;quot;).setParameter(&amp;quot;version&amp;quot;, record.version);
}
&lt;/code>&lt;/pre>
&lt;p>The full code can be found &lt;a href="ttps://github.com/confluentinc/kafka-connect-elasticsearch/pull/422">here&lt;/a>. Let&amp;rsquo;s hope that Confluent developers will find some time to merge that PR.&lt;/p>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>Thank you for reading and leave your feedback &lt;a href="https://github.com/dainiusjocas/blog/issues/12">here&lt;/a>.&lt;/p>
&lt;h3 id="ps">P.S.&lt;/h3>
&lt;p>Of course, this is not the only situation when data can get &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/issues">corrupted&lt;/a>, e.g. changing the number of partitions; when you delete the topic, repopulate it with up-to-date data (also, you skip deletes) then restarting the indexing might pretty much nothing, because all the versions are earlier &lt;code>external version&lt;/code> because offsets are smaller.&lt;/p>
&lt;h3 id="heading">&lt;/h3>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>When Kafka Record keys are not used as Elasticsearch document IDs versioning is not a problem because every Elasticsearch ID is constructed as &lt;code>{topic}+{partition}+{offset}&lt;/code> which creates a new document for every Kafka record, i.e. no versioning is needed. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Elasticsearch 7 supports the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-7.0.html#_internal_versioning_is_no_longer_supported_for_optimistic_concurrency_control">external versioning&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Yes, it is a comment, and it means that the developers were not sure whether to use external versioning for delete operations. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>How to Use Elasticsearch Ingest Pipelines with Kafka Connect Elasticsearch Sink Connector</title><link>https://www.jocas.lt/blog/post/ingest_pipeline_kafka_connect/</link><pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/ingest_pipeline_kafka_connect/</guid><description>&lt;h3 id="tldr">TL;DR&lt;/h3>
&lt;p>Specify your pipeline with the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#dynamic-index-settings">&lt;code>index.default_pipeline&lt;/code>&lt;/a> setting in the index (or index template) settings.&lt;/p>
&lt;h3 id="the-problem">The Problem&lt;/h3>
&lt;p>We need to index the log data into the &lt;a href="https://www.elastic.co/">Elasticsearch&lt;/a> cluster using a &lt;a href="https://docs.confluent.io/current/connect/kafka-connect-elasticsearch/index.html">Kafka Connect Elasticsearch Sink Connector&lt;/a> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, the data should be split into daily indices, and we need to specify the Elasticsearch ingest pipeline.&lt;/p>
&lt;p>The &lt;a href="https://docs.confluent.io/current/connect/kafka-connect-elasticsearch/configuration_options.html">documentation of the connector&lt;/a> doesn&amp;rsquo;t mention anything about ingest pipelines. After a quick consultation with the Internet you discover that there is an open &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/issues/72">issue&lt;/a> that Kafka Connect Elasticsearch Sink Connector doesn&amp;rsquo;t support specifying an Elasticsearch ingest pipeline. WAT?&lt;/p>
&lt;h3 id="the-workaround">The Workaround&lt;/h3>
&lt;p>Say&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, our pipeline&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> just renames an attribute, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-shell">PUT _ingest/pipeline/my_pipeline_id
{
&amp;quot;description&amp;quot; : &amp;quot;renames the field name&amp;quot;,
&amp;quot;processors&amp;quot; : [
{
&amp;quot;rename&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;original_field_name&amp;quot;,
&amp;quot;target_field&amp;quot;: &amp;quot;target_field_name&amp;quot;
}
}
]
}
&lt;/code>&lt;/pre>
&lt;p>The Elasticsearch ingest pipeline for indexing can be specified in several ways:&lt;/p>
&lt;ol>
&lt;li>for each index request as a &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html">URL parameter&lt;/a>,&lt;/li>
&lt;li>per bulk index request as a URL parameter,&lt;/li>
&lt;li>for every &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html#docs-bulk-api-query-params">bulk index request operation&lt;/a>,&lt;/li>
&lt;li>index settings (&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#dynamic-index-settings">a dynamic attribute&lt;/a>),&lt;/li>
&lt;li>index template.&lt;/li>
&lt;/ol>
&lt;p>First three options are not supported by Kafka Connect. The fourth option is not convenient in our case because the data should be split into time-based (e.g. daily) indices and we don&amp;rsquo;t want to do repetitive tasks&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The natural option to follow is to define an &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-templates.html">index template&lt;/a>. In the index template we can specify the &lt;code>index.default_pipeline&lt;/code> parameter, e.g.&lt;/p>
&lt;pre>&lt;code class="language-shell">PUT _index_template/template_1
{
&amp;quot;index_patterns&amp;quot;: [&amp;quot;daily_log*&amp;quot;],
&amp;quot;template&amp;quot;: {
&amp;quot;settings&amp;quot;: {
&amp;quot;index.default_pipeline&amp;quot;: &amp;quot;my_pipeline_id&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note, that for indexing not to fail, we should create the Elasticsearch ingest pipeline&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> &lt;strong>before&lt;/strong> setting up the index template.&lt;/p>
&lt;p>That is it, now when Kafka Connect will create a new daily index the Elasticsearch ingest pipeline is going to be applied to every document without any issues, for free, and in no time.&lt;/p>
&lt;h3 id="bonus">Bonus&lt;/h3>
&lt;p>One thing to note is that only one pipeline can be specified for &lt;code>index.default_pipeline&lt;/code> which might sound a bit limiting. A clever trick to overcome that limitation is to use a series of &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/pipeline-processor.html">pipeline processors&lt;/a> that can invoke other pipelines in the specified order, i.e. pipeline of pipelines.&lt;/p>
&lt;p>Also, there is an index setting called &lt;code>index.final_pipeline&lt;/code> that if specified is going to be executed after all other pipelines.&lt;/p>
&lt;p>Testing pipelines can be done using the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html">&lt;code>_simulate&lt;/code> API&lt;/a>.&lt;/p>
&lt;h3 id="fin">Fin&lt;/h3>
&lt;p>Thanks for reading and leave comments or any other feedback on this blog post in the &lt;a href="https://github.com/dainiusjocas/blog/issues/9">Github issue&lt;/a>. Examples were tested to work with Elasticsearch and Kibana 7.8.1.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>or any other technology that doesn&amp;rsquo;t support, or it is just not possible to specify the Elasticsearch ingest pipeline. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>yes, I know that the same job can be done with the &lt;a href="https://docs.confluent.io/current/connect/transforms/index.html">Kafka Connect Transformations&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>let&amp;rsquo;s leave out the Kafka Connector setup. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>set &lt;code>index.default_pipeline=my_pipeline_id&lt;/code> for every new daily index with, say, a cron-job at midnight. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>technically, before an index is created that matches the template pattern. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>A Neat Trick with Elasticsearch Normalizers</title><link>https://www.jocas.lt/blog/post/elasticsearch-normlizers/</link><pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/elasticsearch-normlizers/</guid><description>&lt;p>To analyze the textual data Elasticsearch uses &lt;strong>analyzers&lt;/strong> while for the keyword analysis there is a thing called a &lt;strong>normalizer&lt;/strong>. In this article I&amp;rsquo;ll explain what the normalizer is and show it&amp;rsquo;s use case for &lt;strong>normalizing&lt;/strong> URLs.&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>A neat use case for keyword normalizers is to extract a specific part of the URL with a char_filter of the &lt;code>pattern_replace&lt;/code> type.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In Elasticsearch the textual data is represented with two data types: &lt;code>text&lt;/code> and &lt;code>keyword&lt;/code>. The &lt;code>text&lt;/code> type is meant to be used for full-text search use cases while &lt;code>keyword&lt;/code> is mean for filtering, sorting, and aggregation.&lt;/p>
&lt;h3 id="tldr-about-analyzers">TL;DR About Analyzers&lt;/h3>
&lt;p>To make a better use of &lt;code>text&lt;/code> data you can setup the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer-anatomy.html">analyzer&lt;/a> which is a combination of three components:&lt;/p>
&lt;ul>
&lt;li>exactly one &lt;strong>tokenizer&lt;/strong>,&lt;/li>
&lt;li>zero or more &lt;strong>character filters&lt;/strong>,&lt;/li>
&lt;li>zero or more &lt;strong>token filters&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>Basically, an analyzer transforms a single &lt;em>string&lt;/em> into &lt;em>words&lt;/em>, e.g. &lt;code>&amp;quot;This is my text&amp;quot;&lt;/code> can be transformed into &lt;code>[&amp;quot;this&amp;quot;, &amp;quot;my&amp;quot;, &amp;quot;text&amp;quot;]&lt;/code> which you can read as:&lt;/p>
&lt;ul>
&lt;li>text is split into tokens by tokenizer,&lt;/li>
&lt;li>each token is lowercased with the a token filter,&lt;/li>
&lt;li>stopwords are removed with another token filter.&lt;/li>
&lt;/ul>
&lt;h3 id="normalizers">Normalizers&lt;/h3>
&lt;p>The &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-normalizers.html">documentation&lt;/a> says that:&lt;/p>
&lt;blockquote>
&lt;p>Normalizers are similar to analyzers except that they may only emit a single token.&lt;/p>
&lt;/blockquote>
&lt;p>Normalizers can only be applied to the &lt;code>keyword&lt;/code> datatype. The cannonical use case is to lowercase structured content such as IDs, email addresses, e.g. a database stores emails in whatever case but searching for emails should be case insensitive. Note that only a subset of available filters can be used by a normalizer: all filters must work on a &lt;strong>per-character basis&lt;/strong>, i.e. no stopwords or stemmers.&lt;/p>
&lt;h3 id="normalizers-for-normalizing-url-data">Normalizers for Normalizing URL Data&lt;/h3>
&lt;p>Storing a URL in a &lt;code>keyword&lt;/code> field allows to filter, sort, and aggregate your data per URL. But what if you need to filter, sort, and aggregate by just one part of the URL and you have little to no control over the upstream data source? You have a couple of options:&lt;/p>
&lt;ul>
&lt;li>convince upstream to extract that one part in their code and send it to you,&lt;/li>
&lt;li>setup a &lt;code>text&lt;/code> field with an analyzer that produces just that one token and enable field data (not a default setup and can get expensive).&lt;/li>
&lt;li>setup a &lt;code>keyword&lt;/code> field with a normalizer with a &lt;code>char_filter&lt;/code>.&lt;/li>
&lt;li>give up.&lt;/li>
&lt;/ul>
&lt;p>I want to explore the &lt;code>keyword&lt;/code> option. In the next section I&amp;rsquo;ll show how to setup normalizers for Elasticsearch URLs.&lt;/p>
&lt;h3 id="the-not-so-synthetic-problem">The not so Synthetic Problem&lt;/h3>
&lt;p>We have a list URLs without a hostname that were used to query Elasticsearch, e.g.: &lt;code>/my_search_index/_search?q=elasticsearch&lt;/code> and we need to split URLs into parts such as: index, operation endpoint, e.g.: &lt;code>_search&lt;/code> or &lt;code>_count&lt;/code>, query filters, etc. In the following example I&amp;rsquo;ll focus on the extracting the index part of the URL.&lt;/p>
&lt;p>Let&amp;rsquo;s create an index:&lt;/p>
&lt;pre>&lt;code>PUT elasticsearch_url_index
{
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;index_extractor_normalizer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;char_filter&amp;quot;: [
&amp;quot;index_name_extractor&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;index_name_extractor&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;/(.+)/.*&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;$1&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot; : {
&amp;quot;properties&amp;quot;: {
&amp;quot;url&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;index_extractor_normalizer&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Here we setup the index with a normalizer &lt;code>index_extractor_normalizer&lt;/code> that has a char filter &lt;code>index_name_extractor&lt;/code> that uses a regex &lt;code>pattern_replace&lt;/code> to extract characters between the first and the second slashes. The mappings have a property &lt;code>url&lt;/code> which is of the &lt;code>keyword&lt;/code> type and have a field &lt;code>index&lt;/code> which is also of the &lt;code>keyword&lt;/code> type and is set up to use the normalizer &lt;code>index_extractor_normalizer&lt;/code>.&lt;/p>
&lt;p>Since the normalizer is basically a collection of filters we can use our good old friend &lt;code>_analyze&lt;/code> API to test how it works.&lt;/p>
&lt;pre>&lt;code>POST elasticsearch_url_index/_analyze
{
&amp;quot;char_filter&amp;quot;: [&amp;quot;index_name_extractor&amp;quot;],
&amp;quot;text&amp;quot;: [&amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;my_search_index&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 40,
&amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
&amp;quot;position&amp;quot; : 0
}
]
}
&lt;/code>&lt;/pre>
&lt;p>Good, exactly as we wanted: &lt;code>/my_search_index/_search?q=elasticsearch&lt;/code> =&amp;gt; &lt;code>my_search_index&lt;/code>.&lt;/p>
&lt;p>Let&amp;rsquo;s index some data:&lt;/p>
&lt;pre>&lt;code>PUT elasticsearch_url_index/_doc/0
{
&amp;quot;url&amp;quot;: &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s try to filter URLs by the index name:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url:my_search_index
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 0,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No results? What? Oh! Wrong field: &lt;code>url&lt;/code> was used instead of &lt;code>url.index&lt;/code>. Let&amp;rsquo;s try once again:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url.index:my_search_index
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.2876821,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;elasticsearch_url_index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;0&amp;quot;,
&amp;quot;_score&amp;quot; : 0.2876821,
&amp;quot;_source&amp;quot; : {
&amp;quot;url&amp;quot; : &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected. Cool.&lt;/p>
&lt;h3 id="bonus-a-trick-with-the-docvalue_fields">Bonus: a Trick with the &lt;code>docvalue_fields&lt;/code>&lt;/h3>
&lt;p>Another neat trick is that we can get out the &lt;code>index&lt;/code> part of the URL from an Elasticsearch index using the &lt;code>docvalue_fields&lt;/code> option in a request ,e.g.:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url.index:my_search_index
{
&amp;quot;docvalue_fields&amp;quot;: [&amp;quot;url.index&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.2876821,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;elasticsearch_url_index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;0&amp;quot;,
&amp;quot;_score&amp;quot; : 0.2876821,
&amp;quot;_source&amp;quot; : {
&amp;quot;url&amp;quot; : &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
},
&amp;quot;fields&amp;quot; : {
&amp;quot;url.index&amp;quot; : [
&amp;quot;my_search_index&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The important part is this one:&lt;/p>
&lt;pre>&lt;code>&amp;quot;fields&amp;quot; : {
&amp;quot;url.index&amp;quot; : [
&amp;quot;my_search_index&amp;quot;
]
}
&lt;/code>&lt;/pre>
&lt;p>A neat thing about the &lt;code>docvalue_fields&lt;/code> is that in the example above the &lt;code>my_search_index&lt;/code> value is not comming from the &lt;code>_source&lt;/code> of the document. This means that we can use &lt;code>keywords&lt;/code> and by extension normalized &lt;code>keywords&lt;/code> to fetch an exact value from the Elasticsearch index and not necessarily the one that was sent to Elasticsearch which somewhat solves our dependency from the upstream systems.&lt;/p>
&lt;h2 id="notes">Notes&lt;/h2>
&lt;p>The setup is done in the Kibana Dev Tools with the Elasticsearch 7.7.0.&lt;/p>
&lt;p>The pattern &lt;code>&amp;quot;/(.+)/.*&amp;quot;&lt;/code> is a bit simplified purely for presentation purposes and doesn&amp;rsquo;t work as expected for URLs with more than 2 slashes, e.g.: &lt;code>/index/type/_search&lt;/code> would produce &lt;code>index/type&lt;/code>. You need something a bit more involved like &lt;code>&amp;quot;/([^/]+)/.*&amp;quot;&lt;/code>.&lt;/p>
&lt;h2 id="fin">Fin&lt;/h2>
&lt;p>That is all I wanted to show you today. Hope it might be useful/interesting to someone down the line. Leave comments on the Github issue &lt;a href="https://github.com/dainiusjocas/blog/issues/7">here&lt;/a>. Cheers!&lt;/p></description></item><item><title>Using Search Templates in Elasticsearch</title><link>https://www.jocas.lt/blog/post/on-search-templates/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/on-search-templates/</guid><description>&lt;p>I want to take a look at &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html">Search Templates&lt;/a> for Elasticsearch. Let&amp;rsquo;s apply them to examples from &lt;a href="https://www.jocas.lt/blog/post/synonym-graph-phrase-search/">previous post on Synonym Graphs&lt;/a>.&lt;/p>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>I&amp;rsquo;m using Elasticsearch 7.5.1.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code>DELETE test_index-1
PUT /test_index-1
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;descrition&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
}
},
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonym_graph_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;my_synonym_graph&amp;quot;
]
},
&amp;quot;lowercase_keyword_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
],
&amp;quot;char_filter&amp;quot;: [
&amp;quot;spaces_to_undescores_filter&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;spaces_to_undescores_filter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;,
&amp;quot;mappings&amp;quot;: [
&amp;quot; \\u0020 =&amp;gt; _&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;my_synonym_graph&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym_graph&amp;quot;,
&amp;quot;lenient&amp;quot;: true,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;
]
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Index the document:&lt;/p>
&lt;pre>&lt;code>PUT test_index-1/_doc/1
{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>prefix very important thing suffix&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="templates">Templates&lt;/h2>
&lt;p>I&amp;rsquo;m very interested in one particular use of the search templates: how flexible is the management of stored seach templates? Can I update a search template while receiving queries?&lt;/p>
&lt;p>Add a template:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Try to run the search:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Exactly as expected.&lt;/p>
&lt;p>When using a stored search template the Elasticsearch client doesn&amp;rsquo;t need to handle the complex query construction.&lt;/p>
&lt;h2 id="templates-are-updateable">Templates are updateable&lt;/h2>
&lt;p>Let&amp;rsquo;s try to update the template with a higher boost value:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 5
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Works.&lt;/p>
&lt;p>Now let&amp;rsquo;s run the same query:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 4,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The scores are 0.5753642 and 1.4384103 that is ~2/5. Cool! This means that without changing (and redeploying) the Elasticsearch client we can change the querying logic, making the query an more dynamic.&lt;/p>
&lt;h2 id="corner-cases">Corner Cases&lt;/h2>
&lt;p>What if we run query has more attributes, e.g.:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;,
&amp;quot;new_attr&amp;quot;: &amp;quot;123&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>Works as expected!&lt;/p>
&lt;p>When &lt;code>query_string&lt;/code> is &lt;code>null&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: null
}
}
&lt;/code>&lt;/pre>
&lt;p>Works!&lt;/p>
&lt;p>What if the param is not provided:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;new_attr&amp;quot;: &amp;quot;value&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>No error!&lt;/p>
&lt;p>What if we provide a list instead of a string:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: [&amp;quot;this&amp;quot;, &amp;quot;Very Important Thing&amp;quot;]
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Instead of profiding one value we can replace it with a list. Good!&lt;/p>
&lt;h2 id="metadata-of-the-search-template">Metadata of the search template&lt;/h2>
&lt;p>It would be great to be able to store some metadata with the search template script, e.g. Git commit SHA of the query. I couldn&amp;rsquo;t find a way to do this. A workaround might be to &lt;code>_name&lt;/code> attribute of the query. E.g.:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;_name&amp;quot;: &amp;quot;GIT COMMIT SHA&amp;quot;,
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 5
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The response:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;GIT COMMIT SHA&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Not great but might be useful.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;ul>
&lt;li>Templates doesn&amp;rsquo;t support search index specification.&lt;/li>
&lt;li>Field names can be parameterized, this feature alows to start/stop using a new/old field.&lt;/li>
&lt;li>Search template can be tested in (even in production cluster) independently.&lt;/li>
&lt;li>We can run our query against &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-search-template.html">multiple search templates&lt;/a>. Combine this with the Profile API and performance can be compared. Explain API also is supported.&lt;/li>
&lt;/ul></description></item><item><title>Phrase Search with Synonym Graph Token Filter in Elasticsearch</title><link>https://www.jocas.lt/blog/post/synonym-graph-phrase-search/</link><pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/synonym-graph-phrase-search/</guid><description>&lt;p>I&amp;rsquo;ve &lt;a href="https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/">written&lt;/a> that if you google for &lt;code>How can you match a long query text to a short text field?&lt;/code> you&amp;rsquo;re advised to use Elasticsearch Percolator. Today I&amp;rsquo;ll show an alternative way of solving the same problem with Elasticsearch.&lt;/p>
&lt;p>The main idea is to use &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/analysis-synonym-graph-tokenfilter.html">Synonym Graph Token Filter&lt;/a> with some data preparation.&lt;/p>
&lt;h2 id="problem-statement">Problem Statement&lt;/h2>
&lt;p>Say that we learned how extract some entity from free form text with techniques such as NER, dictionary annotations, or some fancy Machine Learning. And when this entity is mentioned in the search query we want to boost documents that mention this entity. Also, say you&amp;rsquo;ve ruled out using Elasticsearch Percolator because it increases network latency because it requires additional call to Elasticsearch.&lt;/p>
&lt;p>For further discussion our unstructured text is going to be &lt;code>This description is about a Very Important Thing and something else.&lt;/code> and the extracted entity &lt;code>Very Important Thing&lt;/code>. Our test document looks like :&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>prefix very important thing suffix&lt;/code>&lt;/li>
&lt;li>&lt;code>prefix very important another thing suffix&lt;/code>&lt;/li>
&lt;li>&lt;code>prefix thing suffix&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>All examples are tested on Elasticsearch 7.5.1.&lt;/p>
&lt;h3 id="naive-setup">Naive Setup&lt;/h3>
&lt;p>Let&amp;rsquo;s create an index for our documents:&lt;/p>
&lt;pre>&lt;code>PUT /test_index-2
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;description&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Entity field is of type &lt;code>text&lt;/code> because we want it to be searchable. &lt;code>keyword&lt;/code> type won&amp;rsquo;t work because it does only exact matches and out query most likely will be longer than our entity string.&lt;/p>
&lt;p>Index our document:&lt;/p>
&lt;pre>&lt;code>PUT test_index-2/_doc/1
{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search the index with the query that mentions our &lt;code>very important thing&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.7260926,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.7260926,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Cool, we found what we we looking for.&lt;/p>
&lt;p>Let&amp;rsquo;s try another query, this time with a mention of &lt;code>very important another thing&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important another thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.7260926,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.7260926,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Oh, the results are the same as with the previous query despite the fact that we mention &lt;code>Another Thing&lt;/code> here. But it still might be OK because we matched all the terms of the entity.&lt;/p>
&lt;p>Let&amp;rsquo;s try another query:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Oh no, we still matched our &lt;code>Very Important Thing&lt;/code> while only &lt;code>thing&lt;/code> term is present in the query. But at least this time the score is lower than with previous twoqueries, 0.5753642 vs. 1.7260926. Here we clearly see the problem: we are matching short strings with long strings and partial matches raises problems.&lt;/p>
&lt;h2 id="proposed-solution">Proposed Solution&lt;/h2>
&lt;p>Let&amp;rsquo;s leverage Synonym Graph Token Filter to solve our problem.&lt;/p>
&lt;pre>&lt;code>PUT /test_index-1
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;descrition&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
}
},
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonym_graph_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;my_synonym_graph&amp;quot;
]
},
&amp;quot;lowercase_keyword_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
],
&amp;quot;char_filter&amp;quot;: [
&amp;quot;spaces_to_undescores_filter&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;spaces_to_undescores_filter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;,
&amp;quot;mappings&amp;quot;: [
&amp;quot; \\u0020 =&amp;gt; _&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;my_synonym_graph&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym_graph&amp;quot;,
&amp;quot;lenient&amp;quot;: true,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;
]
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s decompose this large index configuration piece by piece:&lt;/p>
&lt;ol>
&lt;li>The &lt;code>entity&lt;/code> attribute now has separate analyzers for both index and search phases.&lt;/li>
&lt;li>The &lt;code>lowercase_keyword_analyzer&lt;/code> uses keyword tokenizer which means that tokenization will result in the sequence of token of size 1, then it normalizes tokens by lowercasing them and finally &lt;code>spaces_to_undescores_filter&lt;/code>, replaces spaces to underscores. E.g. a string &lt;code>&amp;quot;Very Important Thing&amp;quot;&lt;/code> is transformed into list of tokens &lt;code>[&amp;quot;very_important_thing&amp;quot;]&lt;/code>. Or use out friend &lt;code>_analyze&lt;/code> API:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>POST test_index-1/_analyze
{
&amp;quot;text&amp;quot;: [&amp;quot;Very Important Thing&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;very_important_thing&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 20,
&amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
&amp;quot;position&amp;quot; : 0
}
]
}
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>The &lt;code>synonym_graph_analyzer&lt;/code> use standard tokenizer, which is followed by the &lt;code>lowercase&lt;/code> filter, and then the &lt;code>my_synonym_graph&lt;/code> token filter is applied. We&amp;rsquo;ve set up one synonym &lt;code>&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;&lt;/code>. E.g.&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>POST test_index-1/_analyze
{
&amp;quot;text&amp;quot;: [&amp;quot;prefix very important thing suffix&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;prefix&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 6,
&amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot; : 0
},
{
&amp;quot;token&amp;quot; : &amp;quot;very_important_thing&amp;quot;,
&amp;quot;start_offset&amp;quot; : 7,
&amp;quot;end_offset&amp;quot; : 27,
&amp;quot;type&amp;quot; : &amp;quot;SYNONYM&amp;quot;,
&amp;quot;position&amp;quot; : 1
},
{
&amp;quot;token&amp;quot; : &amp;quot;suffix&amp;quot;,
&amp;quot;start_offset&amp;quot; : 28,
&amp;quot;end_offset&amp;quot; : 34,
&amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot; : 2
}
]
}
&lt;/code>&lt;/pre>
&lt;p>After analysis we have 3 tokens &lt;code>[&amp;quot;prefix&amp;quot;, &amp;quot;very_important_thing&amp;quot;, &amp;quot;suffix&amp;quot;]&lt;/code>. Notice &lt;code>&amp;quot;very_important_thing&amp;quot;&lt;/code> token: this is equal to the right-hand-side from our synonym definitions. Now let&amp;rsquo;s run queries from the previous section:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: exact match -&amp;gt; hit.&lt;/p>
&lt;p>Another query:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important another thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 0,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits! Good! The document is not going to be boosted despite the fact that all tokens match.&lt;/p>
&lt;p>And the last one:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits! Good. This means that also substring doesn&amp;rsquo;t match.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>Synonym Graph Token Filter can &amp;ldquo;replace&amp;rdquo; a sequence of tokens (e.g. a phrase) with another sequence of tokens. In this particular example: many tokens were replaced with one token.&lt;/p>
&lt;ol>
&lt;li>One field can have only one analyzer pair for index and search phases. If we want another analysis pipeline for the &lt;code>entity&lt;/code> attribute we have to create another field with the analyzers specified, e.g. stemmed phrase with lower boost.&lt;/li>
&lt;li>The synonym list must be prepared before the index creation.&lt;/li>
&lt;li>Management of the synonym list might complicate index management, e.g. you use templates for your index management.&lt;/li>
&lt;li>The overal solution in general might look a bit too complicated.&lt;/li>
&lt;/ol></description></item><item><title>Elasticsearch Percolator and Text Analyzers</title><link>https://www.jocas.lt/blog/post/percolator-phrase-analyzers/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/percolator-phrase-analyzers/</guid><description>&lt;p>This time I need to percolate texts with different analyzers for index and search analyzers.&lt;/p>
&lt;p>Let&amp;rsquo;s elaborate a bit on &lt;a href="https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/">previous article&lt;/a> and explicitly declare analyzers to use.&lt;/p>
&lt;p>Define index:&lt;/p>
&lt;pre>&lt;code>PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Then define 2 slightly different percolator queries (notice the difference between &lt;code>&amp;quot;bonsai tree&amp;quot;&lt;/code> and &lt;code>&amp;quot;bonsai, tree&amp;quot;&lt;/code>).&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
PUT /my-index/_doc/2?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s percolate:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 80,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
},
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: 2 documents matched.&lt;/p>
&lt;p>But now lets change the analyzer of the second percolation query to &lt;code>whitespace&lt;/code>:&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/2?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;whitespace&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Run the percolator:&lt;/p>
&lt;pre>&lt;code>
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 5,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: only 1 percolator query matched our input.&lt;/p>
&lt;h2 id="phrases-with-stopwords">Phrases with Stopwords&lt;/h2>
&lt;p>Say, we have a phrase &lt;code>&amp;quot;bonsai is tree&amp;quot;&lt;/code> and we percolate text &lt;code>A new bonsai in tree in the office&lt;/code> with the &lt;code>standard&lt;/code> analyzer for indexing and &lt;code>english&lt;/code> for search analyzer. There should be no matches. Let&amp;rsquo;s try:&lt;/p>
&lt;pre>&lt;code>DELETE my-index
PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;english&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai is tree&amp;quot;
}
}
}
}
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai in tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>And, surprisingly, this yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 2,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai is tree&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>We have a match! Also notice that the highlighter is broken!&lt;/p>
&lt;p>The problem that these two analyzers have different stopword lists (no stopwords for &lt;code>standard&lt;/code> and several English stopwords for &lt;code>english&lt;/code> analyzer) and the phrase contains a stopword that is not shared between analyzers.&lt;/p>
&lt;p>Let&amp;rsquo;s fix this surprise with &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html#search-quote-analyzer">&lt;code>search_quote_analyzer&lt;/code>&lt;/a>.&lt;/p>
&lt;pre>&lt;code>DELETE my-index
PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;english&amp;quot;,
&amp;quot;search_quote_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai is tree&amp;quot;
}
}
}
}
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai in tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits, as expected.&lt;/p>
&lt;p>Let&amp;rsquo;s check if the expected behaviour is still there:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai is tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 4,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.39229375,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.39229375,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai is tree&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai is tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Good. Even the highlighting works.&lt;/p></description></item><item><title>Phrase Highlighting with the Elasticsearch Percolator</title><link>https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/</guid><description>&lt;p>If you google &lt;code>How can you match a long query text to a short text field?&lt;/code> it will point you to the &lt;a href="https://stackoverflow.com/questions/51865747/elasticsearch-match-long-query-text-to-short-field">Stack Overflow page&lt;/a> &lt;a href="https://discuss.elastic.co/t/match-long-query-text-to-short-field/144584/3">or here&lt;/a> where the answer is to use &lt;a href="">Elasticsearch Percolator&lt;/a>.&lt;/p>
&lt;p>My search items are phrases meaning that it should match all terms in order. Let&amp;rsquo;s create a sample setup in Kibana (v7.5) Dev dashboard.&lt;/p>
&lt;ol>
&lt;li>Create an index for percolation:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note on &lt;code>&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;&lt;/code>: this allows &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.6/search-request-highlighting.html#fast-vector-highlighter">Fast Vector Highlighter&lt;/a> to highlight combined phrase not just separate qeury terms.&lt;/p>
&lt;ol start="2">
&lt;li>Store one phrase query:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;bonsai tree&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>Percolate a document:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note on &lt;code>&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;&lt;/code>: this instructs Elasticsearch to use the Fast Vector Highlighter.&lt;/p>
&lt;p>The query yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 23,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : &amp;quot;bonsai tree&amp;quot;
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As we see highlighter correctly marker the search phrase.&lt;/p>
&lt;h2 id="storing-additional-data-with-percolator-queries">Storing additional data with percolator queries&lt;/h2>
&lt;p>Percolation result can be used to connect pieces of information in your system, e.g. store a &lt;code>subscriber_email&lt;/code> attribute of the user that wants to be notified when the query matches along with the percolator query.&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;bonsai tree&amp;quot;
}
},
&amp;quot;subscriber_email&amp;quot;: &amp;quot;subscriber_email@example.com&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Then query:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This query yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 10,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : &amp;quot;bonsai tree&amp;quot;
}
},
&amp;quot;subscriber_email&amp;quot; : &amp;quot;subscriber_email@example.com&amp;quot;
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Now, take the email under the &lt;code>&amp;quot;subscriber_email&amp;quot;&lt;/code> from the response and send an email with the highlight.&lt;/p></description></item></channel></rss>