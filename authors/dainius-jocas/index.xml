<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Dainius Jocas</title><link>https://www.jocas.lt/blog/authors/dainius-jocas/</link><atom:link href="https://www.jocas.lt/blog/authors/dainius-jocas/index.xml" rel="self" type="application/rss+xml"/><description>Dainius Jocas</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021 Dainius Jocas</copyright><lastBuildDate>Fri, 23 Apr 2021 00:00:00 +0000</lastBuildDate><image><url>https://www.jocas.lt/blog/images/icon_hu849715217c2cf577e44af3c34605d58b_27848_512x512_fill_lanczos_center_2.png</url><title>Dainius Jocas</title><link>https://www.jocas.lt/blog/authors/dainius-jocas/</link></image><item><title>lmgrep Text Analysis</title><link>https://www.jocas.lt/blog/post/lucene-text-analysis/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/lucene-text-analysis/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>&lt;code>lmgrep&lt;/code> provides an easy way to play with various text analysis options.
Just download the &lt;code>lmgrep&lt;/code>
&lt;a href="https://github.com/dainiusjocas/lucene-grep/releases" target="_blank" rel="noopener">binary&lt;/a>, run it with &lt;code>--only-analyze&lt;/code>, and observe the list of tokens.&lt;/p>
&lt;pre>&lt;code class="language-shell">echo &amp;quot;Dogs and CATS&amp;quot; | lmgrep \
--only-analyze \
--analysis='
{
&amp;quot;tokenizer&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;standard&amp;quot;},
&amp;quot;token-filters&amp;quot;: [
{&amp;quot;name&amp;quot;: &amp;quot;lowercase&amp;quot;},
{&amp;quot;name&amp;quot;: &amp;quot;englishminimalstem&amp;quot;
}
]
}'
# =&amp;gt; [&amp;quot;dog&amp;quot;,&amp;quot;and&amp;quot;,&amp;quot;cat&amp;quot;]
&lt;/code>&lt;/pre>
&lt;h2 id="text-analysis">Text Analysis&lt;/h2>
&lt;p>The
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html" target="_blank" rel="noopener">Elasticsearch documentation&lt;/a> describes text analysis as:&lt;/p>
&lt;blockquote>
&lt;p>the process of converting unstructured text into a structured format that’s optimized for search.&lt;/p>
&lt;/blockquote>
&lt;p>Therefore, to learn how the full-text search works it is important to understand how the text is, well, analyzed.
The remainder of the post focuses on how text analysis is done in
&lt;a href="https://lucene.apache.org/" target="_blank" rel="noopener">Lucene&lt;/a> which is the library that powers search engines like Elasticsearch and Solr, and what &lt;code>lmgrep&lt;/code> provides to analyze your text.&lt;/p>
&lt;h1 id="lucene">Lucene&lt;/h1>
&lt;p>Text analysis in the Lucene land is defined by 3 types of components:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>list&lt;/strong> of character filters (changes to the text before tokenization, e.g. HTML stripping, character replacement, etc.),&lt;/li>
&lt;li>&lt;strong>one&lt;/strong> tokenizer (splits text into tokens, e.g. at whitespace characters),&lt;/li>
&lt;li>&lt;strong>list&lt;/strong> of token filters (normalizes the tokens, e.g. lowercases all the letters).&lt;/li>
&lt;/ul>
&lt;p>The combination of text analysis components makes an &lt;code>Analyzer&lt;/code>.
You can think that an analyzer is a recipe to convert a string into a list of tokens&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="lmgrep">&lt;code>lmgrep&lt;/code>&lt;/h2>
&lt;p>&lt;code>lmgrep&lt;/code> is a search tool that is based on the
&lt;a href="https://lucene.apache.org/core/8_2_0/monitor/org/apache/lucene/monitor/Monitor.html" target="_blank" rel="noopener">Lucene Monitor&lt;/a> library.
To do the full-text search it needs to do the same thing that likes of Elasticsearch are doing: to analyze text.
&lt;code>lmgrep&lt;/code> packs many text
&lt;a href="https://github.com/dainiusjocas/lucene-grep/blob/v2021.04.23/docs/analysis-components.md" target="_blank" rel="noopener">analysis components&lt;/a>.
Also, it provides a list of
&lt;a href="https://github.com/dainiusjocas/lucene-grep/blob/v2021.04.23/docs/predefined-analyzers.md" target="_blank" rel="noopener">predefined analyzers&lt;/a>.
Nothing special here, the same battle tested and boring Lucene components that gets the job done&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>However, &lt;code>lmgrep&lt;/code> provides one clever twist to text analysis:
a way to specify an analyzer using plain data in JSON, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-shell">echo &amp;quot;&amp;lt;p&amp;gt;foo bars baz&amp;lt;/p&amp;gt;&amp;quot; | \
lmgrep \
--only-analyze \
--analysis='
{
&amp;quot;char-filters&amp;quot;: [
{&amp;quot;name&amp;quot;: &amp;quot;htmlStrip&amp;quot;},
{
&amp;quot;name&amp;quot;: &amp;quot;patternReplace&amp;quot;,
&amp;quot;args&amp;quot;: {
&amp;quot;pattern&amp;quot;: &amp;quot;foo&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;bar&amp;quot;
}
}
],
&amp;quot;tokenizer&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;standard&amp;quot;},
&amp;quot;token-filters&amp;quot;: [
{&amp;quot;name&amp;quot;: &amp;quot;englishMinimalStem&amp;quot;},
{&amp;quot;name&amp;quot;: &amp;quot;uppercase&amp;quot;}
]
}
'
# =&amp;gt; [&amp;quot;BAR&amp;quot;,&amp;quot;BAR&amp;quot;,&amp;quot;BAZ&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>Again, nothing special here, read the docs&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> of an interesting text analysis component, e.g.
character filter
&lt;a href="https://lucene.apache.org/core/8_3_0/analyzers-common/org/apache/lucene/analysis/pattern/PatternReplaceCharFilterFactory.html" target="_blank" rel="noopener">&lt;code>patternReplace&lt;/code>&lt;/a>,
add its config to the &lt;code>--analysis&lt;/code>, and apply it on your text.&lt;/p>
&lt;p>Conceptually it is very similar to what Elasticsearch or Solr are providing: &lt;code>analysis&lt;/code> part in the index configuration JSON in Elasticsearch, and Solr Schemas in XML.&lt;/p>
&lt;p>&lt;code>lmgrep&lt;/code> analysis component has this structure:&lt;/p>
&lt;pre>&lt;code>{&amp;quot;name&amp;quot;: &amp;quot;COMPONENT_NAME&amp;quot;, &amp;quot;args&amp;quot;: {&amp;quot;ARG_NAME&amp;quot;: &amp;quot;ARG_VALUE&amp;quot;}}
&lt;/code>&lt;/pre>
&lt;p>Notes:&lt;/p>
&lt;ul>
&lt;li>some components, e.g. &lt;code>stop&lt;/code> token filter, expect a file as an argument.
To support such components &lt;code>lmgrep&lt;/code> brutally patched Lucene to load data from arbitrary files while preserving the predefined analyzers with, e.g. their custom stop-word files.&lt;/li>
&lt;li>when a predefined analyzer is provided for text analysis then all other analysis components are silently ignored.&lt;/li>
&lt;li>predefined analyzers do not support the &lt;code>args&lt;/code> as of
&lt;a href="https://github.com/dainiusjocas/lucene-grep/issues/82" target="_blank" rel="noopener">now&lt;/a>, just the &lt;code>name&lt;/code>.&lt;/li>
&lt;li>&lt;code>lmgrep&lt;/code> as of now doesn&amp;rsquo;t provide a way to
&lt;a href="https://github.com/dainiusjocas/lucene-grep/issues/83" target="_blank" rel="noopener">share components between analyzers&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>That is pretty much all there is to know about how &lt;code>lmgrep&lt;/code> does text analysis. Try it out and let me know how it goes.&lt;/p>
&lt;h2 id="--only-analyze">&lt;code>--only-analyze&lt;/code>&lt;/h2>
&lt;p>I like the Elasticsearch&amp;rsquo;s
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html" target="_blank" rel="noopener">Analyze API&lt;/a>.
It allows me to look at the raw tokens that are either stored in the index or produced out of the search query.&lt;/p>
&lt;p>To make debugging of &lt;code>lmgrep&lt;/code> easier I wanted to expose something similar to Analyze API. The &lt;code>--only-analyze&lt;/code> flag is my humble attempt to do that.&lt;/p>
&lt;p>When the flag is specified then &lt;code>lmgrep&lt;/code> just outputs a list of tokens that is produced by applying an analyzer on the input text, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-shell">echo &amp;quot;the quick brown fox&amp;quot; | lmgrep --only-analyze
# =&amp;gt; [&amp;quot;the&amp;quot;,&amp;quot;quick&amp;quot;,&amp;quot;brown&amp;quot;,&amp;quot;fox&amp;quot;]
&lt;/code>&lt;/pre>
&lt;h3 id="implementation">Implementation&lt;/h3>
&lt;p>The machinery under the &lt;code>--only-analyze&lt;/code> works as follows:&lt;/p>
&lt;ul>
&lt;li>one thread is dedicated to read and decode the text input (either from STDIN or a file),&lt;/li>
&lt;li>one thread is dedicated to write to the STDOUT,&lt;/li>
&lt;li>the remaining CPU cores can be used by a thread pool that analyzes the text (thanks to Lucene Analyzer implementation being thread-safe).&lt;/li>
&lt;/ul>
&lt;p>On my laptop &lt;code>lmgrep&lt;/code> analyzes ~1GB of text in ~11 seconds and consumes maximum 609 MB of RAM. It should result in ~200 GB of text per hour. IMO, not bad. Of course, the more involved the text analysis is the longer it takes.&lt;/p>
&lt;p>&lt;img src="text-analysis.png" alt="Performance">&lt;/p>
&lt;p>Note that the output of &lt;code>--only-analyze&lt;/code> has the same order as the input. IMO, it makes the output a bit easier to understand. However, preserving the order limits the throughput. It is because the time and resources needed to analyze an individual piece of text can vary greatly, and the required coordination introduces some overhead.&lt;/p>
&lt;p>Consider an example of analyzing the text attributes of a book: assume that the first line sent to &lt;code>lmgrep&lt;/code> is the title of the book, the second line contains a full text of the book, and the third line is the summary. The title is relatively small, it is quickly analyzed and immediately written to STDOUT. The summary is a bit longer but still many times smaller than the body. To preserve the order &lt;code>lmgrep&lt;/code> (before writing the tokens of the summary to STDOUT) waits for the analysis on the body to be finished and written to STDOUT and only then tokens of the summary are written out.&lt;/p>
&lt;p>Notes:&lt;/p>
&lt;ul>
&lt;li>the &lt;code>--explain&lt;/code> flag is coming to &lt;code>lmgrep&lt;/code>;&lt;/li>
&lt;li>the output lines are valid JSON (&lt;code>jq&lt;/code> is your friend);&lt;/li>
&lt;li>the positional arguments for &lt;code>--only-analyze&lt;/code> are interpreted as files and when present then STDIN is ignored.&lt;/li>
&lt;/ul>
&lt;h3 id="interesting-bit">Interesting Bit&lt;/h3>
&lt;p>One thing that constantly frustrates me with Elasticsearch&amp;rsquo;s Analysis API is that I can&amp;rsquo;t specify custom char filters, tokenizer, and token filter directly in the body of the request to the Analysis API.
To observe the output of text analysis that involves custom text analysis components first I have to create an index with an analyzer and then call Analyze API that involves that index. &lt;code>lmgrep&lt;/code> avoids this pain point by allowing to declare text analysis components inline.&lt;/p>
&lt;h2 id="post-script">Post Script&lt;/h2>
&lt;p>All this analyzer construction wizardry is possible because of Lucene&amp;rsquo;s &lt;code>AbstractAnalysisFactory&lt;/code> class and features provided by its subclasses. The
&lt;a href="https://lucene.apache.org/core/8_3_1/analyzers-common/org/apache/lucene/analysis/custom/CustomAnalyzer.html" target="_blank" rel="noopener">&lt;code>CustomAnalyzer&lt;/code>&lt;/a> builder exposes methods that expects a &lt;code>Class&lt;/code> as an argument, e.g.
&lt;a href="https://lucene.apache.org/core/8_3_1/analyzers-common/org/apache/lucene/analysis/custom/CustomAnalyzer.Builder.html#addCharFilter-java.lang.Class-java.util.Map-" target="_blank" rel="noopener">&lt;code>addCharFilter&lt;/code>&lt;/a>. The trick here is that, e.g. the class
&lt;a href="https://lucene.apache.org/core/7_3_1/analyzers-common/org/apache/lucene/analysis/util/TokenFilterFactory.html" target="_blank" rel="noopener">&lt;code>TokenFilterFactory&lt;/code>&lt;/a> provides a method &lt;code>availableTokenFilters&lt;/code> that returns a set of &lt;code>names&lt;/code> of token filters and with those &lt;code>names&lt;/code> you can get a &lt;code>Class&lt;/code> object that can be supplied to &lt;code>CustomAnalyzer&lt;/code> builder methods.&lt;/p>
&lt;p>The discovery of available factory classes is based on the classpath analysis, e.g. fetching all classes where name matches a pattern like &lt;code>.*FilterFactory&lt;/code> and are subclasses of a &lt;code>TokenFilterFactory&lt;/code>. However, for the reasons that were beyond my understanding, when I created my own &lt;code>TokenFilterFactory&lt;/code> class it was not discovered by Lucene &lt;code>¯\_(ツ)_/¯&lt;/code>.&lt;/p>
&lt;p>Yeah, great, but &lt;code>lmgrep&lt;/code> is compiled with the GraalVM &lt;code>native-image&lt;/code> which assumes closed-world and throws the dynamism of the JVM out the window. How then does exactly this TokenFilterFactory-thing-class discovery works? Yes, Native images must include all the classes because at run-time it cannot create classes, but it can be worked around by providing the configuration with the classes that are going to be used at run-time, and those interesting classes can be reflectively discovered at run-time. &lt;code>lmgrep&lt;/code> relies on the Java classes being discoverable at compile-time where the reflection works as expected.&lt;/p>
&lt;p>To instruct the &lt;code>native-image&lt;/code> to discover the Java classes from Clojure code you can specify the class under the regular &lt;code>def&lt;/code> because to the &lt;code>native-image&lt;/code> &lt;code>def&lt;/code>s look like constants and are &lt;strong>evaluated&lt;/strong> at compiled-time. So, if &lt;code>lmgrep&lt;/code> misses some awesome Lucene token filter, all it takes is to add it to the hashmap under a &lt;code>def&lt;/code>.&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>
&lt;a href="http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html" target="_blank" rel="noopener">Lucene TokenStreams are actually graphs&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>If something is missing then let me know by creating an issue
&lt;a href="https://github.com/dainiusjocas/lucene-grep/issues" target="_blank" rel="noopener">here&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Just Google for &amp;ldquo;Lucene &amp;lt;COMPONENT_NAME&amp;gt;&amp;rdquo; &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Don't Change the Partition Count for Kafka Topics!</title><link>https://www.jocas.lt/blog/talk/vilnius-cloud-native-2021-04-07/</link><pubDate>Wed, 07 Apr 2021 19:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/talk/vilnius-cloud-native-2021-04-07/</guid><description>&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/A50Ec2sJ1SRG32" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> &lt;/iframe> &lt;div style="margin-bottom:5px"> &lt;strong> &lt;a href="//www.slideshare.net/Dainius/dont-change-the-partition-count-for-kafka-topics" title="Don&amp;#x27;t change the partition count for kafka topics!" target="_blank">Don&amp;#x27;t change the partition count for kafka topics!&lt;/a> &lt;/strong> from &lt;strong>&lt;a href="https://www.slideshare.net/Dainius" target="_blank">Dainius Jocas&lt;/a>&lt;/strong> &lt;/div></description></item><item><title>Silencing the Lenovo Thinkpad P53 on Linux</title><link>https://www.jocas.lt/blog/post/silencing-p53/</link><pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/silencing-p53/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>Lenovo Thinkpad P53 is a powerful laptop but a regular Linux install is just too loud for me. However, it is just a machine and there must be several control knobs that can make the experience better. In this post, I&amp;rsquo;ll share my setup that makes the laptop to be silent most of the time and performant when needed.&lt;/p>
&lt;h2 id="the-motivation">The motivation&lt;/h2>
&lt;p>To get my daily work done I spin many fat JVMs like Elasticsearch, Kafka, GraalVM native-image tool, &lt;code>docker-compose&lt;/code> clusters, virtual machines, etc. To plough through those tasks a year ago (in early 2020) I&amp;rsquo;ve got a top spec&amp;rsquo;ed Lenovo Thinkpad P53 laptop:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:right">Part&lt;/th>
&lt;th style="text-align:left">Spec&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:right">CPU&lt;/td>
&lt;td style="text-align:left">Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:right">Memory&lt;/td>
&lt;td style="text-align:left">128GB (not a mistake here)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:right">Graphics Processor&lt;/td>
&lt;td style="text-align:left">Nvidia Quadro RTX 4000&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The laptop(!) can be so power-hungry that the charger needs to provide up to 230W(!). Just imagine how loud the fans get when executing computationally intensive tasks. I simply have to leave the room and close the door until the task is finished.&lt;/p>
&lt;p>Anything that is CPU intensive makes the chip hot which spins the fans. By anything here I mean things like regular web browsing (looking at you JIRA), or a simple conference call (looking at you Google Meet and Zoom). I have some doubts that Lenovo has done the best possible job with the fans but I leave that on their conscience.&lt;/p>
&lt;p>Although I run a lot of intensive tasks on a laptop, not all of my tasks are that demanding. Therefore, I want the laptop to be performant when needed (in this case I don&amp;rsquo;t mind it to be loud) and be as silent as possible when that can be achieved. And one additional requirement: I want to work with a Linux machine.&lt;/p>
&lt;h2 id="prehistory">Prehistory&lt;/h2>
&lt;p>When I got the laptop the first thing I&amp;rsquo;ve done was that I&amp;rsquo;ve installed Ubuntu since it is officially supported&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. In a very short time, I&amp;rsquo;ve discovered that at that time it was impossible to silence the laptop to a satisfiable level: Linux just had no drivers that could silence all(!) the fans&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The hope was lost: I had to boot up Windows, setup WSL, and accommodate my development environment there. All because several drivers were missing (sigh). It&amp;rsquo;s worth to mention that I&amp;rsquo;ve set up a Windows box after more than 10 years of not touching Windows. It was just an OEM install with all the drivers, updates, etc. However, Windows setup had its own problems: for some reason (to me it was not a surprise) Windows 10 could not properly handle the sleep-wake-up cycle (its a laptop after all). But hey, I could disable the Nvidia GPU, then enable it, and everything would be working ok-ish again. I could get stuff done.&lt;/p>
&lt;p>However, updating Windows is a thing that can be postponed only that long and when you do an update with a setup that can be seen as being a little bit exotic you might get a broken system that fails to boot up. Yeah, sure, then just run the system fix scripts and you can continue working.&lt;/p>
&lt;p>Of course, it doesn&amp;rsquo;t help to have a good experience that it turned out that the laptop arrived with a faulty motherboard that caused all kinds of troubles among which the most painful was random shutdowns. Also, after those shutdowns sometimes even booting up was a real struggle. I guess that poor machine had only the best intentions and was telling me to stop torturing it and by failing to work gave me some well-deserved leisure time.&lt;/p>
&lt;p>One day the machine after a Windows update really got stuck during the reboot. A blank screen with fans maxed out and no reaction to any button or to power cable unplugging. Several Google searches away and I&amp;rsquo;ve discovered that it was a known problem with Thinkpad (not only P53). So, the motherboard got replaced and the laptop got a fair bit more stable.&lt;/p>
&lt;p>Also, even Windows that has supposedly good driver support from everyone (manufacturer, OS) involved has the power consumption and fan control options that are not all that powerful after all: there are predefined profiles with no manual tunning available. That makes me wonder how that software got accepted to be released in the first place?&lt;/p>
&lt;h2 id="the-linux-setup">The Linux Setup&lt;/h2>
&lt;p>The news somehow came to me that Linux kernel 5.11 has the support for the second fan that is present on some thinkpads&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. It encouraged me to roll up the sleeves and get the P53 fans under my control.&lt;/p>
&lt;p>The main pieces of the puzzle are the Linux kernel 5.11 and Kubuntu 21.04 pre-released: the kernel has the fans driver out of the box, and Kubuntu 21.04 provides Nvidia drivers that work with that kernel.&lt;/p>
&lt;p>Also, there are a couple helper utilities that I&amp;rsquo;ve used:&lt;/p>
&lt;ul>
&lt;li>Fancontrol GUI&lt;/li>
&lt;li>thinkfan (with GUI)&lt;/li>
&lt;li>CPU Power GUI&lt;/li>
&lt;/ul>
&lt;h3 id="linux-kernel">Linux kernel&lt;/h3>
&lt;p>I&amp;rsquo;ve installed 5.11 on Ubuntu 20.10 and it shouldn&amp;rsquo;t come to you as a surprise that Nvidia drivers were broken and the install script from the Nvidia website failed. So, a failed experiment.&lt;/p>
&lt;p>I&amp;rsquo;ve used the
&lt;a href="https://ubuntuhandbook.org/index.php/2020/08/mainline-install-latest-kernel-ubuntu-linux-mint/" target="_blank" rel="noopener">Mainline tool&lt;/a> to install the kernel.&lt;/p>
&lt;h3 id="kubuntu-2104">Kubuntu 21.04&lt;/h3>
&lt;p>When all hope was lost and I was about to go back to Windows for another half a year I got the last thing to try: why not upgrade Kubuntu to 21.04 which is not even in BETA? It would come with a newer kernel and most likely with Nvidia driver support.&lt;/p>
&lt;p>It came with kernel 5.10 and Nvidia drivers worked. Unfortunately, the second fan was not detected. Another failed experiment.&lt;/p>
&lt;p>The last hope was to ask Mainline to install kernel 5.11 and pray that Nvidia drivers are fine with it. Install, reboot, and voila: Nvidia driver works, an external monitor is detected (though only when connected with HDMI), and most importantly both fans are detected and controllable! Hurray!&lt;/p>
&lt;h3 id="fancontrol">Fancontrol&lt;/h3>
&lt;p>
&lt;a href="https://github.com/Maldela/fancontrol-gui" target="_blank" rel="noopener">Fancontrol&lt;/a> is a GUI utility that allows you to control and set up fan profiles by adjusting two dots: for example, the temperature at which to start/stop spinning the fans and when to max them out.&lt;/p>
&lt;p>&lt;img src="fancontrol-system-settings.png" alt="Fancontrol in KDE System Setting">&lt;/p>
&lt;p>Note: I&amp;rsquo;ve installed it from the source code.
Note: there is a standalone GUI but I like the integration into the KDE System Settings.&lt;/p>
&lt;h3 id="thinkfan">Thinkfan&lt;/h3>
&lt;p>Check the set up instructions
&lt;a href="https://gist.github.com/Yatoom/1c80b8afe7fa47a938d3b667ce234559" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>There is even the GUI for the
&lt;a href="https://github.com/scientifichackers/thinkfan-control-gui" target="_blank" rel="noopener">thinkfan&lt;/a>. This will give a window like this:&lt;/p>
&lt;p>&lt;img src="thinkfan-gui.png" alt="thinkfan GUI">&lt;/p>
&lt;p>Note the two lines that starts with &lt;code>Fan&lt;/code>. It means that two Thinkpad fans are detected and controlable.&lt;/p>
&lt;p>Note that with the &lt;code>thinkfan&lt;/code> it is up to you to write the settings into the file.&lt;/p>
&lt;h3 id="cpu-power-gui">CPU Power GUI&lt;/h3>
&lt;p>
&lt;a href="https://ubuntuhandbook.org/index.php/2020/11/cpupower-gui-simple-tool-adjust-cpu-frequency/" target="_blank" rel="noopener">CPU Power GUI&lt;/a> is a simple graphical utility that allows you to change the frequency limits of your CPU and its governor:
&lt;img src="cpupower-gui.png" alt="CPU frequency settings">&lt;/p>
&lt;p>Install it from the official Ubuntu repositories.&lt;/p>
&lt;pre>&lt;code class="language-bash">sudo apt install cpupower-gui
&lt;/code>&lt;/pre>
&lt;p>The trick here is that high CPU frequency leads to lots of heat which in turn spins the fans. When the frequency is limited not that much of the heat is created which prevents fans from kicking in. I&amp;rsquo;ve noticed that the minimal frequency (800 MHz) is enough to get some work done (e.g. zoom call). The temperature rarely goes up more than 60 degrees celsius.&lt;/p>
&lt;p>Of course, with a limited CPU frequency, the machine is noticeably slower, but hey, it is a high-end CPU. The slowdown is most noticeable when starting up an app. For example, IntelliJ works but feels bad with the 800MHz limit.&lt;/p>
&lt;h3 id="greenwithenvy">GreenWithEnvy&lt;/h3>
&lt;p>For the Nvidia fans the
&lt;a href="https://gitlab.com/leinardi/gwe" target="_blank" rel="noopener">GreenWithEnvy&lt;/a> looks promissing but I guess that the Quadro cards are not supported.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>It was a year-long development. Given the current setup, the tasks that are not super demanding can be done on a quiet machine where the only things I hear are my keyboard and the little Marshall Emberton speaker playing my favourite
&lt;a href="https://www.youtube.com/watch?v=ZkW-K5RQdzo" target="_blank" rel="noopener">Rammstein&lt;/a>
&lt;a href="https://www.youtube.com/watch?v=NeQM1c-XCDc" target="_blank" rel="noopener">tunes&lt;/a>. When my crazy performance test is set up, I allow the fans to spin at full speed and get the task done as fast as it can.&lt;/p>
&lt;h2 id="ps-nvidia-and-linux">P.S. Nvidia and Linux&lt;/h2>
&lt;p>Yeah, a laptop with an Nvidia GPU should be a red flag that prevents installing Linux in the first place. Of course, there are drivers and stuff, but in general good luck to have them properly working. However, Ubuntu gives you the setup that is worked out and works by default.&lt;/p>
&lt;p>Also, there are open source drivers called Nouveau but they have problems of their own like external monitor support.&lt;/p>
&lt;p>What is worse with this P53 is that &lt;strong>ALL&lt;/strong> video outputs are attached to the discrete GPU (what engineers in their right mind would do it?). This means that if drivers are not properly set-up Linux will able to output only 1 FPS while (for some reason) the mouse and some other windows (sometimes?) works without any lag.&lt;/p>
&lt;p>Sure, things like
&lt;a href="https://www.nvidia.com/en-us/geforce/technologies/optimus/technology/" target="_blank" rel="noopener">Optimus&lt;/a>,
&lt;a href="https://forums.developer.nvidia.com/t/the-all-new-outputsink-feature-aka-reverse-prime/129828" target="_blank" rel="noopener">reverse PRIME&lt;/a> exist but I can only wish you good luck setting up these.&lt;/p>
&lt;p>In summary, I wish there was a modification of the laptop that had only the integrated graphics.&lt;/p>
&lt;h2 id="pss-setup-on-the-table">P.S.S. Setup on the table&lt;/h2>
&lt;p>P53 is a big and fat laptop. Since most of the time I work from home due to covid-19, I don&amp;rsquo;t need to move or touch the machine that often. Therefore, it can stand on the table as a &amp;ldquo;triangle&amp;rdquo; (see a picture). This position allows air to circulate a little better. Also, in this way, it takes somewhat less space on the table. On the other hand, I can&amp;rsquo;t use the laptop screen and the keyword.&lt;/p>
&lt;p>&lt;img src="thinkpad-setup.jpg" alt="My Table setup">&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="https://certification.ubuntu.com/hardware/202002-27747">https://certification.ubuntu.com/hardware/202002-27747&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>I&amp;rsquo;ve tried several other distros (like Fedora 33) but without any luck. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>&lt;a href="https://www.phoronix.com/scan.php?page=news_item&amp;amp;px=ThinkPad-Dual-Fan-Control-5.8">https://www.phoronix.com/scan.php?page=news_item&amp;amp;px=ThinkPad-Dual-Fan-Control-5.8&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>lmgrep - Lucene Based grep-like Utility</title><link>https://www.jocas.lt/blog/post/intro-to-lucene-grep/</link><pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/intro-to-lucene-grep/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>What if &lt;code>grep&lt;/code> supported the functionality of a proper search engine like
&lt;a href="https://www.elastic.co/elasticsearch/" target="_blank" rel="noopener">Elasticsearch&lt;/a> without a need to install any servers or index the files before searching?
&lt;a href="https://github.com/dainiusjocas/lucene-grep" target="_blank" rel="noopener">&lt;code>lmgrep&lt;/code>&lt;/a> aims to provide you just that.
It is installed as just one executable file without any dependencies, provides a command-line interface, starts-up instantly, and works on macOS, Linux, and, yes, even Windows.&lt;/p>
&lt;h2 id="my-motivation">My motivation&lt;/h2>
&lt;p>Have you ever wished that &lt;code>grep&lt;/code> supported
&lt;a href="https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation" target="_blank" rel="noopener">tokenization&lt;/a>,
&lt;a href="https://en.wikipedia.org/wiki/Stemming" target="_blank" rel="noopener">stemming&lt;/a>, etc, so that you don&amp;rsquo;t have to write wildcard
&lt;a href="https://en.wikipedia.org/wiki/Regular_expression" target="_blank" rel="noopener">regular expressions&lt;/a> all the time? I&amp;rsquo;ve also shared that question and on a one nice day, I&amp;rsquo;ve tried to scratch that itch by exposing the
&lt;a href="https://lucene.apache.org/" target="_blank" rel="noopener">Lucene&lt;/a> query syntax as a CLI utility. &lt;code>lmgep&lt;/code> is the result of my effort.
&lt;a href="https://github.com/dainiusjocas/lucene-grep" target="_blank" rel="noopener">Give it a try&lt;/a> and let me know how it goes.&lt;/p>
&lt;h2 id="full-text-search-vs-grep">Full-text Search vs. &lt;code>grep&lt;/code>&lt;/h2>
&lt;p>I&amp;rsquo;m perfectly aware that comparing Lucene and &lt;code>grep&lt;/code> is like comparing apples to oranges. However, I think that &lt;code>lmgrep&lt;/code> is best compared with the very tool that inspired it, namely &lt;code>grep&lt;/code>.&lt;/p>
&lt;p>Anyway, what does &lt;code>grep&lt;/code> do? &lt;code>grep&lt;/code> reads a line from &lt;code>stdin&lt;/code>, examines the line to see if it should be forwarded to &lt;code>stdout&lt;/code>, and repeats until stdin is exhausted&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. &lt;code>lmgrep&lt;/code> tries to mimick exactly that functionality. Of course, there are many more options to &lt;code>grep&lt;/code> but it is the essence of the tool.&lt;/p>
&lt;p>Several notable advantages of &lt;code>lmgrep&lt;/code> over &lt;code>grep&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Lucene query syntax is better suited for full-text search;&lt;/li>
&lt;li>Boolean operators allow to construct complex, well-designed queries;&lt;/li>
&lt;li>Text analysis can be customized to the language of the documents;&lt;/li>
&lt;li>Fuzzy text searches;&lt;/li>
&lt;li>Flexible text analysis pipeline that includes, lowercasing,
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-asciifolding-tokenfilter.html" target="_blank" rel="noopener">ASCII-folding&lt;/a>, stemming, etc;&lt;/li>
&lt;li>regular expressions can be combined with other Lucene query components;&lt;/li>
&lt;li>Search matches can span multiple lines, i.e. search is not line-oriented.&lt;/li>
&lt;/ul>
&lt;p>Several notable limitations of &lt;code>lmgrep&lt;/code> when compared to &lt;code>grep&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>grep&lt;/code> is faster when it comes to raw speed for large text files;&lt;/li>
&lt;li>&lt;code>grep&lt;/code> has a smaller memory footprint;&lt;/li>
&lt;li>Not all options of &lt;code>grep&lt;/code> are supported;&lt;/li>
&lt;/ul>
&lt;h2 id="why-lucene">Why Lucene?&lt;/h2>
&lt;p>Lucene is a Java library that provides indexing and search features. Lucene has been more than 20 years in development and it is the library that powers many search applications. Also, many developers are already familiar with the Lucene query syntax and know how to leverage it to solve complicated information retrieval problems.&lt;/p>
&lt;p>However powerful Lucene is, it is not well-suited for CLI application. The main problem is the startup time of JVM. To reduce the startup time I&amp;rsquo;ve compiled &lt;code>lmgrep&lt;/code> with the &lt;code>native-image&lt;/code> tool provided by
&lt;a href="https://www.graalvm.org/" target="_blank" rel="noopener">GraalVM&lt;/a>. In this way, the startup time is around 0.01s for Linux, macOS, and Windows.&lt;/p>
&lt;h2 id="how-does-lmgrep-work">How does &lt;code>lmgrep&lt;/code> work?&lt;/h2>
&lt;p>&lt;code>lmgrep&lt;/code> by default expects two parameters: a search query and a
&lt;a href="https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystem.html#getPathMatcher-java.lang.String-" target="_blank" rel="noopener">GLOB pattern&lt;/a> (similar to regexp) to find files to execute &lt;code>lmgrep&lt;/code> on. I assume that the dear reader doesn&amp;rsquo;t want to be tortured by reading the explanation on how the file names are being matched with GLOB, so I&amp;rsquo;ll skip it. Instead, I&amp;rsquo;ll focus on explaining how the search works within a file.&lt;/p>
&lt;p>&lt;code>lmgrep&lt;/code> creates a
&lt;a href="https://lucene.apache.org/core/8_7_0/monitor/org/apache/lucene/monitor/Monitor.html" target="_blank" rel="noopener">Lucene Monitor (Monitor)&lt;/a> object from the provided search query. Then text file is split into lines&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Each line of text is passed to the Monitor for searching. The Monitor then creates an in-memory Lucene index with a single document created out of the line of text. Then the Monitor runs the search query on that in-memory index in the good ol' Lucene way&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. &lt;code>lmgrep&lt;/code> takes the hits, formats them, and sends results to &lt;code>STDOUT&lt;/code>. That is how &lt;code>lmgrep&lt;/code> does the full-text search.&lt;/p>
&lt;p>The overall searching approach is similar to the one of
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-percolate-query.html" target="_blank" rel="noopener">Percolator&lt;/a> in Elasticsearch. &lt;code>lmgrep&lt;/code> just limits the number of stored search queries to one and treats every text line as a document. A cool thing compared with the Percolator is that &lt;code>lmgrep&lt;/code> provides exact offsets of the matched terms while Elasticsearch does not expose offsets when highlighting.&lt;/p>
&lt;p>The described procedure seems to be somewhat inefficient. However, the &lt;strong>query parsing&lt;/strong> for &lt;strong>all&lt;/strong> the lines (and files) is done only once. Also, the searching itself is efficient thanks to Lucene in general and when queries are complicated thanks to the
&lt;a href="https://lucene.apache.org/core/8_2_0/monitor/org/apache/lucene/monitor/Presearcher.html" target="_blank" rel="noopener">Presearcher&lt;/a> of the Lucene Monitor in particular. Presearcher extracts terms from the search query and if none of these terms are in the index then a full query is not executed at all. Of course, many optimizations can be (and will be) implemented for &lt;code>lmgrep&lt;/code> such as batching of the documents. In general, the performance is limited by the Lucene Monitor.&lt;/p>
&lt;p>What about the text analysis pipeline? By default, &lt;code>lmgrep&lt;/code> uses the &lt;code>StandardTokenizer&lt;/code> to tokenize text. Then the tokens are passed through several token filters in the following order: &lt;code>LowerCaseFilter&lt;/code>, &lt;code>ASCIIFoldingFilter&lt;/code>, and &lt;code>SnowballFilter&lt;/code> which is given the &lt;code>EnglishStemmer&lt;/code>. The same analysis pipeline is used for both the indexing and querying. All the components of the analysis pipeline are configurable via CLI flags, see the
&lt;a href="https://github.com/dainiusjocas/lucene-grep/blob/main/README.md#supported-tokenizers" target="_blank" rel="noopener">README&lt;/a>. However, the order of the token filters, as of now, is not configurable. Moreover, various filters are not exposed at all (e.g. &lt;code>StopwordsFilter&lt;/code>, or
&lt;a href="https://lucene.apache.org/core/7_4_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/WordDelimiterGraphFilter.html" target="_blank" rel="noopener">WordDelimiterGraphFilter&lt;/a>, etc.). Supporting a more flexible analysis pipeline configuration is left out for future releases. The more users the tool has the faster new features will be implemented ;)&lt;/p>
&lt;h2 id="prehistory-of-the-lmgrep">Prehistory of the &lt;code>lmgrep&lt;/code>&lt;/h2>
&lt;p>Almost every NLP project that I&amp;rsquo;ve worked on had the component called &lt;strong>dictionary annotator&lt;/strong>. Also, the vast majority of the projects used Elasticsearch in one way or another. The more familiar I&amp;rsquo;ve got with Elasticsearch I&amp;rsquo;ve got, the more of my NLP workload shifted towards implementing it inside Elasticsearch. One day I&amp;rsquo;ve discovered a tool called
&lt;a href="https://github.com/flaxsearch/luwak" target="_blank" rel="noopener">Luwak&lt;/a> (a cool name isn&amp;rsquo;t it?) and read
&lt;a href="https://web.archive.org/web/20201124175132/https://www.flax.co.uk/blog/2016/03/08/helping-bloomberg-build-real-time-news-search-engine/" target="_blank" rel="noopener">more about it&lt;/a>. It kind of opened my eyes: the dictionary annotator can be implemented using Elasticsearch and the dictionary entries can be expressed as Elasticsearch queries. Thankfully, Elasticsearch has Percolator that hides all the complexity of managing temporary indices, batching search requests, etc.&lt;/p>
&lt;p>Then I was given was an NLP project where one of the requirements was to implement data analysis using AWS serverless stuff: Lambda for text processing and Dynamo DB for storage. Of course, one of the required NLP components was a dictionary annotator. Since Elasticsearch was not available (because it is not serverless) I still wanted to continue working with dictionary entries as search queries, I&amp;rsquo;ve decided to leverage the Luwak library. From experiences of that project, the
&lt;a href="https://web.archive.org/web/20201124175132/https://www.flax.co.uk/blog/2016/03/08/helping-bloomberg-build-real-time-news-search-engine/" target="_blank" rel="noopener">Beagle&lt;/a> library was born. &lt;code>lmgrep&lt;/code> is loosely based on Beagle.&lt;/p>
&lt;p>When thinking about how to implement &lt;code>lmgrep&lt;/code> I wanted it to be based on Lucene because of the full-text search features. To provide a good experience the start-up time must be small. To achieve it, &lt;code>lmgrep&lt;/code> had to be compiled with the &lt;code>native-image&lt;/code> tool of the GraalVM. I&amp;rsquo;ve tried but the &lt;code>native-image&lt;/code> doesn&amp;rsquo;t support
&lt;a href="https://web.archive.org/web/20201124175132/https://www.flax.co.uk/blog/2016/03/08/helping-bloomberg-build-real-time-news-search-engine/" target="_blank" rel="noopener">Method Handles&lt;/a> that Lucene uses. Some more hacking was needed. I was lucky when I&amp;rsquo;ve discovered a
&lt;a href="https://web.archive.org/web/2/https://www.morling.dev/blog/how-i-built-a-serverless-search-for-my-blog/" target="_blank" rel="noopener">toy project&lt;/a> where the blog search was implemented on AWS Lambda that was backed by Lucene which was compiled by the &lt;code>native-image&lt;/code> tool. I&amp;rsquo;ve cloned the repo, &lt;code>mvnw install&lt;/code>, then included the artefacts to the dependencies list, and &lt;code>lmgrep&lt;/code> compiled with the &lt;code>native-image&lt;/code> tool successfully.&lt;/p>
&lt;p>Then the most complicated part was to prepare executable binaries for different operating systems. Plenty of CPU, RAM, VirtualBox with Windows and macOS virtual machines, and
&lt;a href="https://github.com/dainiusjocas/lucene-grep/releases/tag/v2021.01.24" target="_blank" rel="noopener">here we go&lt;/a>.&lt;/p>
&lt;p>Did I say how much I enjoyed trying to get stuff done on Windows? None at all. How come that multiple different(!) command prompts are needed to get GraalVM to compile an executable? Now I know that it would a lot better to suffer the pain and to set up the Github Actions pipeline to compile the binaries and upload them to release pages.&lt;/p>
&lt;h2 id="what-is-missing">What is missing?&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> The analysis pipeline is not as flexible as I&amp;rsquo;d like to (UPDATE 2021-04-24:
&lt;a href="https://github.com/dainiusjocas/lucene-grep/pull/81" target="_blank" rel="noopener">implemented&lt;/a>);&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Leverage the multicore CPUs by executing the search in parallel;&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Batch documents for matching;&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Let me know if any?&lt;/li>
&lt;/ul>
&lt;h2 id="what-would-be-cool-ways-to-use-lmgrep">What would be cool ways to use &lt;code>lmgrep&lt;/code>?&lt;/h2>
&lt;ul>
&lt;li>&lt;code>tail&lt;/code> logs to &lt;code>lmgrep&lt;/code> and raise alerts;&lt;/li>
&lt;li>Give an alias for &lt;code>lmgrep&lt;/code> with various options tailored for the code search (Java Example);&lt;/li>
&lt;li>Why not expose
&lt;a href="https://github.com/borkdude/sci" target="_blank" rel="noopener">sci&lt;/a> script as TokenFilter?&lt;/li>
&lt;li>Why not
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenfilter.html" target="_blank" rel="noopener">ngrams token filter&lt;/a> then the search would be resilient to the typing errors?&lt;/li>
&lt;li>Static website search, like AWS Lambda that has lmgrep and goes through all files on demand without upfront indexing.&lt;/li>
&lt;/ul>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>&lt;code>lmgrep&lt;/code> scratched my itch. It was exciting to get it working. I hope that you&amp;rsquo;ll also find it interesting and maybe useful. Give it a try, let me know how it was for you, and most importantly any feedback welcome on how to improve &lt;code>lmgrep&lt;/code>.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://web.archive.org/web/20210116173133/https://swtch.com/~rsc/regexp/regexp4.html">https://web.archive.org/web/20210116173133/https://swtch.com/~rsc/regexp/regexp4.html&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://web.archive.org/web/20161018234331/http://www.techrepublic.com/article/graduating-from-grep-powerful-text-file-searching-with-isearch/">https://web.archive.org/web/20161018234331/http://www.techrepublic.com/article/graduating-from-grep-powerful-text-file-searching-with-isearch/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="https://ideolalia.com/essays/composition-is-interpretation.html">https://ideolalia.com/essays/composition-is-interpretation.html&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>there is no necessity to split text files into lines, it is just to mimik how &lt;code>grep&lt;/code> operates. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>of course, the description is over-simplified, but it is accurate enough to get the overall idea. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>How to Prevent Data Corruption in Elasticsearch When Using Kafka Connect Elasticsearch Sink Connector</title><link>https://www.jocas.lt/blog/post/kc_es_data_consistency/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/kc_es_data_consistency/</guid><description>&lt;h3 id="tldr">TL;DR&lt;/h3>
&lt;p>When the
&lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch" target="_blank" rel="noopener">Elasticsearch indexer&lt;/a> is highly concurrent, Kafka record keys are used as Elasticsearch document IDs, and indexer is set to delete records on &lt;code>null&lt;/code> values, then Kafka Connect Elasticsearch Sink Connector might corrupt your data: documents that should not be deleted end up being deleted, or documents that should be deleted end up still being present in the index. The fix is to use external versioning for deletes in bulk requests as it is proposed in this
&lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/pull/422" target="_blank" rel="noopener">Github Pull Request&lt;/a>.&lt;/p>
&lt;h3 id="the-problem">The problem&lt;/h3>
&lt;p>NOTE: as of version 6.0.0 of the Confluent Platform (last checked on 2020-10-02) the bug that might lead to data corruption is still present.&lt;/p>
&lt;p>Let&amp;rsquo;s focus on a use case where Kafka record key is used as an Elasticsearch document ID&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. I would consider this to be a proper practice when the documents represent a catalog of things.&lt;/p>
&lt;p>Elasticsearch uses
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/optimistic-concurrency-control.html" target="_blank" rel="noopener">optimistic concurrency control&lt;/a>. The job of this concurrency mechanism is to ensure that older version of the document doesn&amp;rsquo;t override a newer version. By default, order of arrival of the operation is applied, but the behaviour can be overriden in
&lt;a href="https://www.elastic.co/blog/elasticsearch-versioning-support" target="_blank" rel="noopener">several ways&lt;/a> depending on the version of Elasticsearch. In this post we focus on concurrent bulk requests, and with a concurrency that involves a network, requests will sometimes arrive out of order.&lt;/p>
&lt;p>To help Elasticsearch resolve the out-of-order indexing requests Kafka Connect Elasticsearch Sink Connector (from here on Kafka Connect for short) leverages the
&lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/blob/7256e9473cea690c373058b88fffd1111870cfe6/src/main/java/io/confluent/connect/elasticsearch/jest/JestElasticsearchClient.java#L564" target="_blank" rel="noopener">&lt;code>external&lt;/code> document&lt;/a> versioning&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Using external versions in Kafka Connect makes sense because we already have versioning in place: Kafka topic partition offsets. If Kafka Connect applies changes to Elasticsearch indices in order of the topic offset, then any update ordering problems would be problems in the upstream system. This is a good guarantee to have.&lt;/p>
&lt;p>Let&amp;rsquo;s add to the mix delete operations. Kafka Connect supports a setting &lt;code>BEHAVIOR_ON_NULL_VALUES_CONFIG&lt;/code> to &lt;code>&amp;quot;delete&amp;quot;&lt;/code>. This setting instructs the Kafka Connect that a document in Elasticsearch with an ID of the kafka record key with &lt;code>null&lt;/code> value (a tombstone message) is going to be deleted. But for some strange reason the deletes &lt;strong>does not use external versioning&lt;/strong>! The line responsible for the described behaviour&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> can be found
&lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/blob/7256e9473cea690c373058b88fffd1111870cfe6/src/main/java/io/confluent/connect/elasticsearch/jest/JestElasticsearchClient.java#L554" target="_blank" rel="noopener">here&lt;/a>. This means that for deletes the order-of-arrival wins. Let&amp;rsquo;s increase the concurrency of bulk requests with the param &lt;code>MAX_IN_FLIGHT_REQUESTS_CONFIG&lt;/code> to a largish number, and the data consistency problems is just round the corner for data that has some largish update ratio.&lt;/p>
&lt;p>The issue is even more pronounced when you re-index data into Elasticsearch and you want to do it as fast as possible, which means doing the indexing concurrently.&lt;/p>
&lt;h3 id="the-example">The Example&lt;/h3>
&lt;p>The code that demonstrated the faulty behaviour can be found in this
&lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/pull/422" target="_blank" rel="noopener">Pull Request&lt;/a>.&lt;/p>
&lt;p>The test case is for testing the case when document should be present in Elasticsearch gets deleted.&lt;/p>
&lt;p>Let&amp;rsquo;s have a little walk over the code snippet:&lt;/p>
&lt;pre>&lt;code class="language-java">Collection&amp;lt;SinkRecord&amp;gt; records = new ArrayList&amp;lt;&amp;gt;();
for (int i = 0; i &amp;lt; numOfRecords - 1 ; i++) {
if (i % 2 == 0) {
SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, null, i);
records.add(sinkRecord);
} else {
record.put(&amp;quot;message&amp;quot;, Integer.toString(i));
SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, i);
records.add(sinkRecord);
}
}
record.put(&amp;quot;message&amp;quot;, Integer.toString(numOfRecords));
SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, numOfRecords);
records.add(sinkRecord);
task.put(records);
task.flush(null);
&lt;/code>&lt;/pre>
&lt;p>Here we send &lt;code>numOfRecords&lt;/code> (which larger than 2) to a Kafka topic. Every second record has &lt;code>null&lt;/code> body (delete operation), and the rest of the records have a sequence number as a &lt;code>message&lt;/code> value. The very last record is &lt;strong>always&lt;/strong> a non-null record with a &lt;code>message&lt;/code> value of &lt;code>numOfRecords&lt;/code>.&lt;/p>
&lt;p>Let&amp;rsquo;s setup a connector:&lt;/p>
&lt;pre>&lt;code class="language-java">KEY_IGNORE_CONFIG = &amp;quot;false&amp;quot;;
MAX_IN_FLIGHT_REQUESTS_CONFIG = Integer.toString(numOfRecords)
BATCH_SIZE_CONFIG = &amp;quot;1&amp;quot;
LINGER_MS_CONFIG = &amp;quot;1&amp;quot;
BEHAVIOR_ON_NULL_VALUES_CONFIG = &amp;quot;delete&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Here we set a connector to use Kafka record key as id &lt;code>KEY_IGNORE_CONFIG = &amp;quot;false&amp;quot;&lt;/code>, set the indexer concurrency to the &lt;code>numOfRecords&lt;/code>; set the indexing batch size to 1 (this creates as many requests to Elasticsearch as there are records in the Kafka topic); set indexer to send requests immediately with &lt;code>LINGER_MS_CONFIG = &amp;quot;1&amp;quot;&lt;/code>; and record with a &lt;code>null&lt;/code> value represents a delete operation.&lt;/p>
&lt;p>With this setup after the indexing is done we expect that in the index we have a document with ID and whose &lt;code>message&lt;/code> value is &lt;code>numOfRecords&lt;/code>. But when ordering of bulk requests is out-of-order then at the end we might have a situation where there is no document in the index at all: the bulk index request with &lt;code>message = numOfRecords&lt;/code> arrived before one of the bulk requests with a delete operation!&lt;/p>
&lt;p>The situation might seem to be a bit far-fetched but for applications like e-commerce where you have a catalog that is frequently updated (e.g. the catalog item should be available in search or not) and updates are modelled as document deletes it happens a bit more often than it might be expected.&lt;/p>
&lt;h3 id="the-fix">The fix&lt;/h3>
&lt;p>The fix is simple: use the same external versioning that is already being used by the indexing requests also for delete requests:&lt;/p>
&lt;pre>&lt;code class="language-java">if (record.version != null) {
req.setParameter(&amp;quot;version_type&amp;quot;, &amp;quot;external&amp;quot;).setParameter(&amp;quot;version&amp;quot;, record.version);
}
&lt;/code>&lt;/pre>
&lt;p>The full code can be found
&lt;a href="ttps://github.com/confluentinc/kafka-connect-elasticsearch/pull/422">here&lt;/a>. Let&amp;rsquo;s hope that Confluent developers will find some time to merge that PR.&lt;/p>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>Thank you for reading and leave your feedback
&lt;a href="https://github.com/dainiusjocas/blog/issues/12" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;h3 id="ps">P.S.&lt;/h3>
&lt;p>Of course, this is not the only situation when data can get
&lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/issues" target="_blank" rel="noopener">corrupted&lt;/a>, e.g. changing the number of partitions; when you delete the topic, repopulate it with up-to-date data (also, you skip deletes) then restarting the indexing might pretty much nothing, because all the versions are earlier &lt;code>external version&lt;/code> because offsets are smaller.&lt;/p>
&lt;h3 id="heading">&lt;/h3>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>When Kafka Record keys are not used as Elasticsearch document IDs versioning is not a problem because every Elasticsearch ID is constructed as &lt;code>{topic}+{partition}+{offset}&lt;/code> which creates a new document for every Kafka record, i.e. no versioning is needed. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Elasticsearch 7 supports the
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-7.0.html#_internal_versioning_is_no_longer_supported_for_optimistic_concurrency_control" target="_blank" rel="noopener">external versioning&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Yes, it is a comment, and it means that the developers were not sure whether to use external versioning for delete operations. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>How to Use Elasticsearch Ingest Pipelines with Kafka Connect Elasticsearch Sink Connector</title><link>https://www.jocas.lt/blog/post/ingest_pipeline_kafka_connect/</link><pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/ingest_pipeline_kafka_connect/</guid><description>&lt;h3 id="tldr">TL;DR&lt;/h3>
&lt;p>Specify your pipeline with the
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#dynamic-index-settings" target="_blank" rel="noopener">&lt;code>index.default_pipeline&lt;/code>&lt;/a> setting in the index (or index template) settings.&lt;/p>
&lt;h3 id="the-problem">The Problem&lt;/h3>
&lt;p>We need to index the log data into the
&lt;a href="https://www.elastic.co/" target="_blank" rel="noopener">Elasticsearch&lt;/a> cluster using a
&lt;a href="https://docs.confluent.io/current/connect/kafka-connect-elasticsearch/index.html" target="_blank" rel="noopener">Kafka Connect Elasticsearch Sink Connector&lt;/a> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, the data should be split into daily indices, and we need to specify the Elasticsearch ingest pipeline.&lt;/p>
&lt;p>The
&lt;a href="https://docs.confluent.io/current/connect/kafka-connect-elasticsearch/configuration_options.html" target="_blank" rel="noopener">documentation of the connector&lt;/a> doesn&amp;rsquo;t mention anything about ingest pipelines. After a quick consultation with the Internet you discover that there is an open
&lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/issues/72" target="_blank" rel="noopener">issue&lt;/a> that Kafka Connect Elasticsearch Sink Connector doesn&amp;rsquo;t support specifying an Elasticsearch ingest pipeline. WAT?&lt;/p>
&lt;h3 id="the-workaround">The Workaround&lt;/h3>
&lt;p>Say&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, our pipeline&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> just renames an attribute, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-shell">PUT _ingest/pipeline/my_pipeline_id
{
&amp;quot;description&amp;quot; : &amp;quot;renames the field name&amp;quot;,
&amp;quot;processors&amp;quot; : [
{
&amp;quot;rename&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;original_field_name&amp;quot;,
&amp;quot;target_field&amp;quot;: &amp;quot;target_field_name&amp;quot;
}
}
]
}
&lt;/code>&lt;/pre>
&lt;p>The Elasticsearch ingest pipeline for indexing can be specified in several ways:&lt;/p>
&lt;ol>
&lt;li>for each index request as a
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html" target="_blank" rel="noopener">URL parameter&lt;/a>,&lt;/li>
&lt;li>per bulk index request as a URL parameter,&lt;/li>
&lt;li>for every
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html#docs-bulk-api-query-params" target="_blank" rel="noopener">bulk index request operation&lt;/a>,&lt;/li>
&lt;li>index settings (
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#dynamic-index-settings" target="_blank" rel="noopener">a dynamic attribute&lt;/a>),&lt;/li>
&lt;li>index template.&lt;/li>
&lt;/ol>
&lt;p>First three options are not supported by Kafka Connect. The fourth option is not convenient in our case because the data should be split into time-based (e.g. daily) indices and we don&amp;rsquo;t want to do repetitive tasks&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The natural option to follow is to define an
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-templates.html" target="_blank" rel="noopener">index template&lt;/a>. In the index template we can specify the &lt;code>index.default_pipeline&lt;/code> parameter, e.g.&lt;/p>
&lt;pre>&lt;code class="language-shell">PUT _index_template/template_1
{
&amp;quot;index_patterns&amp;quot;: [&amp;quot;daily_log*&amp;quot;],
&amp;quot;template&amp;quot;: {
&amp;quot;settings&amp;quot;: {
&amp;quot;index.default_pipeline&amp;quot;: &amp;quot;my_pipeline_id&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note, that for indexing not to fail, we should create the Elasticsearch ingest pipeline&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> &lt;strong>before&lt;/strong> setting up the index template.&lt;/p>
&lt;p>That is it, now when Kafka Connect will create a new daily index the Elasticsearch ingest pipeline is going to be applied to every document without any issues, for free, and in no time.&lt;/p>
&lt;h3 id="bonus">Bonus&lt;/h3>
&lt;p>One thing to note is that only one pipeline can be specified for &lt;code>index.default_pipeline&lt;/code> which might sound a bit limiting. A clever trick to overcome that limitation is to use a series of
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/pipeline-processor.html" target="_blank" rel="noopener">pipeline processors&lt;/a> that can invoke other pipelines in the specified order, i.e. pipeline of pipelines.&lt;/p>
&lt;p>Also, there is an index setting called &lt;code>index.final_pipeline&lt;/code> that if specified is going to be executed after all other pipelines.&lt;/p>
&lt;p>Testing pipelines can be done using the
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html" target="_blank" rel="noopener">&lt;code>_simulate&lt;/code> API&lt;/a>.&lt;/p>
&lt;h3 id="fin">Fin&lt;/h3>
&lt;p>Thanks for reading and leave comments or any other feedback on this blog post in the
&lt;a href="https://github.com/dainiusjocas/blog/issues/9" target="_blank" rel="noopener">Github issue&lt;/a>. Examples were tested to work with Elasticsearch and Kibana 7.8.1.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>or any other technology that doesn&amp;rsquo;t support, or it is just not possible to specify the Elasticsearch ingest pipeline. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>yes, I know that the same job can be done with the
&lt;a href="https://docs.confluent.io/current/connect/transforms/index.html" target="_blank" rel="noopener">Kafka Connect Transformations&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>let&amp;rsquo;s leave out the Kafka Connector setup. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>set &lt;code>index.default_pipeline=my_pipeline_id&lt;/code> for every new daily index with, say, a cron-job at midnight. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>technically, before an index is created that matches the template pattern. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>A Neat Trick with Elasticsearch Normalizers</title><link>https://www.jocas.lt/blog/post/elasticsearch-normlizers/</link><pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/elasticsearch-normlizers/</guid><description>&lt;p>To analyze the textual data Elasticsearch uses &lt;strong>analyzers&lt;/strong> while for the keyword analysis there is a thing called a &lt;strong>normalizer&lt;/strong>. In this article I&amp;rsquo;ll explain what the normalizer is and show it&amp;rsquo;s use case for &lt;strong>normalizing&lt;/strong> URLs.&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>A neat use case for keyword normalizers is to extract a specific part of the URL with a char_filter of the &lt;code>pattern_replace&lt;/code> type.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In Elasticsearch the textual data is represented with two data types: &lt;code>text&lt;/code> and &lt;code>keyword&lt;/code>. The &lt;code>text&lt;/code> type is meant to be used for full-text search use cases while &lt;code>keyword&lt;/code> is mean for filtering, sorting, and aggregation.&lt;/p>
&lt;h3 id="tldr-about-analyzers">TL;DR About Analyzers&lt;/h3>
&lt;p>To make a better use of &lt;code>text&lt;/code> data you can setup the
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer-anatomy.html" target="_blank" rel="noopener">analyzer&lt;/a> which is a combination of three components:&lt;/p>
&lt;ul>
&lt;li>exactly one &lt;strong>tokenizer&lt;/strong>,&lt;/li>
&lt;li>zero or more &lt;strong>character filters&lt;/strong>,&lt;/li>
&lt;li>zero or more &lt;strong>token filters&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>Basically, an analyzer transforms a single &lt;em>string&lt;/em> into &lt;em>words&lt;/em>, e.g. &lt;code>&amp;quot;This is my text&amp;quot;&lt;/code> can be transformed into &lt;code>[&amp;quot;this&amp;quot;, &amp;quot;my&amp;quot;, &amp;quot;text&amp;quot;]&lt;/code> which you can read as:&lt;/p>
&lt;ul>
&lt;li>text is split into tokens by tokenizer,&lt;/li>
&lt;li>each token is lowercased with the a token filter,&lt;/li>
&lt;li>stopwords are removed with another token filter.&lt;/li>
&lt;/ul>
&lt;h3 id="normalizers">Normalizers&lt;/h3>
&lt;p>The
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-normalizers.html" target="_blank" rel="noopener">documentation&lt;/a> says that:&lt;/p>
&lt;blockquote>
&lt;p>Normalizers are similar to analyzers except that they may only emit a single token.&lt;/p>
&lt;/blockquote>
&lt;p>Normalizers can only be applied to the &lt;code>keyword&lt;/code> datatype. The cannonical use case is to lowercase structured content such as IDs, email addresses, e.g. a database stores emails in whatever case but searching for emails should be case insensitive. Note that only a subset of available filters can be used by a normalizer: all filters must work on a &lt;strong>per-character basis&lt;/strong>, i.e. no stopwords or stemmers.&lt;/p>
&lt;h3 id="normalizers-for-normalizing-url-data">Normalizers for Normalizing URL Data&lt;/h3>
&lt;p>Storing a URL in a &lt;code>keyword&lt;/code> field allows to filter, sort, and aggregate your data per URL. But what if you need to filter, sort, and aggregate by just one part of the URL and you have little to no control over the upstream data source? You have a couple of options:&lt;/p>
&lt;ul>
&lt;li>convince upstream to extract that one part in their code and send it to you,&lt;/li>
&lt;li>setup a &lt;code>text&lt;/code> field with an analyzer that produces just that one token and enable field data (not a default setup and can get expensive).&lt;/li>
&lt;li>setup a &lt;code>keyword&lt;/code> field with a normalizer with a &lt;code>char_filter&lt;/code>.&lt;/li>
&lt;li>give up.&lt;/li>
&lt;/ul>
&lt;p>I want to explore the &lt;code>keyword&lt;/code> option. In the next section I&amp;rsquo;ll show how to setup normalizers for Elasticsearch URLs.&lt;/p>
&lt;h3 id="the-not-so-synthetic-problem">The not so Synthetic Problem&lt;/h3>
&lt;p>We have a list URLs without a hostname that were used to query Elasticsearch, e.g.: &lt;code>/my_search_index/_search?q=elasticsearch&lt;/code> and we need to split URLs into parts such as: index, operation endpoint, e.g.: &lt;code>_search&lt;/code> or &lt;code>_count&lt;/code>, query filters, etc. In the following example I&amp;rsquo;ll focus on the extracting the index part of the URL.&lt;/p>
&lt;p>Let&amp;rsquo;s create an index:&lt;/p>
&lt;pre>&lt;code>PUT elasticsearch_url_index
{
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;index_extractor_normalizer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;char_filter&amp;quot;: [
&amp;quot;index_name_extractor&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;index_name_extractor&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;/(.+)/.*&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;$1&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot; : {
&amp;quot;properties&amp;quot;: {
&amp;quot;url&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;index_extractor_normalizer&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Here we setup the index with a normalizer &lt;code>index_extractor_normalizer&lt;/code> that has a char filter &lt;code>index_name_extractor&lt;/code> that uses a regex &lt;code>pattern_replace&lt;/code> to extract characters between the first and the second slashes. The mappings have a property &lt;code>url&lt;/code> which is of the &lt;code>keyword&lt;/code> type and have a field &lt;code>index&lt;/code> which is also of the &lt;code>keyword&lt;/code> type and is set up to use the normalizer &lt;code>index_extractor_normalizer&lt;/code>.&lt;/p>
&lt;p>Since the normalizer is basically a collection of filters we can use our good old friend &lt;code>_analyze&lt;/code> API to test how it works.&lt;/p>
&lt;pre>&lt;code>POST elasticsearch_url_index/_analyze
{
&amp;quot;char_filter&amp;quot;: [&amp;quot;index_name_extractor&amp;quot;],
&amp;quot;text&amp;quot;: [&amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;my_search_index&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 40,
&amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
&amp;quot;position&amp;quot; : 0
}
]
}
&lt;/code>&lt;/pre>
&lt;p>Good, exactly as we wanted: &lt;code>/my_search_index/_search?q=elasticsearch&lt;/code> =&amp;gt; &lt;code>my_search_index&lt;/code>.&lt;/p>
&lt;p>Let&amp;rsquo;s index some data:&lt;/p>
&lt;pre>&lt;code>PUT elasticsearch_url_index/_doc/0
{
&amp;quot;url&amp;quot;: &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s try to filter URLs by the index name:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url:my_search_index
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 0,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No results? What? Oh! Wrong field: &lt;code>url&lt;/code> was used instead of &lt;code>url.index&lt;/code>. Let&amp;rsquo;s try once again:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url.index:my_search_index
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.2876821,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;elasticsearch_url_index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;0&amp;quot;,
&amp;quot;_score&amp;quot; : 0.2876821,
&amp;quot;_source&amp;quot; : {
&amp;quot;url&amp;quot; : &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected. Cool.&lt;/p>
&lt;h3 id="bonus-a-trick-with-the-docvalue_fields">Bonus: a Trick with the &lt;code>docvalue_fields&lt;/code>&lt;/h3>
&lt;p>Another neat trick is that we can get out the &lt;code>index&lt;/code> part of the URL from an Elasticsearch index using the &lt;code>docvalue_fields&lt;/code> option in a request ,e.g.:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url.index:my_search_index
{
&amp;quot;docvalue_fields&amp;quot;: [&amp;quot;url.index&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.2876821,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;elasticsearch_url_index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;0&amp;quot;,
&amp;quot;_score&amp;quot; : 0.2876821,
&amp;quot;_source&amp;quot; : {
&amp;quot;url&amp;quot; : &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
},
&amp;quot;fields&amp;quot; : {
&amp;quot;url.index&amp;quot; : [
&amp;quot;my_search_index&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The important part is this one:&lt;/p>
&lt;pre>&lt;code>&amp;quot;fields&amp;quot; : {
&amp;quot;url.index&amp;quot; : [
&amp;quot;my_search_index&amp;quot;
]
}
&lt;/code>&lt;/pre>
&lt;p>A neat thing about the &lt;code>docvalue_fields&lt;/code> is that in the example above the &lt;code>my_search_index&lt;/code> value is not comming from the &lt;code>_source&lt;/code> of the document. This means that we can use &lt;code>keywords&lt;/code> and by extension normalized &lt;code>keywords&lt;/code> to fetch an exact value from the Elasticsearch index and not necessarily the one that was sent to Elasticsearch which somewhat solves our dependency from the upstream systems.&lt;/p>
&lt;h2 id="notes">Notes&lt;/h2>
&lt;p>The setup is done in the Kibana Dev Tools with the Elasticsearch 7.7.0.&lt;/p>
&lt;p>The pattern &lt;code>&amp;quot;/(.+)/.*&amp;quot;&lt;/code> is a bit simplified purely for presentation purposes and doesn&amp;rsquo;t work as expected for URLs with more than 2 slashes, e.g.: &lt;code>/index/type/_search&lt;/code> would produce &lt;code>index/type&lt;/code>. You need something a bit more involved like &lt;code>&amp;quot;/([^/]+)/.*&amp;quot;&lt;/code>.&lt;/p>
&lt;h2 id="fin">Fin&lt;/h2>
&lt;p>That is all I wanted to show you today. Hope it might be useful/interesting to someone down the line. Leave comments on the Github issue
&lt;a href="https://github.com/dainiusjocas/blog/issues/7" target="_blank" rel="noopener">here&lt;/a>. Cheers!&lt;/p></description></item><item><title>Deploy babashka script to AWS Lambda</title><link>https://www.jocas.lt/blog/post/babashka-aws-lambda/</link><pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/babashka-aws-lambda/</guid><description>&lt;p>TL;DR&lt;/p>
&lt;p>I&amp;rsquo;ve managed to package a simple
&lt;a href="https://github.com/borkdude/babashka" target="_blank" rel="noopener">babashka&lt;/a> script to an AWS Lambda Custom Runtime.
&lt;a href="https://github.com/dainiusjocas/babashka-lambda" target="_blank" rel="noopener">Here&lt;/a> is the code, try for yourself.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Wouldn&amp;rsquo;t it be great to deploy little Clojure code snippets to Custom Lambda Runtime? The main benefits would be:&lt;/p>
&lt;ul>
&lt;li>you would not suffer from java cold-start problems;&lt;/li>
&lt;li>you wouldn&amp;rsquo;t need to compile your project with GraalVM &lt;code>native-image&lt;/code> tool which is time consuming and for anything more advanced is not likely to work anyway;&lt;/li>
&lt;li>babashka supports scripting with a subset of Clojure, which might do the work for you.&lt;/li>
&lt;/ul>
&lt;h2 id="the-plan">The plan&lt;/h2>
&lt;p>I know what it takes to deploy to Lambda Custom Runtime. Last year I&amp;rsquo;ve created a Clojure project template for deploying
&lt;a href="https://github.com/tokenmill/clojure-graalvm-aws-lambda-template" target="_blank" rel="noopener">GraalVM compiled AWS Lambda Custom Runtime&lt;/a>. And babashka is just another self contained binary. It should be too hard to bring two things together and get it working? Challenge accepted.&lt;/p>
&lt;h2 id="packaging">Packaging&lt;/h2>
&lt;p>I like to build software inside Docker containers. In this experiment, for the first attempt I&amp;rsquo;ve used this Dockerfile:&lt;/p>
&lt;pre>&lt;code>FROM borkdude/babashka:latest as BABASHKA
FROM clojure:tools-deps-alpine as BUILDER
RUN apk add --no-cache zip
WORKDIR /var/task
COPY --from=BABASHKA /usr/local/bin/bb bb
ENV GITLIBS=&amp;quot;.gitlibs/&amp;quot;
COPY lambda/bootstrap bootstrap
COPY deps.edn deps.edn
RUN clojure -Sdeps '{:mvn/local-repo &amp;quot;./.m2/repository&amp;quot;}' -Spath &amp;gt; cp
COPY src/ src/
COPY resources/ resources/
RUN zip -q -r function.zip bb cp bootstrap .gitlibs/ .m2/ src/ resources/ deps.edn
&lt;/code>&lt;/pre>
&lt;p>Here:&lt;/p>
&lt;ul>
&lt;li>copy &lt;code>bb&lt;/code> binary from babashka Docker image,&lt;/li>
&lt;li>download the dependencies for babashka script using &lt;code>clojure&lt;/code> (both, maven and git dependencies are supported, like is described
&lt;a href="https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/" target="_blank" rel="noopener">here&lt;/a>),&lt;/li>
&lt;li>write a classpath to the &lt;code>cp&lt;/code> file,&lt;/li>
&lt;li>copy all source code,&lt;/li>
&lt;li>zip the required contents to the &lt;code>function.zip&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Every line of this dockerfile is packed with details but I&amp;rsquo;ll leave it for the future posts.&lt;/p>
&lt;p>I&amp;rsquo;ve packaged all dependencies for lambda into &lt;code>function.zip&lt;/code>. The contents of the archive are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>bb&lt;/code>: babashka binary&lt;/li>
&lt;li>&lt;code>bootstrap&lt;/code>: AWS Lambda entry point script&lt;/li>
&lt;li>&lt;code>cp&lt;/code>: generated classpath text file&lt;/li>
&lt;li>&lt;code>deps.edn&lt;/code>&lt;/li>
&lt;li>&lt;code>.gitlibs&lt;/code>: directory with gitlibs&lt;/li>
&lt;li>&lt;code>.m2&lt;/code>: directory with Maven dependencies&lt;/li>
&lt;li>&lt;code>resources&lt;/code>:&lt;/li>
&lt;li>&lt;code>src&lt;/code>: directory with babashka scripts&lt;/li>
&lt;/ul>
&lt;h2 id="custom-runtime-discoveries">Custom runtime discoveries&lt;/h2>
&lt;p>Finally, having all dependencies packaged up, I&amp;rsquo;ve deployed the &lt;code>function.zip&lt;/code> to AWS Lambda. The first error message was not very
&lt;a href="https://gist.github.com/dainiusjocas/feafeef5653ff2c6e8c7b2d9627a831d" target="_blank" rel="noopener">encouraging&lt;/a>:&lt;/p>
&lt;pre>&lt;code class="language-text">Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.create() failed. errno: 38 Function not implemented
Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed.
JavaFrameAnchor dump:
No anchors
TopFrame info:
TotalFrameSize in CodeInfoTable 32
VMThreads info:
VMThread 0000000003042750 STATUS_IN_JAVA (safepoints disabled) java.lang.Thread@0x264fa98
VM Thread State for current thread 0000000003042750:
0 (8 bytes): com.oracle.svm.jni.JNIThreadLocalEnvironment.jniFunctions = (bytes)
0000000003042750: 0000000002293a88
8 (32 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.regularTLAB = (bytes)
0000000003042758: 00007f7809500000 00007f7809600000
0000000003042768: 00007f7809507160 0000000000000000
40 (8 bytes): com.oracle.svm.core.heap.NoAllocationVerifier.openVerifiers = (Object) null
48 (8 bytes): com.oracle.svm.core.jdk.IdentityHashCodeSupport.hashCodeGeneratorTL = (Object) null
56 (8 bytes): com.oracle.svm.core.snippets.SnippetRuntime.currentException = (Object) null
64 (8 bytes): com.oracle.svm.core.thread.JavaThreads.currentThread = (Object) java.lang.Thread 000000000264fa98
72 (8 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.activeTimer = (Object) null
80 (8 bytes): com.oracle.svm.jni.JNIObjectHandles.handles = (Object) com.oracle.svm.core.handles.ThreadLocalHandles 00007f7809501558
88 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPendingException.pendingException = (Object) null
96 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPinnedObjects.pinnedObjectsListHead = (Object) null
104 (8 bytes): com.oracle.svm.jni.JNIThreadOwnedMonitors.ownedMonitors = (Object) null
112 (8 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.freeList = (Word) 0 0000000000000000
120 (8 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.stackBoundaryTL = (Word) 1 0000000000000001
128 (8 bytes): com.oracle.svm.core.stack.JavaFrameAnchors.lastAnchor = (Word) 0 0000000000000000
136 (8 bytes): com.oracle.svm.core.thread.VMThreads.IsolateTL = (Word) 25636864 0000000001873000
144 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadHandleTL = (Word) 50477184 0000000003023880
152 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadIdTL = (Word) 50477184 0000000003023880
160 (8 bytes): com.oracle.svm.core.thread.VMThreads.nextTL = (Word) 0 0000000000000000
168 (4 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.yellowZoneStateTL = (int) -16843010 fefefefe
172 (4 bytes): com.oracle.svm.core.snippets.ImplicitExceptions.implicitExceptionsAreFatal = (int) 0 00000000
176 (4 bytes): com.oracle.svm.core.thread.Safepoint.safepointRequested = (int) 2147473200 7fffd730
180 (4 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.currentPauseDepth = (int) 0 00000000
184 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.safepointsDisabledTL = (int) 1 00000001
188 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.statusTL = (int) 1 00000001
VMOperation dump:
No VMOperation in progress
Dump Counters:
Raw Stacktrace:
00007ffeb8e0a940: 000000000186e776 000000000207b9d0
00007ffeb8e0a950: 0000000001873000 000000000085b37c
00007ffeb8e0a960: 000000000084540a 00000000008454ca
00007ffeb8e0a970: 000000000264f128 000000000264ef58
00007ffeb8e0a980: 00007f78095018d8 0000000002650640
00007ffeb8e0a990: 000000000264f128 0000002602650c18
00007ffeb8e0a9a0: 0000000000845444 00007ffeb8e0a970
00007ffeb8e0a9b0: 0000000000000000 0000000000845f6e
00007ffeb8e0a9c0: 0000000002650e18 0000000002650c18
00007ffeb8e0a9d0: 0000000002650e18 0000000002070c60
00007ffeb8e0a9e0: 00000000021f48f8 00000000012b77e6
00007ffeb8e0a9f0: 0000000002650e18 0000000002650c18
00007ffeb8e0aa00: 0000001000000000 0000000002070c60
00007ffeb8e0aa10: 00007f7809507138 0000000000477f69
00007ffeb8e0aa20: 00007f7809503b88 00007f7809501910
00007ffeb8e0aa30: 00007f7809507138 00000000004831b4
00007ffeb8e0aa40: 0000000000000010 000000000085d16d
00007ffeb8e0aa50: 000000000000003b 00000000008b4bdb
00007ffeb8e0aa60: 000000000291e970 00007f7809504828
00007ffeb8e0aa70: 0000000100000007 0000000001079a70
00007ffeb8e0aa80: 00007f78095070b8 00007f7809507080
00007ffeb8e0aa90: 0000000001873000 000000000291e970
00007ffeb8e0aaa0: 00007f7809506f78 00007f78095070b8
00007ffeb8e0aab0: 0000000000000008 0000000000000010
00007ffeb8e0aac0: 0000000000000010 00000000008144a1
00007ffeb8e0aad0: 0000000000000007 0000000000cd7c2e
00007ffeb8e0aae0: 00007f7809504938 0000000001873000
00007ffeb8e0aaf0: 0000000002205088 00007f78095070b8
00007ffeb8e0ab00: 00007f7809507080 0000000cc0001000
00007ffeb8e0ab10: 0000000000000000 0000000000cd73eb
00007ffeb8e0ab20: 00007f7809503b58 00007f78095070b8
00007ffeb8e0ab30: 00007f7809507080 00007f78095038e0
00007ffeb8e0ab40: 00007f7807c8e388 000000000205e900
00007ffeb8e0ab50: 00007f7809501350 000000240000000c
00007ffeb8e0ab60: 000000000000000c 00007f78095038e0
00007ffeb8e0ab70: d15c483b00000000 00000000004830e5
00007ffeb8e0ab80: 0000000000000007 00007f78095038e0
00007ffeb8e0ab90: 00007f78095038e0 00000000006f2b33
00007ffeb8e0aba0: 000000000205e900 0000000002070448
00007ffeb8e0abb0: 00007f78095070b8 0000000000cd8b3d
00007ffeb8e0abc0: 00000000020864c8 0000000000cbffc1
00007ffeb8e0abd0: 0000000002070448 00007f78095070b8
00007ffeb8e0abe0: 0000000c00000000 00007f7809505ef8
00007ffeb8e0abf0: 00007f78095070d8 00007f7809504840
00007ffeb8e0ac00: 7cab467402070d98 0000000000fbfc08
00007ffeb8e0ac10: 0000000002634470 00007f7809507020
00007ffeb8e0ac20: 0000000001873000 00007f78095070d8
00007ffeb8e0ac30: 00007f7809504840 0000000000cc187e
00007ffeb8e0ac40: 0000000000000000 0000000000000000
00007ffeb8e0ac50: 00007f7807c91840 00007f7809504840
00007ffeb8e0ac60: 0000000002070d98 0000000000cc17b9
00007ffeb8e0ac70: 0000000000c848f0 00007f78095038e0
00007ffeb8e0ac80: 0000000002b33a78 0000000100cc4f83
00007ffeb8e0ac90: 0000000000483140 00000000004b5713
00007ffeb8e0aca0: 0000000002070d98 0000000000cdae9a
00007ffeb8e0acb0: 000000000209a600 00007f78095038e0
00007ffeb8e0acc0: 0000000002b33a78 000000000047c576
00007ffeb8e0acd0: 000000000209a600 000000000209a630
00007ffeb8e0ace0: 0000000002a1b8d8 0000000002a1b408
00007ffeb8e0acf0: 000000000209a600 00000000017acc23
00007ffeb8e0ad00: 0000000000000001 0000000000001000
00007ffeb8e0ad10: 0000000000000000 0000000000000000
00007ffeb8e0ad20: 0000000000000000 0000000000000000
00007ffeb8e0ad30: 0000000000000000 0000000000000000
Stacktrace Stage0:
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 FrameSize 32
RSP 00007ffeb8e0a960 RIP 000000000085b37c FrameSize 16
RSP 00007ffeb8e0a970 RIP 00000000008454ca FrameSize 80
RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e FrameSize 48
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 FrameSize 48
RSP 00007ffeb8e0aa20 RIP 0000000000477f69 FrameSize 32
RSP 00007ffeb8e0aa40 RIP 00000000004831b4 FrameSize 320
RSP 00007ffeb8e0ab80 RIP 00000000004830e5 FrameSize 32
RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 FrameSize 256
RSP 00007ffeb8e0aca0 RIP 00000000004b5713 FrameSize 48
RSP 00007ffeb8e0acd0 RIP 000000000047c576 FrameSize 160
RSP 00007ffeb8e0ad70 RIP 000000000047c285 FrameSize 32
RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 FrameSize 256
RSP 00007ffeb8e0ae90 RIP 000000000048f162 FrameSize 32
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c FrameSize 1
Stacktrace Stage1:
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a960 RIP 000000000085b37c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a970 RIP 00000000008454ca com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aa20 RIP 0000000000477f69 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aa40 RIP 00000000004831b4 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ab80 RIP 00000000004830e5 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aca0 RIP 00000000004b5713 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0acd0 RIP 000000000047c576 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ad70 RIP 000000000047c285 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ae90 RIP 000000000048f162 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
Full Stacktrace:
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.jdk.VMErrorSubstitutions.shutdown(VMErrorSubstitutions.java:111)
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:74)
RSP 00007ffeb8e0a960 RIP 000000000085b37c [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:59)
RSP 00007ffeb8e0a970 RIP 00000000008454ca [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.ensureInitialized(SunMiscSubstitutions.java:176)
RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.numberFromName(SunMiscSubstitutions.java:223)
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.findSignal(Signal.java:78)
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.&amp;lt;init&amp;gt;(Signal.java:140)
RSP 00007ffeb8e0aa20 RIP 0000000000477f69 [image code] babashka.impl.pipe_signal_handler$handle_pipe_BANG_.invokeStatic(pipe_signal_handler.clj:11)
RSP 00007ffeb8e0aa40 RIP 00000000004831b4 [image code] babashka.main$main.invokeStatic(main.clj:282)
RSP 00007ffeb8e0ab80 RIP 00000000004830e5 [image code] babashka.main$main.doInvoke(main.clj:282)
RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137)
RSP 00007ffeb8e0aca0 RIP 00000000004b5713 [image code] clojure.core$apply.invokeStatic(core.clj:665)
RSP 00007ffeb8e0acd0 RIP 000000000047c576 [image code] babashka.main$_main.invokeStatic(main.clj:442)
RSP 00007ffeb8e0ad70 RIP 000000000047c285 [image code] babashka.main$_main.doInvoke(main.clj:437)
RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137)
RSP 00007ffeb8e0ae90 RIP 000000000048f162 [image code] babashka.main.main(Unknown Source)
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.runCore(JavaMainWrapper.java:151)
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:186)
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.code.IsolateEnterStub.JavaMainWrapper_run_5087f5482cc9a6abc971913ece43acb471d2631b(IsolateEnterStub.java:0)
[Native image heap boundaries:
ReadOnly Primitives: 0x1873008 .. 0x206f048
ReadOnly References: 0x206ff78 .. 0x24fc9f8
Writable Primitives: 0x24fd000 .. 0x26343e0
Writable References: 0x2634470 .. 0x2ba42c0]
[Heap:
[Young generation:
[youngSpace:
aligned: 0/0 unaligned: 0/0]]
[Old generation:
[fromSpace:
aligned: 0/0 unaligned: 0/0]
[toSpace:
aligned: 0/0 unaligned: 0/0]
]
[Unused:
aligned: 0/0]]
Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed.
RequestId: 263ff1be-425d-4dcb-9ea5-67020dc3041b Error: Runtime exited with error: exit status 99
Runtime.ExitError
&lt;/code>&lt;/pre>
&lt;h2 id="the-fight">The fight&lt;/h2>
&lt;p>After some Googling I&amp;rsquo;ve discovered several related clues
&lt;a href="https://github.com/oracle/graal/issues/841" target="_blank" rel="noopener">here&lt;/a> and
&lt;a href="https://github.com/quarkusio/quarkus/issues/4262" target="_blank" rel="noopener">here&lt;/a>. They say that signals are not supported in AWS lambda. So, why not to disable signals for babashka and see what happens? I&amp;rsquo;ve forked the repo, made a flag that disables PIPE signal handling, deployed babashka to the
&lt;a href="https://hub.docker.com/r/dainiusjocas/babashka" target="_blank" rel="noopener">docker hub&lt;/a> and tried to deploy lambda once again.&lt;/p>
&lt;p>And? It worked:&lt;/p>
&lt;pre>&lt;code class="language-shell">make function-name=$(make get-function-name) invoke-function
=&amp;gt;
{&amp;quot;test&amp;quot;:&amp;quot;test914&amp;quot;}{
&amp;quot;StatusCode&amp;quot;: 200,
&amp;quot;ExecutedVersion&amp;quot;: &amp;quot;$LATEST&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>
&lt;a href="https://github.com/dainiusjocas/babashka-lambda" target="_blank" rel="noopener">Here&lt;/a> is the example of babashka script that can be deployed to AWS Lambda.&lt;/p>
&lt;ul>
&lt;li>The &lt;code>function.zip&lt;/code> weights just 18MB.&lt;/li>
&lt;li>The cold startup of the Lambda that is given 128MB of RAM is ~400ms. Subsequent calls ranges from 4ms and 120ms. The more RAM you give the faster lambda gets.&lt;/li>
&lt;li>I can develop the code in Cursive as the structure is like of an ordinary Clojure deps.edn project (and it can be used on the JVM).&lt;/li>
&lt;li>I made a
&lt;a href="https://github.com/borkdude/babashka/pull/305" target="_blank" rel="noopener">PR to babashka&lt;/a> and I&amp;rsquo;ve got accepted.&lt;/li>
&lt;/ul>
&lt;h2 id="next-steps">Next Steps&lt;/h2>
&lt;ul>
&lt;li>Fix Problem building on macos (&lt;code>/tmp&lt;/code> dir is not writable).&lt;/li>
&lt;li>Get rid of AWS CloudFormation part.&lt;/li>
&lt;li>Work a bit more to support AWS API Gateway.&lt;/li>
&lt;li>Create a template for such projects.&lt;/li>
&lt;/ul></description></item><item><title>Using Search Templates in Elasticsearch</title><link>https://www.jocas.lt/blog/post/on-search-templates/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/on-search-templates/</guid><description>&lt;p>I want to take a look at
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html" target="_blank" rel="noopener">Search Templates&lt;/a> for Elasticsearch. Let&amp;rsquo;s apply them to examples from
&lt;a href="https://www.jocas.lt/blog/post/synonym-graph-phrase-search/" target="_blank" rel="noopener">previous post on Synonym Graphs&lt;/a>.&lt;/p>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>I&amp;rsquo;m using Elasticsearch 7.5.1.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code>DELETE test_index-1
PUT /test_index-1
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;descrition&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
}
},
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonym_graph_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;my_synonym_graph&amp;quot;
]
},
&amp;quot;lowercase_keyword_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
],
&amp;quot;char_filter&amp;quot;: [
&amp;quot;spaces_to_undescores_filter&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;spaces_to_undescores_filter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;,
&amp;quot;mappings&amp;quot;: [
&amp;quot; \\u0020 =&amp;gt; _&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;my_synonym_graph&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym_graph&amp;quot;,
&amp;quot;lenient&amp;quot;: true,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;
]
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Index the document:&lt;/p>
&lt;pre>&lt;code>PUT test_index-1/_doc/1
{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>prefix very important thing suffix&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="templates">Templates&lt;/h2>
&lt;p>I&amp;rsquo;m very interested in one particular use of the search templates: how flexible is the management of stored seach templates? Can I update a search template while receiving queries?&lt;/p>
&lt;p>Add a template:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Try to run the search:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Exactly as expected.&lt;/p>
&lt;p>When using a stored search template the Elasticsearch client doesn&amp;rsquo;t need to handle the complex query construction.&lt;/p>
&lt;h2 id="templates-are-updateable">Templates are updateable&lt;/h2>
&lt;p>Let&amp;rsquo;s try to update the template with a higher boost value:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 5
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Works.&lt;/p>
&lt;p>Now let&amp;rsquo;s run the same query:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 4,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The scores are 0.5753642 and 1.4384103 that is ~2/5. Cool! This means that without changing (and redeploying) the Elasticsearch client we can change the querying logic, making the query an more dynamic.&lt;/p>
&lt;h2 id="corner-cases">Corner Cases&lt;/h2>
&lt;p>What if we run query has more attributes, e.g.:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;,
&amp;quot;new_attr&amp;quot;: &amp;quot;123&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>Works as expected!&lt;/p>
&lt;p>When &lt;code>query_string&lt;/code> is &lt;code>null&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: null
}
}
&lt;/code>&lt;/pre>
&lt;p>Works!&lt;/p>
&lt;p>What if the param is not provided:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;new_attr&amp;quot;: &amp;quot;value&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>No error!&lt;/p>
&lt;p>What if we provide a list instead of a string:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: [&amp;quot;this&amp;quot;, &amp;quot;Very Important Thing&amp;quot;]
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Instead of profiding one value we can replace it with a list. Good!&lt;/p>
&lt;h2 id="metadata-of-the-search-template">Metadata of the search template&lt;/h2>
&lt;p>It would be great to be able to store some metadata with the search template script, e.g. Git commit SHA of the query. I couldn&amp;rsquo;t find a way to do this. A workaround might be to &lt;code>_name&lt;/code> attribute of the query. E.g.:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;_name&amp;quot;: &amp;quot;GIT COMMIT SHA&amp;quot;,
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 5
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The response:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;GIT COMMIT SHA&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Not great but might be useful.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;ul>
&lt;li>Templates doesn&amp;rsquo;t support search index specification.&lt;/li>
&lt;li>Field names can be parameterized, this feature alows to start/stop using a new/old field.&lt;/li>
&lt;li>Search template can be tested in (even in production cluster) independently.&lt;/li>
&lt;li>We can run our query against
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-search-template.html" target="_blank" rel="noopener">multiple search templates&lt;/a>. Combine this with the Profile API and performance can be compared. Explain API also is supported.&lt;/li>
&lt;/ul></description></item><item><title>Phrase Search with Synonym Graph Token Filter in Elasticsearch</title><link>https://www.jocas.lt/blog/post/synonym-graph-phrase-search/</link><pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/synonym-graph-phrase-search/</guid><description>&lt;p>I&amp;rsquo;ve
&lt;a href="https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/" target="_blank" rel="noopener">written&lt;/a> that if you google for &lt;code>How can you match a long query text to a short text field?&lt;/code> you&amp;rsquo;re advised to use Elasticsearch Percolator. Today I&amp;rsquo;ll show an alternative way of solving the same problem with Elasticsearch.&lt;/p>
&lt;p>The main idea is to use
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/analysis-synonym-graph-tokenfilter.html" target="_blank" rel="noopener">Synonym Graph Token Filter&lt;/a> with some data preparation.&lt;/p>
&lt;h2 id="problem-statement">Problem Statement&lt;/h2>
&lt;p>Say that we learned how extract some entity from free form text with techniques such as NER, dictionary annotations, or some fancy Machine Learning. And when this entity is mentioned in the search query we want to boost documents that mention this entity. Also, say you&amp;rsquo;ve ruled out using Elasticsearch Percolator because it increases network latency because it requires additional call to Elasticsearch.&lt;/p>
&lt;p>For further discussion our unstructured text is going to be &lt;code>This description is about a Very Important Thing and something else.&lt;/code> and the extracted entity &lt;code>Very Important Thing&lt;/code>. Our test document looks like :&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>prefix very important thing suffix&lt;/code>&lt;/li>
&lt;li>&lt;code>prefix very important another thing suffix&lt;/code>&lt;/li>
&lt;li>&lt;code>prefix thing suffix&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>All examples are tested on Elasticsearch 7.5.1.&lt;/p>
&lt;h3 id="naive-setup">Naive Setup&lt;/h3>
&lt;p>Let&amp;rsquo;s create an index for our documents:&lt;/p>
&lt;pre>&lt;code>PUT /test_index-2
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;description&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Entity field is of type &lt;code>text&lt;/code> because we want it to be searchable. &lt;code>keyword&lt;/code> type won&amp;rsquo;t work because it does only exact matches and out query most likely will be longer than our entity string.&lt;/p>
&lt;p>Index our document:&lt;/p>
&lt;pre>&lt;code>PUT test_index-2/_doc/1
{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search the index with the query that mentions our &lt;code>very important thing&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.7260926,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.7260926,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Cool, we found what we we looking for.&lt;/p>
&lt;p>Let&amp;rsquo;s try another query, this time with a mention of &lt;code>very important another thing&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important another thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.7260926,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.7260926,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Oh, the results are the same as with the previous query despite the fact that we mention &lt;code>Another Thing&lt;/code> here. But it still might be OK because we matched all the terms of the entity.&lt;/p>
&lt;p>Let&amp;rsquo;s try another query:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Oh no, we still matched our &lt;code>Very Important Thing&lt;/code> while only &lt;code>thing&lt;/code> term is present in the query. But at least this time the score is lower than with previous twoqueries, 0.5753642 vs. 1.7260926. Here we clearly see the problem: we are matching short strings with long strings and partial matches raises problems.&lt;/p>
&lt;h2 id="proposed-solution">Proposed Solution&lt;/h2>
&lt;p>Let&amp;rsquo;s leverage Synonym Graph Token Filter to solve our problem.&lt;/p>
&lt;pre>&lt;code>PUT /test_index-1
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;descrition&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
}
},
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonym_graph_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;my_synonym_graph&amp;quot;
]
},
&amp;quot;lowercase_keyword_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
],
&amp;quot;char_filter&amp;quot;: [
&amp;quot;spaces_to_undescores_filter&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;spaces_to_undescores_filter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;,
&amp;quot;mappings&amp;quot;: [
&amp;quot; \\u0020 =&amp;gt; _&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;my_synonym_graph&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym_graph&amp;quot;,
&amp;quot;lenient&amp;quot;: true,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;
]
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s decompose this large index configuration piece by piece:&lt;/p>
&lt;ol>
&lt;li>The &lt;code>entity&lt;/code> attribute now has separate analyzers for both index and search phases.&lt;/li>
&lt;li>The &lt;code>lowercase_keyword_analyzer&lt;/code> uses keyword tokenizer which means that tokenization will result in the sequence of token of size 1, then it normalizes tokens by lowercasing them and finally &lt;code>spaces_to_undescores_filter&lt;/code>, replaces spaces to underscores. E.g. a string &lt;code>&amp;quot;Very Important Thing&amp;quot;&lt;/code> is transformed into list of tokens &lt;code>[&amp;quot;very_important_thing&amp;quot;]&lt;/code>. Or use out friend &lt;code>_analyze&lt;/code> API:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>POST test_index-1/_analyze
{
&amp;quot;text&amp;quot;: [&amp;quot;Very Important Thing&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;very_important_thing&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 20,
&amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
&amp;quot;position&amp;quot; : 0
}
]
}
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>The &lt;code>synonym_graph_analyzer&lt;/code> use standard tokenizer, which is followed by the &lt;code>lowercase&lt;/code> filter, and then the &lt;code>my_synonym_graph&lt;/code> token filter is applied. We&amp;rsquo;ve set up one synonym &lt;code>&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;&lt;/code>. E.g.&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>POST test_index-1/_analyze
{
&amp;quot;text&amp;quot;: [&amp;quot;prefix very important thing suffix&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;prefix&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 6,
&amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot; : 0
},
{
&amp;quot;token&amp;quot; : &amp;quot;very_important_thing&amp;quot;,
&amp;quot;start_offset&amp;quot; : 7,
&amp;quot;end_offset&amp;quot; : 27,
&amp;quot;type&amp;quot; : &amp;quot;SYNONYM&amp;quot;,
&amp;quot;position&amp;quot; : 1
},
{
&amp;quot;token&amp;quot; : &amp;quot;suffix&amp;quot;,
&amp;quot;start_offset&amp;quot; : 28,
&amp;quot;end_offset&amp;quot; : 34,
&amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot; : 2
}
]
}
&lt;/code>&lt;/pre>
&lt;p>After analysis we have 3 tokens &lt;code>[&amp;quot;prefix&amp;quot;, &amp;quot;very_important_thing&amp;quot;, &amp;quot;suffix&amp;quot;]&lt;/code>. Notice &lt;code>&amp;quot;very_important_thing&amp;quot;&lt;/code> token: this is equal to the right-hand-side from our synonym definitions. Now let&amp;rsquo;s run queries from the previous section:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: exact match -&amp;gt; hit.&lt;/p>
&lt;p>Another query:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important another thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 0,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits! Good! The document is not going to be boosted despite the fact that all tokens match.&lt;/p>
&lt;p>And the last one:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits! Good. This means that also substring doesn&amp;rsquo;t match.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>Synonym Graph Token Filter can &amp;ldquo;replace&amp;rdquo; a sequence of tokens (e.g. a phrase) with another sequence of tokens. In this particular example: many tokens were replaced with one token.&lt;/p>
&lt;ol>
&lt;li>One field can have only one analyzer pair for index and search phases. If we want another analysis pipeline for the &lt;code>entity&lt;/code> attribute we have to create another field with the analyzers specified, e.g. stemmed phrase with lower boost.&lt;/li>
&lt;li>The synonym list must be prepared before the index creation.&lt;/li>
&lt;li>Management of the synonym list might complicate index management, e.g. you use templates for your index management.&lt;/li>
&lt;li>The overal solution in general might look a bit too complicated.&lt;/li>
&lt;/ol></description></item><item><title>Elasticsearch Percolator and Text Analyzers</title><link>https://www.jocas.lt/blog/post/percolator-phrase-analyzers/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/percolator-phrase-analyzers/</guid><description>&lt;p>This time I need to percolate texts with different analyzers for index and search analyzers.&lt;/p>
&lt;p>Let&amp;rsquo;s elaborate a bit on
&lt;a href="https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/" target="_blank" rel="noopener">previous article&lt;/a> and explicitly declare analyzers to use.&lt;/p>
&lt;p>Define index:&lt;/p>
&lt;pre>&lt;code>PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Then define 2 slightly different percolator queries (notice the difference between &lt;code>&amp;quot;bonsai tree&amp;quot;&lt;/code> and &lt;code>&amp;quot;bonsai, tree&amp;quot;&lt;/code>).&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
PUT /my-index/_doc/2?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s percolate:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 80,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
},
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: 2 documents matched.&lt;/p>
&lt;p>But now lets change the analyzer of the second percolation query to &lt;code>whitespace&lt;/code>:&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/2?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;whitespace&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Run the percolator:&lt;/p>
&lt;pre>&lt;code>
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 5,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: only 1 percolator query matched our input.&lt;/p>
&lt;h2 id="phrases-with-stopwords">Phrases with Stopwords&lt;/h2>
&lt;p>Say, we have a phrase &lt;code>&amp;quot;bonsai is tree&amp;quot;&lt;/code> and we percolate text &lt;code>A new bonsai in tree in the office&lt;/code> with the &lt;code>standard&lt;/code> analyzer for indexing and &lt;code>english&lt;/code> for search analyzer. There should be no matches. Let&amp;rsquo;s try:&lt;/p>
&lt;pre>&lt;code>DELETE my-index
PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;english&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai is tree&amp;quot;
}
}
}
}
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai in tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>And, surprisingly, this yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 2,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai is tree&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>We have a match! Also notice that the highlighter is broken!&lt;/p>
&lt;p>The problem that these two analyzers have different stopword lists (no stopwords for &lt;code>standard&lt;/code> and several English stopwords for &lt;code>english&lt;/code> analyzer) and the phrase contains a stopword that is not shared between analyzers.&lt;/p>
&lt;p>Let&amp;rsquo;s fix this surprise with
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html#search-quote-analyzer" target="_blank" rel="noopener">&lt;code>search_quote_analyzer&lt;/code>&lt;/a>.&lt;/p>
&lt;pre>&lt;code>DELETE my-index
PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;english&amp;quot;,
&amp;quot;search_quote_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai is tree&amp;quot;
}
}
}
}
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai in tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits, as expected.&lt;/p>
&lt;p>Let&amp;rsquo;s check if the expected behaviour is still there:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai is tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 4,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.39229375,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.39229375,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai is tree&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai is tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Good. Even the highlighting works.&lt;/p></description></item><item><title>Phrase Highlighting with the Elasticsearch Percolator</title><link>https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/</guid><description>&lt;p>If you google &lt;code>How can you match a long query text to a short text field?&lt;/code> it will point you to the
&lt;a href="https://stackoverflow.com/questions/51865747/elasticsearch-match-long-query-text-to-short-field" target="_blank" rel="noopener">Stack Overflow page&lt;/a>
&lt;a href="https://discuss.elastic.co/t/match-long-query-text-to-short-field/144584/3" target="_blank" rel="noopener">or here&lt;/a> where the answer is to use
&lt;a href="">Elasticsearch Percolator&lt;/a>.&lt;/p>
&lt;p>My search items are phrases meaning that it should match all terms in order. Let&amp;rsquo;s create a sample setup in Kibana (v7.5) Dev dashboard.&lt;/p>
&lt;ol>
&lt;li>Create an index for percolation:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note on &lt;code>&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;&lt;/code>: this allows
&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.6/search-request-highlighting.html#fast-vector-highlighter" target="_blank" rel="noopener">Fast Vector Highlighter&lt;/a> to highlight combined phrase not just separate qeury terms.&lt;/p>
&lt;ol start="2">
&lt;li>Store one phrase query:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;bonsai tree&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>Percolate a document:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note on &lt;code>&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;&lt;/code>: this instructs Elasticsearch to use the Fast Vector Highlighter.&lt;/p>
&lt;p>The query yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 23,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : &amp;quot;bonsai tree&amp;quot;
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As we see highlighter correctly marker the search phrase.&lt;/p>
&lt;h2 id="storing-additional-data-with-percolator-queries">Storing additional data with percolator queries&lt;/h2>
&lt;p>Percolation result can be used to connect pieces of information in your system, e.g. store a &lt;code>subscriber_email&lt;/code> attribute of the user that wants to be notified when the query matches along with the percolator query.&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;bonsai tree&amp;quot;
}
},
&amp;quot;subscriber_email&amp;quot;: &amp;quot;subscriber_email@example.com&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Then query:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This query yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 10,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : &amp;quot;bonsai tree&amp;quot;
}
},
&amp;quot;subscriber_email&amp;quot; : &amp;quot;subscriber_email@example.com&amp;quot;
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Now, take the email under the &lt;code>&amp;quot;subscriber_email&amp;quot;&lt;/code> from the response and send an email with the highlight.&lt;/p></description></item><item><title>Using Uberdeps to Build AWS Lambda Uberjar</title><link>https://www.jocas.lt/blog/post/uberdeps-for-aws-lambda/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/uberdeps-for-aws-lambda/</guid><description>&lt;p>I was writing a Clojure application and the plan was to deploy it as a AWS Lambda. The question I&amp;rsquo;m going to answer in this blog post is: how to build an uberjar for AWS Lambda with
&lt;a href="https://github.com/tonsky/uberdeps" target="_blank" rel="noopener">Uberdeps&lt;/a>?&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>Add an alias to the &lt;code>deps.edn&lt;/code> for uberjar building:&lt;/p>
&lt;pre>&lt;code>{:aliases {:uberjar
{:extra-deps {uberdeps {:mvn/version &amp;quot;0.1.6&amp;quot;}}
:main-opts [&amp;quot;-m&amp;quot; &amp;quot;uberdeps.uberjar&amp;quot;]}}}
&lt;/code>&lt;/pre>
&lt;p>Create an executable file &lt;code>compile.clj&lt;/code> in the project root folder:&lt;/p>
&lt;pre>&lt;code class="language-bash">touch compile.clj
chmod +x compile.clj
&lt;/code>&lt;/pre>
&lt;p>Put this code in the &lt;code>compile.clj&lt;/code> file:&lt;/p>
&lt;script src="https://gist.github.com/dainiusjocas/e9b154d7a1cbdca8558cd7c5d730d5d0.js">&lt;/script>
&lt;p>Run:&lt;/p>
&lt;pre>&lt;code class="language-bash">(rm -rf classes &amp;amp;&amp;amp; \
mkdir classes &amp;amp;&amp;amp; \
./compile.clj &amp;amp;&amp;amp; \
clojure -A:uberjar --target target/UBERJAR_NAME.jar)
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;d advise put that last script into a &lt;code>Makefile&lt;/code> ;)&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>To deploy your Clojure code to AWS Lambda you need to package it as an uberjar. If your project is managed with &lt;code>deps.edn&lt;/code>, basically you&amp;rsquo;re on your own to find a suitable library to package your code.&lt;/p>
&lt;p>For some time to build uberjars for &lt;code>deps.edn&lt;/code> projects I was using
&lt;a href="https://github.com/luchiniatwork/cambada" target="_blank" rel="noopener">Cambada&lt;/a>. It did the job but I was not entirely happy with the library for a couple of reasons:&lt;/p>
&lt;ul>
&lt;li>the library seems to be no longer maintained;&lt;/li>
&lt;li>it has various
&lt;a href="https://github.com/luchiniatwork/cambada/issues" target="_blank" rel="noopener">bugs&lt;/a> with transitive Git dependencies. I&amp;rsquo;ve found out that these bugs are fixed in a
&lt;a href="https://github.com/xfthhxk/cambada" target="_blank" rel="noopener">fork&lt;/a> of the Cambada and I used it as a git dependency.&lt;/li>
&lt;/ul>
&lt;p>Because building an uberjar for &lt;code>deps.edn&lt;/code> boils down to just finding a library there is always temptation to try something new.&lt;/p>
&lt;h2 id="enter-uberdeps">Enter Uberdeps&lt;/h2>
&lt;p>For my toy project I wanted to try out
&lt;a href="https://github.com/tonsky/uberdeps" target="_blank" rel="noopener">Uberdeps&lt;/a>. The introduction
&lt;a href="https://tonsky.me/blog/uberdeps/" target="_blank" rel="noopener">blog post&lt;/a> got me interested and I really liked the main idea:&lt;/p>
&lt;blockquote>
&lt;p>Takes deps.edn and packs an uberjar out of it.&lt;/p>
&lt;/blockquote>
&lt;p>Sounds like exactly what I need.&lt;/p>
&lt;h2 id="trouble">Trouble&lt;/h2>
&lt;p>I&amp;rsquo;ve written my application, added all the things needed to deploy it as an AWS Lambda, build an uberjar with Uberdeps, deployed the app with the AWS CloudFormation, but when I&amp;rsquo;ve invoked the Lambda I&amp;rsquo;ve received an error:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;message&amp;quot; : &amp;quot;Internal server error&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>After searching through the AWS CloudWatch logs I&amp;rsquo;ve found:&lt;/p>
&lt;pre>&lt;code>Class not found: my.Lambda: java.lang.ClassNotFoundException
java.lang.ClassNotFoundException: my.Lambda
at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>my.Lambda&lt;/code> class was not found.&lt;/p>
&lt;p>After taking a look at the contents of the uberjar I&amp;rsquo;ve noticed that the &lt;code>my.Lambda&lt;/code> class is indeed not inside the Uberjar. Ah, it seems that AOT (Ahead-of-Time) is not done out of the box. After searching and not finding a flag or some parameter that I need to pass to force the AOT compilation in the Uberdeps README, I&amp;rsquo;ve discovered an already closed
&lt;a href="https://github.com/tonsky/uberdeps/pull/11" target="_blank" rel="noopener">pull request&lt;/a>: the AOT compilation functionality is not implemented.&lt;/p>
&lt;p>I was in trouble.&lt;/p>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;p>The solution was to manually perform AOT compilation of the relevant namespaces right before building an uberjar and then instruct Uberdeps to put the resulting class files into the uberjar.&lt;/p>
&lt;p>To do AOT compilation I&amp;rsquo;ve written a Clojure script &lt;code>compile.clj&lt;/code>:&lt;/p>
&lt;script src="https://gist.github.com/dainiusjocas/e9b154d7a1cbdca8558cd7c5d730d5d0.js">&lt;/script>
&lt;p>Inspiration on how to write the script was taken from
&lt;a href="https://www.reddit.com/r/Clojure/comments/8ltsrs/standalone_script_with_clj_including_dependencies/" target="_blank" rel="noopener">here&lt;/a> and
&lt;a href="https://github.com/tonsky/datascript/blob/master/release.clj" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>To instruct Uberdeps to put class files to the uberjar I&amp;rsquo;ve added &lt;code>classes&lt;/code> directory to the &lt;code>:paths&lt;/code> vector in &lt;code>deps.edn&lt;/code>.&lt;/p>
&lt;p>Just for the convenience, in the Makefile I&amp;rsquo;ve put commands for AOT compilation right before the command to build an uberjar:&lt;/p>
&lt;pre>&lt;code>uberjar:
rm -rf classes
mkdir classes
./compile.clj
clojure -A:uberjar --target target/my-jar-name.jar
&lt;/code>&lt;/pre>
&lt;p>And that is it! I have an uberjar with &lt;code>my.Lambda&lt;/code> class and the AWS Lambda runtime is happy.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>The solution is not bullet proof because:&lt;/p>
&lt;ul>
&lt;li>it assumes that the main &lt;code>deps.end&lt;/code> file is called &lt;code>deps.edn&lt;/code>;&lt;/li>
&lt;li>compiled classes are put in the &lt;code>classes&lt;/code> directory;&lt;/li>
&lt;li>the alias for which namespaces should be AOT compiled is the default alias.&lt;/li>
&lt;/ul>
&lt;p>I hope that when a more generic solution will be needed either the Uberdeps will have an option for AOT compilatoin or I&amp;rsquo;ll be clever enough to deal with the situation and write a follow up blog post with the workaround.&lt;/p></description></item><item><title>Using Gitlab CI Cache for Clojure Dependencies</title><link>https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/</link><pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/</guid><description>&lt;p>I want to share my hard-won lessons on how to setup the Gitlab CI for Clojure projects based on tools.deps. I think that the Gitlab CI is a wonderful tool for CI workloads. But when you&amp;rsquo;re going a bit sideways from the documented ways of doing things you have to do a bit of discovery for yourself.&lt;/p>
&lt;h2 id="gitlab-ci-cachesetup">Gitlab CI Cache Setup&lt;/h2>
&lt;p>Usually I want to cache dependencies between all build and all branches. To achieve this I hard-code the cache key at the root of the &lt;code>.gitlab-ci.yml&lt;/code> file e.g.:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
&lt;/code>&lt;/pre>
&lt;p>When it comes to caching Clojure dependencies we have to be aware that there different types of dependencies. Two most common ones are: Maven and gitlibs.&lt;/p>
&lt;p>The Gitlab CI cache works &lt;strong>only&lt;/strong> with directories &lt;strong>inside the project directory&lt;/strong>. While local repositories (i.e. cache) for Clojure dependencies &lt;strong>by default&lt;/strong> are stored &lt;strong>outside the project directory&lt;/strong> (&lt;code>~/.m2&lt;/code> and &lt;code>~/.gitlibs&lt;/code>). Therefore, we have to provide parameters for our build tool to change the default directories for storing the dependencies.&lt;/p>
&lt;p>To specify Maven local repository we can provide &lt;code>:mvn/local-repo&lt;/code> parameter e.g.:&lt;/p>
&lt;pre>&lt;code class="language-yaml">clojure -Sdeps '{:mvn/local-repo &amp;quot;./.m2/repository&amp;quot;}' -A:test
&lt;/code>&lt;/pre>
&lt;p>Having configured local maven repository in our &lt;code>gitlab-ci.yml&lt;/code> we can specify:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
paths:
- ./.m2/repository
&lt;/code>&lt;/pre>
&lt;p>When it comes to gitlibs there is no public API for changing the default directory in &lt;code>tools.deps&lt;/code>. But the underlying &lt;code>tools.gitlibs&lt;/code> uses an environment variable to set where to store the
&lt;a href="https://github.com/clojure/tools.gitlibs/blob/b7acb151b97952409103094794f5fc6f4d7d3840/src/main/clojure/clojure/tools/gitlibs.clj#L23" target="_blank" rel="noopener">gitlibs conveniently named &lt;strong>GITLIBS&lt;/strong>&lt;/a>. E.g.&lt;/p>
&lt;pre>&lt;code class="language-bash">$ (export GITLIBS=&amp;quot;.gitlibs/&amp;quot; &amp;amp;&amp;amp; clojure -A:test)
&lt;/code>&lt;/pre>
&lt;p>Of course, we should not forget to configure the cache:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
paths:
- ./.gitlibs
&lt;/code>&lt;/pre>
&lt;p>To use caching for both types of dependencies:&lt;/p>
&lt;pre>&lt;code class="language-bash">(export GITLIBS=&amp;quot;.gitlibs/&amp;quot; &amp;amp;&amp;amp; clojure -Sdeps '{:mvn/local-repo &amp;quot;./.m2/repository&amp;quot;}' -A:test)
&lt;/code>&lt;/pre>
&lt;p>And setup the cache:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
paths:
- ./.m2/repository
- ./.gitlibs
&lt;/code>&lt;/pre>
&lt;p>If you want to disable cache for a particular job (e.g. you&amp;rsquo;re linting with
&lt;a href="https://github.com/borkdude/clj-kondo" target="_blank" rel="noopener">clj-kondo&lt;/a>, which is delivered as a
&lt;a href="https://www.graalvm.org/" target="_blank" rel="noopener">GraalVM&lt;/a> compiled
&lt;a href="https://www.graalvm.org/docs/reference-manual/native-image/" target="_blank" rel="noopener">native image&lt;/a>), just give an empty map for a job&amp;rsquo;s cache setup, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-yaml">lint:
stage: test
image: borkdude/clj-kondo
cache: {}
when: always
script:
- clj-kondo --lint src test
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;ve used the Gitlab CI cache while working on a streaming-text search library
&lt;a href="https://github.com/tokenmill/beagle" target="_blank" rel="noopener">Beagle&lt;/a>. A full .gitlab-ci.yml file example of the setup can be found
&lt;a href="https://github.com/tokenmill/beagle/blob/master/.gitlab-ci.yml" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>Hope this helps!&lt;/p></description></item><item><title>Clojure Workflow @ TokenMill</title><link>https://www.jocas.lt/blog/talk/vilnius-clojure-meetup/</link><pubDate>Thu, 30 May 2019 19:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/talk/vilnius-clojure-meetup/</guid><description>&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/1Qto0UWEglVdpB" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> &lt;/iframe> &lt;div style="margin-bottom:5px"> &lt;strong> &lt;a href="//www.slideshare.net/Dainius/clojure-workflow-tokenmill-by-dainius-jocas" title="Clojure workflow @ TokenMill by Dainius Jocas" target="_blank">Clojure workflow @ TokenMill by Dainius Jocas&lt;/a> &lt;/strong>&lt;/div>
&lt;p>The source code of the demo project can be found
&lt;a href="https://github.com/dainiusjocas/clojure-meetup-vilnius-2019-05-30" target="_blank" rel="noopener">here&lt;/a>.&lt;/p></description></item></channel></rss>