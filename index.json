[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.jocas.lt/blog/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blog/authors/admin/","section":"authors","summary":"","tags":null,"title":"Dainius Jocas","type":"authors"},{"authors":["Dainius Jocas"],"categories":["elasticsearch","kibana"],"content":"TL;DR curl \u0026quot;http://localhost:9200/index-with-many-shards/_search\u0026quot; -H 'Content-Type: application/json' -d'{\u0026quot;profile\u0026quot;: true}' | jq ' .profile.shards = [.profile.shards[0]]' | pbcopy  and paste into the Kibana\u0026rsquo;s Search Profile panel.\nProblem When you profile a complex Elasticsearch query that targets many shards then Kibana might need a very long time (think, minutes) to visualize the profiling data. It might be due to some bug in the Kibana or maybe you just throw too much data in there and since Javascript is single threaded it just takes time. Anyway, you want to see the profile data visualization because reading the raw JSON is not your thing.\nSolution A trick you can try is to visualize only a part of the profile data. What part? Let\u0026rsquo;s say the profile data from only one shard.\nThe Elasticsearch response with the profile data has this shape:\n{ \u0026quot;took\u0026quot;: 3, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 66, \u0026quot;successful\u0026quot;: 66, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: { \u0026quot;value\u0026quot;: 10000, \u0026quot;relation\u0026quot;: \u0026quot;gte\u0026quot; }, \u0026quot;max_score\u0026quot;: null, \u0026quot;hits\u0026quot;: [] }, \u0026quot;profile\u0026quot;: { \u0026quot;shards\u0026quot;: [ ] } }  And the important bits are under .profile.shards array (jq syntax). Let\u0026rsquo;s create a little jq script that would transform the response body to include the profile data from only one shard:\njq '.profile.shards = [.profile.shards[0]]'  The output is now much smaller.\nFor this script to work its input must be the full body of the Elasticsearch response in JSON. Luckily for us, a simple curl (that can be copied directly from Kibana Dev Tools) does the job well, e.g.:\ncurl \u0026quot;http://localhost:9200/index-with-many-shards/_search\u0026quot; -H 'Content-Type: application/json' -d'{\u0026quot;profile\u0026quot;: true}'  Pipe the output of that curl command to the jq script:\ncurl \u0026quot;http://localhost:9200/index-with-many-shards/_search\u0026quot; -H 'Content-Type: application/json' -d'{\u0026quot;profile\u0026quot;: true}' | jq '.profile.shards = [.profile.shards[0]]'  Now just select the output with you mouse, copy, and then paste it in Kibana\u0026rsquo;s Search Profile panel and investigate the query profile.\nBut using the mouse in terminal is not sleek One aditional trick to make the process sleeker is to send the profile data to the clipboard directly from your terminal and then paste in Kibana. This can be done by simply piping the output to your clipboard. Unfortunately, the script is different for different operating systems.\nExample in macOS:\ncurl \u0026quot;http://localhost:9200/index-with-many-shards/_search\u0026quot; -H 'Content-Type: application/json' -d'{\u0026quot;profile\u0026quot;: true}' | jq '.profile.shards = [.profile.shards[0]]' | pbcopy  Example in Kubuntu 21.04:\ncurl \u0026quot;http://localhost:9200/index-with-many-shards/_search\u0026quot; -H 'Content-Type: application/json' -d'{\u0026quot;profile\u0026quot;: true}' | jq '.profile.shards = [.profile.shards[0]]' | xclip -selection clip  ","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626998400,"objectID":"af8f2df811a6677db75a86a575360874","permalink":"https://www.jocas.lt/blog/post/kibana-profile-many-shards-hack/","publishdate":"2021-07-23T00:00:00Z","relpermalink":"/blog/post/kibana-profile-many-shards-hack/","section":"post","summary":"A little trick on how to profile complex Elasticsearch queries that target many shards.","tags":["elasticsearch","kibana"],"title":"Make Elasticsearch Query Profiling Faster in Kibana","type":"post"},{"authors":["Dainius Jocas"],"categories":null,"content":" A live notebook used during the presentation can be found (and played with) here.\nKudos for the nextjournal.com!\n","date":1624990500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624990500,"objectID":"2fb0aa8be6e93260c9ee16b69cd8c0c4","permalink":"https://www.jocas.lt/blog/talk/london-information-retrieval-meetup-2021-06/","publishdate":"2021-06-29T22:00:00Z","relpermalink":"/blog/talk/london-information-retrieval-meetup-2021-06/","section":"talk","summary":"Introduction to Lucene-grep","tags":["meetup","lucene","lucene-grep","clojure","graalvm"],"title":"Lucene-grep a.k.a. lmgrep","type":"talk"},{"authors":["Dainius Jocas"],"categories":null,"content":"  ","date":1623877200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623877200,"objectID":"6e0448c8791a86646a02d8e02e80569c","permalink":"https://www.jocas.lt/blog/talk/berlin-buzzwords-2021/","publishdate":"2021-06-17T19:00:00Z","relpermalink":"/blog/talk/berlin-buzzwords-2021/","section":"talk","summary":"Several simple hacks to get more out of your Elasticsearch cluster","tags":["berlin buzzwords 2021","Elasticsearch","Vinted"],"title":"Tips and Tricks to Scale Elasticsearch for 1M RPMs and Beyond","type":"talk"},{"authors":["Dainius Jocas"],"categories":null,"content":"  Lessons Learned While Scaling Elasticsearch at Vinted  from Dainius Jocas  Also the video is available:\n ","date":1622142000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622142000,"objectID":"262167a75a2c093581d9f94fec3d4539","permalink":"https://www.jocas.lt/blog/talk/elasticsearch-community-meetup/","publishdate":"2021-05-27T19:00:00Z","relpermalink":"/blog/talk/elasticsearch-community-meetup/","section":"talk","summary":"Adventures of operating Elasticseach at scale","tags":["meetup","elasticsearch","vinted"],"title":"Lessons Learned While Scaling Elasticsearch @ Vinted","type":"talk"},{"authors":["Dainius Jocas"],"categories":null,"content":"  Don\u0026#x27;t change the partition count for kafka topics!  from Dainius Jocas  ","date":1620842400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620842400,"objectID":"3861b7dbf3a441c737de4ffbd49a2101","permalink":"https://www.jocas.lt/blog/talk/wearedevelopers-2021-05-12/","publishdate":"2021-05-12T19:00:00Z","relpermalink":"/blog/talk/wearedevelopers-2021-05-12/","section":"talk","summary":"Kafka, Kafka Connect, Elasticsearch, and Data Consistency","tags":["meetup","kafka","kafka connect","elasticsearch"],"title":"Don't Change the Partition Count for Kafka Topics!","type":"talk"},{"authors":["Dainius Jocas"],"categories":["Lucene"],"content":"TL;DR lmgrep provides an easy way to play with various text analysis options. Just download the lmgrep binary, run it with --only-analyze, and observe the list of tokens.\necho \u0026quot;Dogs and CATS\u0026quot; | lmgrep \\ --only-analyze \\ --analysis=' { \u0026quot;tokenizer\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;standard\u0026quot;}, \u0026quot;token-filters\u0026quot;: [ {\u0026quot;name\u0026quot;: \u0026quot;lowercase\u0026quot;}, {\u0026quot;name\u0026quot;: \u0026quot;englishminimalstem\u0026quot;} ] }' # =\u0026gt; [\u0026quot;dog\u0026quot;,\u0026quot;and\u0026quot;,\u0026quot;cat\u0026quot;]  Text Analysis The Elasticsearch documentation describes text analysis as:\n the process of converting unstructured text into a structured format that’s optimized for search.\n Therefore, to learn how the full-text search works it is important to understand how the text is, well, analyzed. The remainder of the post focuses on how text analysis is done in Lucene which is the library that powers search engines like Elasticsearch and Solr, and what lmgrep provides to analyze your text.\nLucene Text analysis in the Lucene land is defined by 3 types of components:\n list of character filters (changes to the text before tokenization, e.g. HTML stripping, character replacement, etc.), one tokenizer (splits text into tokens, e.g. at whitespace characters), list of token filters (normalizes the tokens, e.g. lowercases all the letters).  The combination of text analysis components makes an Analyzer. You can think that an analyzer is a recipe to convert a string into a list of tokens1.\nlmgrep lmgrep is a search tool that is based on the Lucene Monitor library. To do the full-text search it needs to do the same thing that likes of Elasticsearch are doing: to analyze text. lmgrep packs many text analysis components. Also, it provides a list of predefined analyzers. Nothing special here, the same battle tested and boring Lucene components that gets the job done2.\nHowever, lmgrep provides one clever twist to text analysis: a way to specify an analyzer using plain data in JSON, e.g.:\necho \u0026quot;\u0026lt;p\u0026gt;foo bars baz\u0026lt;/p\u0026gt;\u0026quot; | \\ lmgrep \\ --only-analyze \\ --analysis=' { \u0026quot;char-filters\u0026quot;: [ {\u0026quot;name\u0026quot;: \u0026quot;htmlStrip\u0026quot;}, { \u0026quot;name\u0026quot;: \u0026quot;patternReplace\u0026quot;, \u0026quot;args\u0026quot;: { \u0026quot;pattern\u0026quot;: \u0026quot;foo\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;bar\u0026quot; } } ], \u0026quot;tokenizer\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;standard\u0026quot;}, \u0026quot;token-filters\u0026quot;: [ {\u0026quot;name\u0026quot;: \u0026quot;englishMinimalStem\u0026quot;}, {\u0026quot;name\u0026quot;: \u0026quot;uppercase\u0026quot;} ] } ' # =\u0026gt; [\u0026quot;BAR\u0026quot;,\u0026quot;BAR\u0026quot;,\u0026quot;BAZ\u0026quot;]  Again, nothing special here, read the docs3 of an interesting text analysis component, e.g. character filter patternReplace, add its config to the --analysis, and apply it on your text.\nConceptually it is very similar to what Elasticsearch or Solr are providing: analysis part in the index configuration JSON in Elasticsearch, and Solr Schemas in XML.\nlmgrep analysis component has this structure:\n{\u0026quot;name\u0026quot;: \u0026quot;COMPONENT_NAME\u0026quot;, \u0026quot;args\u0026quot;: {\u0026quot;ARG_NAME\u0026quot;: \u0026quot;ARG_VALUE\u0026quot;}}  Notes:\n some components, e.g. stop token filter, expect a file as an argument. To support such components lmgrep brutally patched Lucene to load data from arbitrary files while preserving the predefined analyzers with, e.g. their custom stop-word files. when a predefined analyzer is provided for text analysis then all other analysis components are silently ignored. predefined analyzers do not support the args as of now, just the name. lmgrep as of now doesn\u0026rsquo;t provide a way to share components between analyzers.  That is pretty much all there is to know about how lmgrep does text analysis. Try it out and let me know how it goes.\n--only-analyze I like the Elasticsearch\u0026rsquo;s Analyze API. It allows me to look at the raw tokens that are either stored in the index or produced out of the search query.\nTo make debugging of lmgrep easier I wanted to expose something similar to Analyze API. The --only-analyze flag is my humble attempt to do that.\nWhen the flag is specified then lmgrep just outputs a list of tokens that is produced by applying an analyzer on the input text, e.g.:\necho \u0026quot;the quick brown fox\u0026quot; | lmgrep --only-analyze # =\u0026gt; [\u0026quot;the\u0026quot;,\u0026quot;quick\u0026quot;,\u0026quot;brown\u0026quot;,\u0026quot;fox\u0026quot;]  Implementation The machinery under the --only-analyze works as follows:\n one thread is dedicated to read and decode the text input (either from STDIN or a file), one thread is dedicated to write to the STDOUT, the remaining CPU cores can be used by a thread pool that analyzes the text (thanks to Lucene Analyzer implementation being thread-safe).  On my laptop lmgrep analyzes ~1GB of text in ~11 seconds and consumes maximum 609 MB of RAM. It should result in ~200 GB of text per hour. IMO, not bad. Of course, the more involved the text analysis is the longer it takes.\nNote that the output of --only-analyze has the same order as the input. IMO, it makes the output a bit easier to understand. However, preserving the order limits the throughput. It is because the time and resources needed to analyze an individual piece of text can vary greatly, and the required coordination introduces some overhead.\nConsider an example of analyzing the text attributes of a book: assume that the first line sent to lmgrep is the title of the book, the second line contains a full text of the book, and the third line is the summary. The title is relatively small, it is quickly analyzed and immediately written to STDOUT. The summary is a bit longer but still many times smaller than the body. To preserve the order lmgrep (before writing the tokens of the summary to STDOUT) waits for the analysis on the body to be finished and written to STDOUT and only then tokens of the summary are written out.\nNotes:\n the --explain flag is coming to lmgrep; the output lines are valid JSON (jq is your friend); the positional arguments for --only-analyze are interpreted as files and when present then STDIN is ignored.  Interesting Bit One thing that constantly frustrates me with Elasticsearch\u0026rsquo;s Analysis API is that I can\u0026rsquo;t specify custom char filters, tokenizer, and token filter directly in the body of the request to the Analysis API. To observe the output of text analysis that involves custom text analysis components first I have to create an index with an analyzer and then call Analyze API that involves that index. lmgrep avoids this pain point by allowing to declare text analysis components inline.\nPost Script All this analyzer construction wizardry is possible because of Lucene\u0026rsquo;s AbstractAnalysisFactory class and features provided by its subclasses. The CustomAnalyzer builder exposes methods that expects a Class as an argument, e.g. addCharFilter. The trick here is that, e.g. the class TokenFilterFactory provides a method availableTokenFilters that returns a set of names of token filters and with those names you can get a Class object that can be supplied to CustomAnalyzer builder methods.\nThe discovery of available factory classes is based on the classpath analysis, e.g. fetching all classes where name matches a pattern like .*FilterFactory and are subclasses of a TokenFilterFactory. However, for the reasons that were beyond my understanding, when I created my own TokenFilterFactory class it was not discovered by Lucene ¯\\_(ツ)_/¯.\nYeah, great, but lmgrep is compiled with the GraalVM native-image which assumes closed-world and throws the dynamism of the JVM out the window. How then does exactly this TokenFilterFactory-thing-class discovery works? Yes, Native images must include all the classes because at run-time it cannot create classes, but it can be worked around by providing the configuration with the classes that are going to be used at run-time, and those interesting classes can be reflectively discovered at run-time. lmgrep relies on the Java classes being discoverable at compile-time where the reflection works as expected.\nTo instruct the native-image to discover the Java classes from Clojure code you can specify the class under the regular def because to the native-image defs look like constants and are evaluated at compiled-time. So, if lmgrep misses some awesome Lucene token filter, all it takes is to add it to the hashmap under a def.\nFootnotes   Lucene TokenStreams are actually graphs \u0026#x21a9;\u0026#xfe0e;\n If something is missing then let me know by creating an issue here. \u0026#x21a9;\u0026#xfe0e;\n Just Google for \u0026ldquo;Lucene \u0026lt;COMPONENT_NAME\u0026gt;\u0026rdquo; \u0026#x21a9;\u0026#xfe0e;\n   ","date":1619136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619136000,"objectID":"892920d9055cec214fbc69f67037b7bc","permalink":"https://www.jocas.lt/blog/post/lucene-text-analysis/","publishdate":"2021-04-23T00:00:00Z","relpermalink":"/blog/post/lucene-text-analysis/","section":"post","summary":"`lmgrep` exposes easy to use interface to work with Lucene text analysis","tags":["Lucene","tokenization"],"title":"lmgrep Text Analysis","type":"post"},{"authors":["Dainius Jocas"],"categories":null,"content":"  Don\u0026#x27;t change the partition count for kafka topics!  from Dainius Jocas  ","date":1617822000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617822000,"objectID":"9a6af0ba39da40f35d43fa7656640380","permalink":"https://www.jocas.lt/blog/talk/vilnius-cloud-native-2021-04-07/","publishdate":"2021-04-13T19:00:00Z","relpermalink":"/blog/talk/vilnius-cloud-native-2021-04-07/","section":"talk","summary":"Kafka, Kafka Connect, Elasticsearch, and Data Consistency","tags":["meetup","kafka","kafka connect","elasticsearch"],"title":"Don't Change the Partition Count for Kafka Topics!","type":"talk"},{"authors":["Dainius Jocas"],"categories":["Linux","Hacks"],"content":"TL;DR Lenovo Thinkpad P53 is a powerful laptop but a regular Linux install is just too loud for me. However, it is just a machine and there must be several control knobs that can make the experience better. In this post, I\u0026rsquo;ll share my setup that makes the laptop to be silent most of the time and performant when needed.\nThe motivation To get my daily work done I spin many fat JVMs like Elasticsearch, Kafka, GraalVM native-image tool, docker-compose clusters, virtual machines, etc. To plough through those tasks a year ago (in early 2020) I\u0026rsquo;ve got a top spec\u0026rsquo;ed Lenovo Thinkpad P53 laptop:\n   Part Spec     CPU Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz   Memory 128GB (not a mistake here)   Graphics Processor Nvidia Quadro RTX 4000    The laptop(!) can be so power-hungry that the charger needs to provide up to 230W(!). Just imagine how loud the fans get when executing computationally intensive tasks. I simply have to leave the room and close the door until the task is finished.\nAnything that is CPU intensive makes the chip hot which spins the fans. By anything here I mean things like regular web browsing (looking at you JIRA), or a simple conference call (looking at you Google Meet and Zoom). I have some doubts that Lenovo has done the best possible job with the fans but I leave that on their conscience.\nAlthough I run a lot of intensive tasks on a laptop, not all of my tasks are that demanding. Therefore, I want the laptop to be performant when needed (in this case I don\u0026rsquo;t mind it to be loud) and be as silent as possible when that can be achieved. And one additional requirement: I want to work with a Linux machine.\nPrehistory When I got the laptop the first thing I\u0026rsquo;ve done was that I\u0026rsquo;ve installed Ubuntu since it is officially supported1. In a very short time, I\u0026rsquo;ve discovered that at that time it was impossible to silence the laptop to a satisfiable level: Linux just had no drivers that could silence all(!) the fans2.\nThe hope was lost: I had to boot up Windows, setup WSL, and accommodate my development environment there. All because several drivers were missing (sigh). It\u0026rsquo;s worth to mention that I\u0026rsquo;ve set up a Windows box after more than 10 years of not touching Windows. It was just an OEM install with all the drivers, updates, etc. However, Windows setup had its own problems: for some reason (to me it was not a surprise) Windows 10 could not properly handle the sleep-wake-up cycle (its a laptop after all). But hey, I could disable the Nvidia GPU, then enable it, and everything would be working ok-ish again. I could get stuff done.\nHowever, updating Windows is a thing that can be postponed only that long and when you do an update with a setup that can be seen as being a little bit exotic you might get a broken system that fails to boot up. Yeah, sure, then just run the system fix scripts and you can continue working.\nOf course, it doesn\u0026rsquo;t help to have a good experience that it turned out that the laptop arrived with a faulty motherboard that caused all kinds of troubles among which the most painful was random shutdowns. Also, after those shutdowns sometimes even booting up was a real struggle. I guess that poor machine had only the best intentions and was telling me to stop torturing it and by failing to work gave me some well-deserved leisure time.\nOne day the machine after a Windows update really got stuck during the reboot. A blank screen with fans maxed out and no reaction to any button or to power cable unplugging. Several Google searches away and I\u0026rsquo;ve discovered that it was a known problem with Thinkpad (not only P53). So, the motherboard got replaced and the laptop got a fair bit more stable.\nAlso, even Windows that has supposedly good driver support from everyone (manufacturer, OS) involved has the power consumption and fan control options that are not all that powerful after all: there are predefined profiles with no manual tunning available. That makes me wonder how that software got accepted to be released in the first place?\nThe Linux Setup The news somehow came to me that Linux kernel 5.11 has the support for the second fan that is present on some thinkpads3. It encouraged me to roll up the sleeves and get the P53 fans under my control.\nThe main pieces of the puzzle are the Linux kernel 5.11 and Kubuntu 21.04 pre-released: the kernel has the fans driver out of the box, and Kubuntu 21.04 provides Nvidia drivers that work with that kernel.\nAlso, there are a couple helper utilities that I\u0026rsquo;ve used:\n Fancontrol GUI thinkfan (with GUI) CPU Power GUI  Linux kernel I\u0026rsquo;ve installed 5.11 on Ubuntu 20.10 and it shouldn\u0026rsquo;t come to you as a surprise that Nvidia drivers were broken and the install script from the Nvidia website failed. So, a failed experiment.\nI\u0026rsquo;ve used the Mainline tool to install the kernel.\nKubuntu 21.04 When all hope was lost and I was about to go back to Windows for another half a year I got the last thing to try: why not upgrade Kubuntu to 21.04 which is not even in BETA? It would come with a newer kernel and most likely with Nvidia driver support.\nIt came with kernel 5.10 and Nvidia drivers worked. Unfortunately, the second fan was not detected. Another failed experiment.\nThe last hope was to ask Mainline to install kernel 5.11 and pray that Nvidia drivers are fine with it. Install, reboot, and voila: Nvidia driver works, an external monitor is detected (though only when connected with HDMI), and most importantly both fans are detected and controllable! Hurray!\nFancontrol Fancontrol is a GUI utility that allows you to control and set up fan profiles by adjusting two dots: for example, the temperature at which to start/stop spinning the fans and when to max them out.\nNote: I\u0026rsquo;ve installed it from the source code. Note: there is a standalone GUI but I like the integration into the KDE System Settings.\nThinkfan Check the set up instructions here.\nThere is even the GUI for the thinkfan. This will give a window like this:\nNote the two lines that starts with Fan. It means that two Thinkpad fans are detected and controlable.\nNote that with the thinkfan it is up to you to write the settings into the file.\nCPU Power GUI CPU Power GUI is a simple graphical utility that allows you to change the frequency limits of your CPU and its governor: Install it from the official Ubuntu repositories.\nsudo apt install cpupower-gui  The trick here is that high CPU frequency leads to lots of heat which in turn spins the fans. When the frequency is limited not that much of the heat is created which prevents fans from kicking in. I\u0026rsquo;ve noticed that the minimal frequency (800 MHz) is enough to get some work done (e.g. zoom call). The temperature rarely goes up more than 60 degrees celsius.\nOf course, with a limited CPU frequency, the machine is noticeably slower, but hey, it is a high-end CPU. The slowdown is most noticeable when starting up an app. For example, IntelliJ works but feels bad with the 800MHz limit.\nGreenWithEnvy For the Nvidia fans the GreenWithEnvy looks promissing but I guess that the Quadro cards are not supported.\nSummary It was a year-long development. Given the current setup, the tasks that are not super demanding can be done on a quiet machine where the only things I hear are my keyboard and the little Marshall Emberton speaker playing my favourite Rammstein tunes. When my crazy performance test is set up, I allow the fans to spin at full speed and get the task done as fast as it can.\nP.S. Nvidia and Linux Yeah, a laptop with an Nvidia GPU should be a red flag that prevents installing Linux in the first place. Of course, there are drivers and stuff, but in general good luck to have them properly working. However, Ubuntu gives you the setup that is worked out and works by default.\nAlso, there are open source drivers called Nouveau but they have problems of their own like external monitor support.\nWhat is worse with this P53 is that ALL video outputs are attached to the discrete GPU (what engineers in their right mind would do it?). This means that if drivers are not properly set-up Linux will able to output only 1 FPS while (for some reason) the mouse and some other windows (sometimes?) works without any lag.\nSure, things like Optimus, reverse PRIME exist but I can only wish you good luck setting up these.\nIn summary, I wish there was a modification of the laptop that had only the integrated graphics.\nP.S.S. Setup on the table P53 is a big and fat laptop. Since most of the time I work from home due to covid-19, I don\u0026rsquo;t need to move or touch the machine that often. Therefore, it can stand on the table as a \u0026ldquo;triangle\u0026rdquo; (see a picture). This position allows air to circulate a little better. Also, in this way, it takes somewhat less space on the table. On the other hand, I can\u0026rsquo;t use the laptop screen and the keyword.\nFootnotes   https://certification.ubuntu.com/hardware/202002-27747 \u0026#x21a9;\u0026#xfe0e;\n I\u0026rsquo;ve tried several other distros (like Fedora 33) but without any luck. \u0026#x21a9;\u0026#xfe0e;\n https://www.phoronix.com/scan.php?page=news_item\u0026amp;px=ThinkPad-Dual-Fan-Control-5.8 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1613779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613779200,"objectID":"30f3e453ab8efeaabdb9a5beead26d36","permalink":"https://www.jocas.lt/blog/post/silencing-p53/","publishdate":"2021-02-20T00:00:00Z","relpermalink":"/blog/post/silencing-p53/","section":"post","summary":"A battle story on how I've managed to silence the the Lenovo P53 lapton","tags":["Linux","Hacks"],"title":"Silencing the Lenovo Thinkpad P53 on Linux","type":"post"},{"authors":["Dainius Jocas"],"categories":["Lucene","GraalVM"],"content":"TL;DR What if grep supported the functionality of a proper search engine like Elasticsearch without a need to install any servers or index the files before searching? lmgrep aims to provide you just that. It is installed as just one executable file without any dependencies, provides a command-line interface, starts-up instantly, and works on macOS, Linux, and, yes, even Windows.\nSee the source code here.\nMy motivation Have you ever wished that grep supported tokenization, stemming, etc, so that you don\u0026rsquo;t have to write wildcard regular expressions all the time? I\u0026rsquo;ve also shared that question and on a one nice day, I\u0026rsquo;ve tried to scratch that itch by exposing the Lucene query syntax as a CLI utility. lmgep is the result of my effort. Give it a try and let me know how it goes.\nFull-text Search vs. grep I\u0026rsquo;m perfectly aware that comparing Lucene and grep is like comparing apples to oranges. However, I think that lmgrep is best compared with the very tool that inspired it, namely grep.\nAnyway, what does grep do? grep reads a line from stdin, examines the line to see if it should be forwarded to stdout, and repeats until stdin is exhausted1. lmgrep tries to mimick exactly that functionality. Of course, there are many more options to grep but it is the essence of the tool.\nSeveral notable advantages of lmgrep over grep:\n Lucene query syntax is better suited for full-text search; Boolean operators allow to construct complex, well-designed queries; Text analysis can be customized to the language of the documents; Fuzzy text searches; Flexible text analysis pipeline that includes, lowercasing, ASCII-folding, stemming, etc; regular expressions can be combined with other Lucene query components; Search matches can span multiple lines, i.e. search is not line-oriented.  Several notable limitations of lmgrep when compared to grep:\n grep is faster when it comes to raw speed for large text files; grep has a smaller memory footprint; Not all options of grep are supported;  Why Lucene? Lucene is a Java library that provides indexing and search features. Lucene has been more than 20 years in development and it is the library that powers many search applications. Also, many developers are already familiar with the Lucene query syntax and know how to leverage it to solve complicated information retrieval problems.\nHowever powerful Lucene is, it is not well-suited for CLI application. The main problem is the startup time of JVM. To reduce the startup time I\u0026rsquo;ve compiled lmgrep with the native-image tool provided by GraalVM. In this way, the startup time is around 0.01s for Linux, macOS, and Windows.\nHow does lmgrep work? lmgrep by default expects two parameters: a search query and a GLOB pattern (similar to regexp) to find files to execute lmgrep on. I assume that the dear reader doesn\u0026rsquo;t want to be tortured by reading the explanation on how the file names are being matched with GLOB, so I\u0026rsquo;ll skip it. Instead, I\u0026rsquo;ll focus on explaining how the search works within a file.\nlmgrep creates a Lucene Monitor (Monitor) object from the provided search query. Then text file is split into lines2. Each line of text is passed to the Monitor for searching. The Monitor then creates an in-memory Lucene index with a single document created out of the line of text. Then the Monitor runs the search query on that in-memory index in the good ol' Lucene way3. lmgrep takes the hits, formats them, and sends results to STDOUT. That is how lmgrep does the full-text search.\nThe overall searching approach is similar to the one of Percolator in Elasticsearch. lmgrep just limits the number of stored search queries to one and treats every text line as a document. A cool thing compared with the Percolator is that lmgrep provides exact offsets of the matched terms while Elasticsearch does not expose offsets when highlighting.\nThe described procedure seems to be somewhat inefficient. However, the query parsing for all the lines (and files) is done only once. Also, the searching itself is efficient thanks to Lucene in general and when queries are complicated thanks to the Presearcher of the Lucene Monitor in particular. Presearcher extracts terms from the search query and if none of these terms are in the index then a full query is not executed at all. Of course, many optimizations can be (and will be) implemented for lmgrep such as batching of the documents. In general, the performance is limited by the Lucene Monitor.\nWhat about the text analysis pipeline? By default, lmgrep uses the StandardTokenizer to tokenize text. Then the tokens are passed through several token filters in the following order: LowerCaseFilter, ASCIIFoldingFilter, and SnowballFilter which is given the EnglishStemmer. The same analysis pipeline is used for both the indexing and querying. All the components of the analysis pipeline are configurable via CLI flags, see the README. However, the order of the token filters, as of now, is not configurable. Moreover, various filters are not exposed at all (e.g. StopwordsFilter, or WordDelimiterGraphFilter, etc.). Supporting a more flexible analysis pipeline configuration is left out for future releases. The more users the tool has the faster new features will be implemented ;)\nPrehistory of the lmgrep Almost every NLP project that I\u0026rsquo;ve worked on had the component called dictionary annotator. Also, the vast majority of the projects used Elasticsearch in one way or another. The more familiar I\u0026rsquo;ve got with Elasticsearch I\u0026rsquo;ve got, the more of my NLP workload shifted towards implementing it inside Elasticsearch. One day I\u0026rsquo;ve discovered a tool called Luwak (a cool name isn\u0026rsquo;t it?) and read more about it. It kind of opened my eyes: the dictionary annotator can be implemented using Elasticsearch and the dictionary entries can be expressed as Elasticsearch queries. Thankfully, Elasticsearch has Percolator that hides all the complexity of managing temporary indices, batching search requests, etc.\nThen I was given was an NLP project where one of the requirements was to implement data analysis using AWS serverless stuff: Lambda for text processing and Dynamo DB for storage. Of course, one of the required NLP components was a dictionary annotator. Since Elasticsearch was not available (because it is not serverless) I still wanted to continue working with dictionary entries as search queries, I\u0026rsquo;ve decided to leverage the Luwak library. From experiences of that project, the Beagle library was born. lmgrep is loosely based on Beagle.\nWhen thinking about how to implement lmgrep I wanted it to be based on Lucene because of the full-text search features. To provide a good experience the start-up time must be small. To achieve it, lmgrep had to be compiled with the native-image tool of the GraalVM. I\u0026rsquo;ve tried but the native-image doesn\u0026rsquo;t support Method Handles that Lucene uses. Some more hacking was needed. I was lucky when I\u0026rsquo;ve discovered a toy project where the blog search was implemented on AWS Lambda that was backed by Lucene which was compiled by the native-image tool. I\u0026rsquo;ve cloned the repo, mvnw install, then included the artefacts to the dependencies list, and lmgrep compiled with the native-image tool successfully.\nThen the most complicated part was to prepare executable binaries for different operating systems. Plenty of CPU, RAM, VirtualBox with Windows and macOS virtual machines, and here we go.\nDid I say how much I enjoyed trying to get stuff done on Windows? None at all. How come that multiple different(!) command prompts are needed to get GraalVM to compile an executable? Now I know that it would a lot better to suffer the pain and to set up the Github Actions pipeline to compile the binaries and upload them to release pages.\nWhat is missing?  The analysis pipeline is not as flexible as I\u0026rsquo;d like to (UPDATE 2021-04-24: implemented); Leverage the multicore CPUs by executing the search in parallel; Batch documents for matching; Let me know if any?  What would be cool ways to use lmgrep?  tail logs to lmgrep and raise alerts; Give an alias for lmgrep with various options tailored for the code search (Java Example); Why not expose sci script as TokenFilter? Why not ngrams token filter then the search would be resilient to the typing errors? Static website search, like AWS Lambda that has lmgrep and goes through all files on demand without upfront indexing.  Summary lmgrep scratched my itch. It was exciting to get it working. I hope that you\u0026rsquo;ll also find it interesting and maybe useful. Give it a try, let me know how it was for you, and most importantly any feedback welcome on how to improve lmgrep.\nReferences  https://web.archive.org/web/20210116173133/https://swtch.com/~rsc/regexp/regexp4.html https://web.archive.org/web/20161018234331/http://www.techrepublic.com/article/graduating-from-grep-powerful-text-file-searching-with-isearch/  Footnotes   https://ideolalia.com/essays/composition-is-interpretation.html \u0026#x21a9;\u0026#xfe0e;\n there is no necessity to split text files into lines, it is just to mimik how grep operates. \u0026#x21a9;\u0026#xfe0e;\n of course, the description is over-simplified, but it is accurate enough to get the overall idea. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1611964800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611964800,"objectID":"8c249e35c6148b85767671c356a4fb60","permalink":"https://www.jocas.lt/blog/post/intro-to-lucene-grep/","publishdate":"2021-01-30T00:00:00Z","relpermalink":"/blog/post/intro-to-lucene-grep/","section":"post","summary":"Introduction to `lmgrep`: what is it and what it isn't. My a motivation to build it.","tags":["Lucene","GraalVM","grep"],"title":"lmgrep - Lucene Based grep-like Utility","type":"post"},{"authors":["Dainius Jocas"],"categories":["Elasticsearch","Kafka Connect"],"content":"TL;DR When the Elasticsearch indexer is highly concurrent, Kafka record keys are used as Elasticsearch document IDs, and indexer is set to delete records on null values, then Kafka Connect Elasticsearch Sink Connector might corrupt your data: documents that should not be deleted end up being deleted, or documents that should be deleted end up still being present in the index. The fix is to use external versioning for deletes in bulk requests as it is proposed in this Github Pull Request.\nThe problem NOTE: as of version 6.0.0 of the Confluent Platform (last checked on 2020-10-02) the bug that might lead to data corruption is still present.\nLet\u0026rsquo;s focus on a use case where Kafka record key is used as an Elasticsearch document ID1. I would consider this to be a proper practice when the documents represent a catalog of things.\nElasticsearch uses optimistic concurrency control. The job of this concurrency mechanism is to ensure that older version of the document doesn\u0026rsquo;t override a newer version. By default, order of arrival of the operation is applied, but the behaviour can be overriden in several ways depending on the version of Elasticsearch. In this post we focus on concurrent bulk requests, and with a concurrency that involves a network, requests will sometimes arrive out of order.\nTo help Elasticsearch resolve the out-of-order indexing requests Kafka Connect Elasticsearch Sink Connector (from here on Kafka Connect for short) leverages the external document versioning2. Using external versions in Kafka Connect makes sense because we already have versioning in place: Kafka topic partition offsets. If Kafka Connect applies changes to Elasticsearch indices in order of the topic offset, then any update ordering problems would be problems in the upstream system. This is a good guarantee to have.\nLet\u0026rsquo;s add to the mix delete operations. Kafka Connect supports a setting BEHAVIOR_ON_NULL_VALUES_CONFIG to \u0026quot;delete\u0026quot;. This setting instructs the Kafka Connect that a document in Elasticsearch with an ID of the kafka record key with null value (a tombstone message) is going to be deleted. But for some strange reason the deletes does not use external versioning! The line responsible for the described behaviour3 can be found here. This means that for deletes the order-of-arrival wins. Let\u0026rsquo;s increase the concurrency of bulk requests with the param MAX_IN_FLIGHT_REQUESTS_CONFIG to a largish number, and the data consistency problems is just round the corner for data that has some largish update ratio.\nThe issue is even more pronounced when you re-index data into Elasticsearch and you want to do it as fast as possible, which means doing the indexing concurrently.\nThe Example The code that demonstrated the faulty behaviour can be found in this Pull Request.\nThe test case is for testing the case when document should be present in Elasticsearch gets deleted.\nLet\u0026rsquo;s have a little walk over the code snippet:\nCollection\u0026lt;SinkRecord\u0026gt; records = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; numOfRecords - 1 ; i++) { if (i % 2 == 0) { SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, null, i); records.add(sinkRecord); } else { record.put(\u0026quot;message\u0026quot;, Integer.toString(i)); SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, i); records.add(sinkRecord); } } record.put(\u0026quot;message\u0026quot;, Integer.toString(numOfRecords)); SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, numOfRecords); records.add(sinkRecord); task.put(records); task.flush(null);  Here we send numOfRecords (which larger than 2) to a Kafka topic. Every second record has null body (delete operation), and the rest of the records have a sequence number as a message value. The very last record is always a non-null record with a message value of numOfRecords.\nLet\u0026rsquo;s setup a connector:\nKEY_IGNORE_CONFIG = \u0026quot;false\u0026quot;; MAX_IN_FLIGHT_REQUESTS_CONFIG = Integer.toString(numOfRecords) BATCH_SIZE_CONFIG = \u0026quot;1\u0026quot; LINGER_MS_CONFIG = \u0026quot;1\u0026quot; BEHAVIOR_ON_NULL_VALUES_CONFIG = \u0026quot;delete\u0026quot;  Here we set a connector to use Kafka record key as id KEY_IGNORE_CONFIG = \u0026quot;false\u0026quot;, set the indexer concurrency to the numOfRecords; set the indexing batch size to 1 (this creates as many requests to Elasticsearch as there are records in the Kafka topic); set indexer to send requests immediately with LINGER_MS_CONFIG = \u0026quot;1\u0026quot;; and record with a null value represents a delete operation.\nWith this setup after the indexing is done we expect that in the index we have a document with ID and whose message value is numOfRecords. But when ordering of bulk requests is out-of-order then at the end we might have a situation where there is no document in the index at all: the bulk index request with message = numOfRecords arrived before one of the bulk requests with a delete operation!\nThe situation might seem to be a bit far-fetched but for applications like e-commerce where you have a catalog that is frequently updated (e.g. the catalog item should be available in search or not) and updates are modelled as document deletes it happens a bit more often than it might be expected.\nThe fix The fix is simple: use the same external versioning that is already being used by the indexing requests also for delete requests:\nif (record.version != null) { req.setParameter(\u0026quot;version_type\u0026quot;, \u0026quot;external\u0026quot;).setParameter(\u0026quot;version\u0026quot;, record.version); }  The full code can be found here. Let\u0026rsquo;s hope that Confluent developers will find some time to merge that PR.\nConclusion Thank you for reading and leave your feedback here.\nP.S. Of course, this is not the only situation when data can get corrupted, e.g. changing the number of partitions; when you delete the topic, repopulate it with up-to-date data (also, you skip deletes) then restarting the indexing might pretty much nothing, because all the versions are earlier external version because offsets are smaller.\n   When Kafka Record keys are not used as Elasticsearch document IDs versioning is not a problem because every Elasticsearch ID is constructed as {topic}+{partition}+{offset} which creates a new document for every Kafka record, i.e. no versioning is needed. \u0026#x21a9;\u0026#xfe0e;\n Elasticsearch 7 supports the external versioning. \u0026#x21a9;\u0026#xfe0e;\n Yes, it is a comment, and it means that the developers were not sure whether to use external versioning for delete operations. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1601683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601683200,"objectID":"98b4170bae1f25c6aeaa33eab0ec010f","permalink":"https://www.jocas.lt/blog/post/kc_es_data_consistency/","publishdate":"2020-10-03T00:00:00Z","relpermalink":"/blog/post/kc_es_data_consistency/","section":"post","summary":"A shout-out about a lurking bug in the Kafka Connect Elasticsearch Sink connector","tags":["Elasticsearch","Kafka Connect"],"title":"How to Prevent Data Corruption in Elasticsearch When Using Kafka Connect Elasticsearch Sink Connector","type":"post"},{"authors":["Dainius Jocas"],"categories":["Elasticsearch","Kafka Connect"],"content":"TL;DR Specify your pipeline with the index.default_pipeline setting in the index (or index template) settings.\nThe Problem We need to index the log data into the Elasticsearch cluster using a Kafka Connect Elasticsearch Sink Connector 1, the data should be split into daily indices, and we need to specify the Elasticsearch ingest pipeline.\nThe documentation of the connector doesn\u0026rsquo;t mention anything about ingest pipelines. After a quick consultation with the Internet you discover that there is an open issue that Kafka Connect Elasticsearch Sink Connector doesn\u0026rsquo;t support specifying an Elasticsearch ingest pipeline. WAT?\nThe Workaround Say2, our pipeline3 just renames an attribute, e.g.:\nPUT _ingest/pipeline/my_pipeline_id { \u0026quot;description\u0026quot; : \u0026quot;renames the field name\u0026quot;, \u0026quot;processors\u0026quot; : [ { \u0026quot;rename\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;original_field_name\u0026quot;, \u0026quot;target_field\u0026quot;: \u0026quot;target_field_name\u0026quot; } } ] }  The Elasticsearch ingest pipeline for indexing can be specified in several ways:\n for each index request as a URL parameter, per bulk index request as a URL parameter, for every bulk index request operation, index settings (a dynamic attribute), index template.  First three options are not supported by Kafka Connect. The fourth option is not convenient in our case because the data should be split into time-based (e.g. daily) indices and we don\u0026rsquo;t want to do repetitive tasks4. The natural option to follow is to define an index template. In the index template we can specify the index.default_pipeline parameter, e.g.\nPUT _index_template/template_1 { \u0026quot;index_patterns\u0026quot;: [\u0026quot;daily_log*\u0026quot;], \u0026quot;template\u0026quot;: { \u0026quot;settings\u0026quot;: { \u0026quot;index.default_pipeline\u0026quot;: \u0026quot;my_pipeline_id\u0026quot; } } }  Note, that for indexing not to fail, we should create the Elasticsearch ingest pipeline5 before setting up the index template.\nThat is it, now when Kafka Connect will create a new daily index the Elasticsearch ingest pipeline is going to be applied to every document without any issues, for free, and in no time.\nBonus One thing to note is that only one pipeline can be specified for index.default_pipeline which might sound a bit limiting. A clever trick to overcome that limitation is to use a series of pipeline processors that can invoke other pipelines in the specified order, i.e. pipeline of pipelines.\nAlso, there is an index setting called index.final_pipeline that if specified is going to be executed after all other pipelines.\nTesting pipelines can be done using the _simulate API.\nFin Thanks for reading and leave comments or any other feedback on this blog post in the Github issue. Examples were tested to work with Elasticsearch and Kibana 7.8.1.\n  or any other technology that doesn\u0026rsquo;t support, or it is just not possible to specify the Elasticsearch ingest pipeline. \u0026#x21a9;\u0026#xfe0e;\n yes, I know that the same job can be done with the Kafka Connect Transformations. \u0026#x21a9;\u0026#xfe0e;\n let\u0026rsquo;s leave out the Kafka Connector setup. \u0026#x21a9;\u0026#xfe0e;\n set index.default_pipeline=my_pipeline_id for every new daily index with, say, a cron-job at midnight. \u0026#x21a9;\u0026#xfe0e;\n technically, before an index is created that matches the template pattern. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1596326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596326400,"objectID":"06ca0d3a6f9d9bff5e50afb4cab089bd","permalink":"https://www.jocas.lt/blog/post/ingest_pipeline_kafka_connect/","publishdate":"2020-08-02T00:00:00Z","relpermalink":"/blog/post/ingest_pipeline_kafka_connect/","section":"post","summary":"A workaround on how to leverage the Elasticsearch Ingest Pipelines when using Kafka Connect","tags":["Elasticsearch","Kafka Connect"],"title":"How to Use Elasticsearch Ingest Pipelines with Kafka Connect Elasticsearch Sink Connector","type":"post"},{"authors":["Dainius Jocas"],"categories":["Elasticsearch"],"content":"To analyze the textual data Elasticsearch uses analyzers while for the keyword analysis there is a thing called a normalizer. In this article I\u0026rsquo;ll explain what the normalizer is and show it\u0026rsquo;s use case for normalizing URLs.\nTL;DR A neat use case for keyword normalizers is to extract a specific part of the URL with a char_filter of the pattern_replace type.\nIntroduction In Elasticsearch the textual data is represented with two data types: text and keyword. The text type is meant to be used for full-text search use cases while keyword is mean for filtering, sorting, and aggregation.\nTL;DR About Analyzers To make a better use of text data you can setup the analyzer which is a combination of three components:\n exactly one tokenizer, zero or more character filters, zero or more token filters.  Basically, an analyzer transforms a single string into words, e.g. \u0026quot;This is my text\u0026quot; can be transformed into [\u0026quot;this\u0026quot;, \u0026quot;my\u0026quot;, \u0026quot;text\u0026quot;] which you can read as:\n text is split into tokens by tokenizer, each token is lowercased with the a token filter, stopwords are removed with another token filter.  Normalizers The documentation says that:\n Normalizers are similar to analyzers except that they may only emit a single token.\n Normalizers can only be applied to the keyword datatype. The cannonical use case is to lowercase structured content such as IDs, email addresses, e.g. a database stores emails in whatever case but searching for emails should be case insensitive. Note that only a subset of available filters can be used by a normalizer: all filters must work on a per-character basis, i.e. no stopwords or stemmers.\nNormalizers for Normalizing URL Data Storing a URL in a keyword field allows to filter, sort, and aggregate your data per URL. But what if you need to filter, sort, and aggregate by just one part of the URL and you have little to no control over the upstream data source? You have a couple of options:\n convince upstream to extract that one part in their code and send it to you, setup a text field with an analyzer that produces just that one token and enable field data (not a default setup and can get expensive). setup a keyword field with a normalizer with a char_filter. give up.  I want to explore the keyword option. In the next section I\u0026rsquo;ll show how to setup normalizers for Elasticsearch URLs.\nThe not so Synthetic Problem We have a list URLs without a hostname that were used to query Elasticsearch, e.g.: /my_search_index/_search?q=elasticsearch and we need to split URLs into parts such as: index, operation endpoint, e.g.: _search or _count, query filters, etc. In the following example I\u0026rsquo;ll focus on the extracting the index part of the URL.\nLet\u0026rsquo;s create an index:\nPUT elasticsearch_url_index { \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;normalizer\u0026quot;: { \u0026quot;index_extractor_normalizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;char_filter\u0026quot;: [ \u0026quot;index_name_extractor\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;index_name_extractor\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern_replace\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;/(.+)/.*\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;$1\u0026quot; } } } } }, \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot;: { \u0026quot;url\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;fields\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;normalizer\u0026quot;: \u0026quot;index_extractor_normalizer\u0026quot; } } } } } }  Here we setup the index with a normalizer index_extractor_normalizer that has a char filter index_name_extractor that uses a regex pattern_replace to extract characters between the first and the second slashes. The mappings have a property url which is of the keyword type and have a field index which is also of the keyword type and is set up to use the normalizer index_extractor_normalizer.\nSince the normalizer is basically a collection of filters we can use our good old friend _analyze API to test how it works.\nPOST elasticsearch_url_index/_analyze { \u0026quot;char_filter\u0026quot;: [\u0026quot;index_name_extractor\u0026quot;], \u0026quot;text\u0026quot;: [\u0026quot;/my_search_index/_search?q=elasticsearch\u0026quot;] }  Produces:\n{ \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;my_search_index\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 40, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 0 } ] }  Good, exactly as we wanted: /my_search_index/_search?q=elasticsearch =\u0026gt; my_search_index.\nLet\u0026rsquo;s index some data:\nPUT elasticsearch_url_index/_doc/0 { \u0026quot;url\u0026quot;: \u0026quot;/my_search_index/_search?q=elasticsearch\u0026quot; }  Let\u0026rsquo;s try to filter URLs by the index name:\nGET elasticsearch_url_index/_search?q=url:my_search_index  Produces:\n{ \u0026quot;took\u0026quot; : 0, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No results? What? Oh! Wrong field: url was used instead of url.index. Let\u0026rsquo;s try once again:\nGET elasticsearch_url_index/_search?q=url.index:my_search_index  Produces:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.2876821, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;elasticsearch_url_index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;0\u0026quot;, \u0026quot;_score\u0026quot; : 0.2876821, \u0026quot;_source\u0026quot; : { \u0026quot;url\u0026quot; : \u0026quot;/my_search_index/_search?q=elasticsearch\u0026quot; } } ] } }  As expected. Cool.\nBonus: a Trick with the docvalue_fields Another neat trick is that we can get out the index part of the URL from an Elasticsearch index using the docvalue_fields option in a request ,e.g.:\nGET elasticsearch_url_index/_search?q=url.index:my_search_index { \u0026quot;docvalue_fields\u0026quot;: [\u0026quot;url.index\u0026quot;] }  Produces:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.2876821, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;elasticsearch_url_index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;0\u0026quot;, \u0026quot;_score\u0026quot; : 0.2876821, \u0026quot;_source\u0026quot; : { \u0026quot;url\u0026quot; : \u0026quot;/my_search_index/_search?q=elasticsearch\u0026quot; }, \u0026quot;fields\u0026quot; : { \u0026quot;url.index\u0026quot; : [ \u0026quot;my_search_index\u0026quot; ] } } ] } }  The important part is this one:\n\u0026quot;fields\u0026quot; : { \u0026quot;url.index\u0026quot; : [ \u0026quot;my_search_index\u0026quot; ] }  A neat thing about the docvalue_fields is that in the example above the my_search_index value is not comming from the _source of the document. This means that we can use keywords and by extension normalized keywords to fetch an exact value from the Elasticsearch index and not necessarily the one that was sent to Elasticsearch which somewhat solves our dependency from the upstream systems.\nNotes The setup is done in the Kibana Dev Tools with the Elasticsearch 7.7.0.\nThe pattern \u0026quot;/(.+)/.*\u0026quot; is a bit simplified purely for presentation purposes and doesn\u0026rsquo;t work as expected for URLs with more than 2 slashes, e.g.: /index/type/_search would produce index/type. You need something a bit more involved like \u0026quot;/([^/]+)/.*\u0026quot;.\nFin That is all I wanted to show you today. Hope it might be useful/interesting to someone down the line. Leave comments on the Github issue here. Cheers!\n","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"997b41d22469f11e9b603503b55089b9","permalink":"https://www.jocas.lt/blog/post/elasticsearch-normlizers/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/blog/post/elasticsearch-normlizers/","section":"post","summary":"In this article I'll explain what the normalizer is and show it's use case for **normalizing** URLs.","tags":["Elasticsearch"],"title":"A Neat Trick with Elasticsearch Normalizers","type":"post"},{"authors":["Dainius Jocas"],"categories":["AWS Lambda","babashka","Clojure","GraalVM"],"content":"TL;DR\nI\u0026rsquo;ve managed to package a simple babashka script to an AWS Lambda Custom Runtime. Here is the code, try for yourself.\nMotivation Wouldn\u0026rsquo;t it be great to deploy little Clojure code snippets to Custom Lambda Runtime? The main benefits would be:\n you would not suffer from java cold-start problems; you wouldn\u0026rsquo;t need to compile your project with GraalVM native-image tool which is time consuming and for anything more advanced is not likely to work anyway; babashka supports scripting with a subset of Clojure, which might do the work for you.  The plan I know what it takes to deploy to Lambda Custom Runtime. Last year I\u0026rsquo;ve created a Clojure project template for deploying GraalVM compiled AWS Lambda Custom Runtime. And babashka is just another self contained binary. It should be too hard to bring two things together and get it working? Challenge accepted.\nPackaging I like to build software inside Docker containers. In this experiment, for the first attempt I\u0026rsquo;ve used this Dockerfile:\nFROM borkdude/babashka:latest as BABASHKA FROM clojure:tools-deps-alpine as BUILDER RUN apk add --no-cache zip WORKDIR /var/task COPY --from=BABASHKA /usr/local/bin/bb bb ENV GITLIBS=\u0026quot;.gitlibs/\u0026quot; COPY lambda/bootstrap bootstrap COPY deps.edn deps.edn RUN clojure -Sdeps '{:mvn/local-repo \u0026quot;./.m2/repository\u0026quot;}' -Spath \u0026gt; cp COPY src/ src/ COPY resources/ resources/ RUN zip -q -r function.zip bb cp bootstrap .gitlibs/ .m2/ src/ resources/ deps.edn  Here:\n copy bb binary from babashka Docker image, download the dependencies for babashka script using clojure (both, maven and git dependencies are supported, like is described here), write a classpath to the cp file, copy all source code, zip the required contents to the function.zip.  Every line of this dockerfile is packed with details but I\u0026rsquo;ll leave it for the future posts.\nI\u0026rsquo;ve packaged all dependencies for lambda into function.zip. The contents of the archive are:\n bb: babashka binary bootstrap: AWS Lambda entry point script cp: generated classpath text file deps.edn .gitlibs: directory with gitlibs .m2: directory with Maven dependencies resources: src: directory with babashka scripts  Custom runtime discoveries Finally, having all dependencies packaged up, I\u0026rsquo;ve deployed the function.zip to AWS Lambda. The first error message was not very encouraging:\nUtil_sun_misc_Signal.ensureInitialized: CSunMiscSignal.create() failed. errno: 38 Function not implemented Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed. JavaFrameAnchor dump: No anchors TopFrame info: TotalFrameSize in CodeInfoTable 32 VMThreads info: VMThread 0000000003042750 STATUS_IN_JAVA (safepoints disabled) java.lang.Thread@0x264fa98 VM Thread State for current thread 0000000003042750: 0 (8 bytes): com.oracle.svm.jni.JNIThreadLocalEnvironment.jniFunctions = (bytes) 0000000003042750: 0000000002293a88 8 (32 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.regularTLAB = (bytes) 0000000003042758: 00007f7809500000 00007f7809600000 0000000003042768: 00007f7809507160 0000000000000000 40 (8 bytes): com.oracle.svm.core.heap.NoAllocationVerifier.openVerifiers = (Object) null 48 (8 bytes): com.oracle.svm.core.jdk.IdentityHashCodeSupport.hashCodeGeneratorTL = (Object) null 56 (8 bytes): com.oracle.svm.core.snippets.SnippetRuntime.currentException = (Object) null 64 (8 bytes): com.oracle.svm.core.thread.JavaThreads.currentThread = (Object) java.lang.Thread 000000000264fa98 72 (8 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.activeTimer = (Object) null 80 (8 bytes): com.oracle.svm.jni.JNIObjectHandles.handles = (Object) com.oracle.svm.core.handles.ThreadLocalHandles 00007f7809501558 88 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPendingException.pendingException = (Object) null 96 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPinnedObjects.pinnedObjectsListHead = (Object) null 104 (8 bytes): com.oracle.svm.jni.JNIThreadOwnedMonitors.ownedMonitors = (Object) null 112 (8 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.freeList = (Word) 0 0000000000000000 120 (8 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.stackBoundaryTL = (Word) 1 0000000000000001 128 (8 bytes): com.oracle.svm.core.stack.JavaFrameAnchors.lastAnchor = (Word) 0 0000000000000000 136 (8 bytes): com.oracle.svm.core.thread.VMThreads.IsolateTL = (Word) 25636864 0000000001873000 144 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadHandleTL = (Word) 50477184 0000000003023880 152 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadIdTL = (Word) 50477184 0000000003023880 160 (8 bytes): com.oracle.svm.core.thread.VMThreads.nextTL = (Word) 0 0000000000000000 168 (4 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.yellowZoneStateTL = (int) -16843010 fefefefe 172 (4 bytes): com.oracle.svm.core.snippets.ImplicitExceptions.implicitExceptionsAreFatal = (int) 0 00000000 176 (4 bytes): com.oracle.svm.core.thread.Safepoint.safepointRequested = (int) 2147473200 7fffd730 180 (4 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.currentPauseDepth = (int) 0 00000000 184 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.safepointsDisabledTL = (int) 1 00000001 188 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.statusTL = (int) 1 00000001 VMOperation dump: No VMOperation in progress Dump Counters: Raw Stacktrace: 00007ffeb8e0a940: 000000000186e776 000000000207b9d0 00007ffeb8e0a950: 0000000001873000 000000000085b37c 00007ffeb8e0a960: 000000000084540a 00000000008454ca 00007ffeb8e0a970: 000000000264f128 000000000264ef58 00007ffeb8e0a980: 00007f78095018d8 0000000002650640 00007ffeb8e0a990: 000000000264f128 0000002602650c18 00007ffeb8e0a9a0: 0000000000845444 00007ffeb8e0a970 00007ffeb8e0a9b0: 0000000000000000 0000000000845f6e 00007ffeb8e0a9c0: 0000000002650e18 0000000002650c18 00007ffeb8e0a9d0: 0000000002650e18 0000000002070c60 00007ffeb8e0a9e0: 00000000021f48f8 00000000012b77e6 00007ffeb8e0a9f0: 0000000002650e18 0000000002650c18 00007ffeb8e0aa00: 0000001000000000 0000000002070c60 00007ffeb8e0aa10: 00007f7809507138 0000000000477f69 00007ffeb8e0aa20: 00007f7809503b88 00007f7809501910 00007ffeb8e0aa30: 00007f7809507138 00000000004831b4 00007ffeb8e0aa40: 0000000000000010 000000000085d16d 00007ffeb8e0aa50: 000000000000003b 00000000008b4bdb 00007ffeb8e0aa60: 000000000291e970 00007f7809504828 00007ffeb8e0aa70: 0000000100000007 0000000001079a70 00007ffeb8e0aa80: 00007f78095070b8 00007f7809507080 00007ffeb8e0aa90: 0000000001873000 000000000291e970 00007ffeb8e0aaa0: 00007f7809506f78 00007f78095070b8 00007ffeb8e0aab0: 0000000000000008 0000000000000010 00007ffeb8e0aac0: 0000000000000010 00000000008144a1 00007ffeb8e0aad0: 0000000000000007 0000000000cd7c2e 00007ffeb8e0aae0: 00007f7809504938 0000000001873000 00007ffeb8e0aaf0: 0000000002205088 00007f78095070b8 00007ffeb8e0ab00: 00007f7809507080 0000000cc0001000 00007ffeb8e0ab10: 0000000000000000 0000000000cd73eb 00007ffeb8e0ab20: 00007f7809503b58 00007f78095070b8 00007ffeb8e0ab30: 00007f7809507080 00007f78095038e0 00007ffeb8e0ab40: 00007f7807c8e388 000000000205e900 00007ffeb8e0ab50: 00007f7809501350 000000240000000c 00007ffeb8e0ab60: 000000000000000c 00007f78095038e0 00007ffeb8e0ab70: d15c483b00000000 00000000004830e5 00007ffeb8e0ab80: 0000000000000007 00007f78095038e0 00007ffeb8e0ab90: 00007f78095038e0 00000000006f2b33 00007ffeb8e0aba0: 000000000205e900 0000000002070448 00007ffeb8e0abb0: 00007f78095070b8 0000000000cd8b3d 00007ffeb8e0abc0: 00000000020864c8 0000000000cbffc1 00007ffeb8e0abd0: 0000000002070448 00007f78095070b8 00007ffeb8e0abe0: 0000000c00000000 00007f7809505ef8 00007ffeb8e0abf0: 00007f78095070d8 00007f7809504840 00007ffeb8e0ac00: 7cab467402070d98 0000000000fbfc08 00007ffeb8e0ac10: 0000000002634470 00007f7809507020 00007ffeb8e0ac20: 0000000001873000 00007f78095070d8 00007ffeb8e0ac30: 00007f7809504840 0000000000cc187e 00007ffeb8e0ac40: 0000000000000000 0000000000000000 00007ffeb8e0ac50: 00007f7807c91840 00007f7809504840 00007ffeb8e0ac60: 0000000002070d98 0000000000cc17b9 00007ffeb8e0ac70: 0000000000c848f0 00007f78095038e0 00007ffeb8e0ac80: 0000000002b33a78 0000000100cc4f83 00007ffeb8e0ac90: 0000000000483140 00000000004b5713 00007ffeb8e0aca0: 0000000002070d98 0000000000cdae9a 00007ffeb8e0acb0: 000000000209a600 00007f78095038e0 00007ffeb8e0acc0: 0000000002b33a78 000000000047c576 00007ffeb8e0acd0: 000000000209a600 000000000209a630 00007ffeb8e0ace0: 0000000002a1b8d8 0000000002a1b408 00007ffeb8e0acf0: 000000000209a600 00000000017acc23 00007ffeb8e0ad00: 0000000000000001 0000000000001000 00007ffeb8e0ad10: 0000000000000000 0000000000000000 00007ffeb8e0ad20: 0000000000000000 0000000000000000 00007ffeb8e0ad30: 0000000000000000 0000000000000000 Stacktrace Stage0: RSP 00007ffeb8e0a940 RIP 000000000085b3f6 FrameSize 32 RSP 00007ffeb8e0a960 RIP 000000000085b37c FrameSize 16 RSP 00007ffeb8e0a970 RIP 00000000008454ca FrameSize 80 RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e FrameSize 48 RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 FrameSize 48 RSP 00007ffeb8e0aa20 RIP 0000000000477f69 FrameSize 32 RSP 00007ffeb8e0aa40 RIP 00000000004831b4 FrameSize 320 RSP 00007ffeb8e0ab80 RIP 00000000004830e5 FrameSize 32 RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 FrameSize 256 RSP 00007ffeb8e0aca0 RIP 00000000004b5713 FrameSize 48 RSP 00007ffeb8e0acd0 RIP 000000000047c576 FrameSize 160 RSP 00007ffeb8e0ad70 RIP 000000000047c285 FrameSize 32 RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 FrameSize 256 RSP 00007ffeb8e0ae90 RIP 000000000048f162 FrameSize 32 RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c FrameSize 1 Stacktrace Stage1: RSP 00007ffeb8e0a940 RIP 000000000085b3f6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a960 RIP 000000000085b37c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a970 RIP 00000000008454ca com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aa20 RIP 0000000000477f69 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aa40 RIP 00000000004831b4 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ab80 RIP 00000000004830e5 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aca0 RIP 00000000004b5713 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0acd0 RIP 000000000047c576 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ad70 RIP 000000000047c285 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ae90 RIP 000000000048f162 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code Full Stacktrace: RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.jdk.VMErrorSubstitutions.shutdown(VMErrorSubstitutions.java:111) RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:74) RSP 00007ffeb8e0a960 RIP 000000000085b37c [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:59) RSP 00007ffeb8e0a970 RIP 00000000008454ca [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.ensureInitialized(SunMiscSubstitutions.java:176) RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.numberFromName(SunMiscSubstitutions.java:223) RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.findSignal(Signal.java:78) RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.\u0026lt;init\u0026gt;(Signal.java:140) RSP 00007ffeb8e0aa20 RIP 0000000000477f69 [image code] babashka.impl.pipe_signal_handler$handle_pipe_BANG_.invokeStatic(pipe_signal_handler.clj:11) RSP 00007ffeb8e0aa40 RIP 00000000004831b4 [image code] babashka.main$main.invokeStatic(main.clj:282) RSP 00007ffeb8e0ab80 RIP 00000000004830e5 [image code] babashka.main$main.doInvoke(main.clj:282) RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137) RSP 00007ffeb8e0aca0 RIP 00000000004b5713 [image code] clojure.core$apply.invokeStatic(core.clj:665) RSP 00007ffeb8e0acd0 RIP 000000000047c576 [image code] babashka.main$_main.invokeStatic(main.clj:442) RSP 00007ffeb8e0ad70 RIP 000000000047c285 [image code] babashka.main$_main.doInvoke(main.clj:437) RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137) RSP 00007ffeb8e0ae90 RIP 000000000048f162 [image code] babashka.main.main(Unknown Source) RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.runCore(JavaMainWrapper.java:151) RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:186) RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.code.IsolateEnterStub.JavaMainWrapper_run_5087f5482cc9a6abc971913ece43acb471d2631b(IsolateEnterStub.java:0) [Native image heap boundaries: ReadOnly Primitives: 0x1873008 .. 0x206f048 ReadOnly References: 0x206ff78 .. 0x24fc9f8 Writable Primitives: 0x24fd000 .. 0x26343e0 Writable References: 0x2634470 .. 0x2ba42c0] [Heap: [Young generation: [youngSpace: aligned: 0/0 unaligned: 0/0]] [Old generation: [fromSpace: aligned: 0/0 unaligned: 0/0] [toSpace: aligned: 0/0 unaligned: 0/0] ] [Unused: aligned: 0/0]] Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed. RequestId: 263ff1be-425d-4dcb-9ea5-67020dc3041b Error: Runtime exited with error: exit status 99 Runtime.ExitError  The fight After some Googling I\u0026rsquo;ve discovered several related clues here and here. They say that signals are not supported in AWS lambda. So, why not to disable signals for babashka and see what happens? I\u0026rsquo;ve forked the repo, made a flag that disables PIPE signal handling, deployed babashka to the docker hub and tried to deploy lambda once again.\nAnd? It worked:\nmake function-name=$(make get-function-name) invoke-function =\u0026gt; {\u0026quot;test\u0026quot;:\u0026quot;test914\u0026quot;}{ \u0026quot;StatusCode\u0026quot;: 200, \u0026quot;ExecutedVersion\u0026quot;: \u0026quot;$LATEST\u0026quot; }  Summary Here is the example of babashka script that can be deployed to AWS Lambda.\n The function.zip weights just 18MB. The cold startup of the Lambda that is given 128MB of RAM is ~400ms. Subsequent calls ranges from 4ms and 120ms. The more RAM you give the faster lambda gets. I can develop the code in Cursive as the structure is like of an ordinary Clojure deps.edn project (and it can be used on the JVM). I made a PR to babashka and I\u0026rsquo;ve got accepted.  Next Steps  Fix Problem building on macos (/tmp dir is not writable). Get rid of AWS CloudFormation part. Work a bit more to support AWS API Gateway. Create a template for such projects.  ","date":1584748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584748800,"objectID":"2d26caeb6ab5a7424a8d00ad7870fe3d","permalink":"https://www.jocas.lt/blog/post/babashka-aws-lambda/","publishdate":"2020-03-21T00:00:00Z","relpermalink":"/blog/post/babashka-aws-lambda/","section":"post","summary":"Adventures with babashka and AWS Lambda.","tags":["AWS Lambda","babashka","clojure","GraalVM"],"title":"Deploy babashka script to AWS Lambda","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch"],"content":"I want to take a look at Search Templates for Elasticsearch. Let\u0026rsquo;s apply them to examples from previous post on Synonym Graphs.\nSetup I\u0026rsquo;m using Elasticsearch 7.5.1.\nIndex configuration:\nDELETE test_index-1 PUT /test_index-1 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;descrition\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;entity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;lowercase_keyword_analyzer\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;synonym_graph_analyzer\u0026quot; } } }, \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;synonym_graph_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_synonym_graph\u0026quot; ] }, \u0026quot;lowercase_keyword_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot; ], \u0026quot;char_filter\u0026quot;: [ \u0026quot;spaces_to_undescores_filter\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;spaces_to_undescores_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mapping\u0026quot;, \u0026quot;mappings\u0026quot;: [ \u0026quot; \\\\u0020 =\u0026gt; _\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;my_synonym_graph\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym_graph\u0026quot;, \u0026quot;lenient\u0026quot;: true, \u0026quot;synonyms\u0026quot;: [ \u0026quot;very important thing =\u0026gt; very_important_thing\u0026quot; ] } } } } } }  Index the document:\nPUT test_index-1/_doc/1 { \u0026quot;description\u0026quot;: \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot;: \u0026quot;Very Important Thing\u0026quot; }  Search queries:\n prefix very important thing suffix  Templates I\u0026rsquo;m very interested in one particular use of the search templates: how flexible is the management of stored seach templates? Can I update a search template while receiving queries?\nAdd a template:\nPOST _scripts/synonym-graph-search { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;mustache\u0026quot;, \u0026quot;source\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;{{query_string}}\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } } } }  Try to run the search:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: \u0026quot;suffix very important thing prefix\u0026quot; } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.5753642, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.5753642, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Exactly as expected.\nWhen using a stored search template the Elasticsearch client doesn\u0026rsquo;t need to handle the complex query construction.\nTemplates are updateable Let\u0026rsquo;s try to update the template with a higher boost value:\nPOST _scripts/synonym-graph-search { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;mustache\u0026quot;, \u0026quot;source\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;{{query_string}}\u0026quot;, \u0026quot;boost\u0026quot;: 5 } } } } } }  Works.\nNow let\u0026rsquo;s run the same query:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: \u0026quot;suffix very important thing prefix\u0026quot; } }  This yields:\n{ \u0026quot;took\u0026quot; : 4, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.4384103, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.4384103, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  The scores are 0.5753642 and 1.4384103 that is ~2/5. Cool! This means that without changing (and redeploying) the Elasticsearch client we can change the querying logic, making the query an more dynamic.\nCorner Cases What if we run query has more attributes, e.g.:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: \u0026quot;suffix very important thing prefix\u0026quot;, \u0026quot;new_attr\u0026quot;: \u0026quot;123\u0026quot; } }  Works as expected!\nWhen query_string is null:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: null } }  Works!\nWhat if the param is not provided:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;new_attr\u0026quot;: \u0026quot;value\u0026quot; } }  No error!\nWhat if we provide a list instead of a string:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: [\u0026quot;this\u0026quot;, \u0026quot;Very Important Thing\u0026quot;] } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.4384103, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.4384103, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Instead of profiding one value we can replace it with a list. Good!\nMetadata of the search template It would be great to be able to store some metadata with the search template script, e.g. Git commit SHA of the query. I couldn\u0026rsquo;t find a way to do this. A workaround might be to _name attribute of the query. E.g.:\nPOST _scripts/synonym-graph-search { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;mustache\u0026quot;, \u0026quot;source\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;_name\u0026quot;: \u0026quot;GIT COMMIT SHA\u0026quot;, \u0026quot;query\u0026quot;: \u0026quot;{{query_string}}\u0026quot;, \u0026quot;boost\u0026quot;: 5 } } } } } }  The response:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.4384103, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.4384103, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; }, \u0026quot;matched_queries\u0026quot; : [ \u0026quot;GIT COMMIT SHA\u0026quot; ] } ] } }  Not great but might be useful.\nDiscussion  Templates doesn\u0026rsquo;t support search index specification. Field names can be parameterized, this feature alows to start/stop using a new/old field. Search template can be tested in (even in production cluster) independently. We can run our query against multiple search templates. Combine this with the Profile API and performance can be compared. Explain API also is supported.  ","date":1577059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577059200,"objectID":"dae7e72b85246f090d3d6b15c05e133d","permalink":"https://www.jocas.lt/blog/post/on-search-templates/","publishdate":"2019-12-23T00:00:00Z","relpermalink":"/blog/post/on-search-templates/","section":"post","summary":"A couple of examples and notes on using Elasticsearch search templates","tags":["elasticsearch"],"title":"Using Search Templates in Elasticsearch","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch"],"content":"I\u0026rsquo;ve written that if you google for How can you match a long query text to a short text field? you\u0026rsquo;re advised to use Elasticsearch Percolator. Today I\u0026rsquo;ll show an alternative way of solving the same problem with Elasticsearch.\nThe main idea is to use Synonym Graph Token Filter with some data preparation.\nProblem Statement Say that we learned how extract some entity from free form text with techniques such as NER, dictionary annotations, or some fancy Machine Learning. And when this entity is mentioned in the search query we want to boost documents that mention this entity. Also, say you\u0026rsquo;ve ruled out using Elasticsearch Percolator because it increases network latency because it requires additional call to Elasticsearch.\nFor further discussion our unstructured text is going to be This description is about a Very Important Thing and something else. and the extracted entity Very Important Thing. Our test document looks like :\n{ \u0026quot;description\u0026quot;: \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot;: \u0026quot;Very Important Thing\u0026quot; }  Search queries:\n prefix very important thing suffix prefix very important another thing suffix prefix thing suffix  All examples are tested on Elasticsearch 7.5.1.\nNaive Setup Let\u0026rsquo;s create an index for our documents:\nPUT /test_index-2 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;entity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } }  Entity field is of type text because we want it to be searchable. keyword type won\u0026rsquo;t work because it does only exact matches and out query most likely will be longer than our entity string.\nIndex our document:\nPUT test_index-2/_doc/1 { \u0026quot;description\u0026quot;: \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot;: \u0026quot;Very Important Thing\u0026quot; }  Search the index with the query that mentions our very important thing:\nGET test_index-2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.7260926, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-2\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.7260926, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Cool, we found what we we looking for.\nLet\u0026rsquo;s try another query, this time with a mention of very important another thing:\nGET test_index-2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important another thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.7260926, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-2\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.7260926, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Oh, the results are the same as with the previous query despite the fact that we mention Another Thing here. But it still might be OK because we matched all the terms of the entity.\nLet\u0026rsquo;s try another query:\nGET test_index-2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.5753642, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-2\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.5753642, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Oh no, we still matched our Very Important Thing while only thing term is present in the query. But at least this time the score is lower than with previous twoqueries, 0.5753642 vs. 1.7260926. Here we clearly see the problem: we are matching short strings with long strings and partial matches raises problems.\nProposed Solution Let\u0026rsquo;s leverage Synonym Graph Token Filter to solve our problem.\nPUT /test_index-1 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;descrition\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;entity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;lowercase_keyword_analyzer\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;synonym_graph_analyzer\u0026quot; } } }, \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;synonym_graph_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_synonym_graph\u0026quot; ] }, \u0026quot;lowercase_keyword_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot; ], \u0026quot;char_filter\u0026quot;: [ \u0026quot;spaces_to_undescores_filter\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;spaces_to_undescores_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mapping\u0026quot;, \u0026quot;mappings\u0026quot;: [ \u0026quot; \\\\u0020 =\u0026gt; _\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;my_synonym_graph\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym_graph\u0026quot;, \u0026quot;lenient\u0026quot;: true, \u0026quot;synonyms\u0026quot;: [ \u0026quot;very important thing =\u0026gt; very_important_thing\u0026quot; ] } } } } } }  Let\u0026rsquo;s decompose this large index configuration piece by piece:\n The entity attribute now has separate analyzers for both index and search phases. The lowercase_keyword_analyzer uses keyword tokenizer which means that tokenization will result in the sequence of token of size 1, then it normalizes tokens by lowercasing them and finally spaces_to_undescores_filter, replaces spaces to underscores. E.g. a string \u0026quot;Very Important Thing\u0026quot; is transformed into list of tokens [\u0026quot;very_important_thing\u0026quot;]. Or use out friend _analyze API:  POST test_index-1/_analyze { \u0026quot;text\u0026quot;: [\u0026quot;Very Important Thing\u0026quot;], \u0026quot;analyzer\u0026quot;: \u0026quot;lowercase_keyword_analyzer\u0026quot; }  This yields:\n{ \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;very_important_thing\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 20, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 0 } ] }  The synonym_graph_analyzer use standard tokenizer, which is followed by the lowercase filter, and then the my_synonym_graph token filter is applied. We\u0026rsquo;ve set up one synonym \u0026quot;very important thing =\u0026gt; very_important_thing\u0026quot;. E.g.  POST test_index-1/_analyze { \u0026quot;text\u0026quot;: [\u0026quot;prefix very important thing suffix\u0026quot;], \u0026quot;analyzer\u0026quot;: \u0026quot;synonym_graph_analyzer\u0026quot; }  This yields:\n{ \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;prefix\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 6, \u0026quot;type\u0026quot; : \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot; : 0 }, { \u0026quot;token\u0026quot; : \u0026quot;very_important_thing\u0026quot;, \u0026quot;start_offset\u0026quot; : 7, \u0026quot;end_offset\u0026quot; : 27, \u0026quot;type\u0026quot; : \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot; : 1 }, { \u0026quot;token\u0026quot; : \u0026quot;suffix\u0026quot;, \u0026quot;start_offset\u0026quot; : 28, \u0026quot;end_offset\u0026quot; : 34, \u0026quot;type\u0026quot; : \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot; : 2 } ] }  After analysis we have 3 tokens [\u0026quot;prefix\u0026quot;, \u0026quot;very_important_thing\u0026quot;, \u0026quot;suffix\u0026quot;]. Notice \u0026quot;very_important_thing\u0026quot; token: this is equal to the right-hand-side from our synonym definitions. Now let\u0026rsquo;s run queries from the previous section:\nGET test_index-1/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.5753642, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.5753642, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  As expected: exact match -\u0026gt; hit.\nAnother query:\nGET test_index-1/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important another thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 0, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No hits! Good! The document is not going to be boosted despite the fact that all tokens match.\nAnd the last one:\nGET test_index-1/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No hits! Good. This means that also substring doesn\u0026rsquo;t match.\nDiscussion Synonym Graph Token Filter can \u0026ldquo;replace\u0026rdquo; a sequence of tokens (e.g. a phrase) with another sequence of tokens. In this particular example: many tokens were replaced with one token.\n One field can have only one analyzer pair for index and search phases. If we want another analysis pipeline for the entity attribute we have to create another field with the analyzers specified, e.g. stemmed phrase with lower boost. The synonym list must be prepared before the index creation. Management of the synonym list might complicate index management, e.g. you use templates for your index management. The overal solution in general might look a bit too complicated.  ","date":1576972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576972800,"objectID":"bbf1ba4712ba08644677c25dbacd8b0b","permalink":"https://www.jocas.lt/blog/post/synonym-graph-phrase-search/","publishdate":"2019-12-22T00:00:00Z","relpermalink":"/blog/post/synonym-graph-phrase-search/","section":"post","summary":"An idea on how to search for short string with a long query string","tags":["elasticsearch"],"title":"Phrase Search with Synonym Graph Token Filter in Elasticsearch","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch","percolator"],"content":"This time I need to percolate texts with different analyzers for index and search analyzers.\nLet\u0026rsquo;s elaborate a bit on previous article and explicitly declare analyzers to use.\nDefine index:\nPUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } }  Then define 2 slightly different percolator queries (notice the difference between \u0026quot;bonsai tree\u0026quot; and \u0026quot;bonsai, tree\u0026quot;).\nPUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai tree\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot; } } } } PUT /my-index/_doc/2?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai, tree\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot; } } } }  Let\u0026rsquo;s percolate:\nGET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 80, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 2, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai, tree\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;standard\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } }, { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai tree\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;standard\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  As expected: 2 documents matched.\nBut now lets change the analyzer of the second percolation query to whitespace:\nPUT /my-index/_doc/2?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai, tree\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;whitespace\u0026quot; } } } }  Run the percolator:\n GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 5, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai tree\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;standard\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  As expected: only 1 percolator query matched our input.\nPhrases with Stopwords Say, we have a phrase \u0026quot;bonsai is tree\u0026quot; and we percolate text A new bonsai in tree in the office with the standard analyzer for indexing and english for search analyzer. There should be no matches. Let\u0026rsquo;s try:\nDELETE my-index PUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;english\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } } PUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai is tree\u0026quot; } } } } GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai in tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  And, surprisingly, this yields:\n{ \u0026quot;took\u0026quot; : 2, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai is tree\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] } } ] } }  We have a match! Also notice that the highlighter is broken!\nThe problem that these two analyzers have different stopword lists (no stopwords for standard and several English stopwords for english analyzer) and the phrase contains a stopword that is not shared between analyzers.\nLet\u0026rsquo;s fix this surprise with search_quote_analyzer.\nDELETE my-index PUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;english\u0026quot;, \u0026quot;search_quote_analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } } PUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai is tree\u0026quot; } } } } GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai in tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No hits, as expected.\nLet\u0026rsquo;s check if the expected behaviour is still there:\nGET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai is tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 4, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.39229375, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.39229375, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai is tree\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai is tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  Good. Even the highlighting works.\n","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"e3f59a2e678c8500ecbac9f71472142b","permalink":"https://www.jocas.lt/blog/post/percolator-phrase-analyzers/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/blog/post/percolator-phrase-analyzers/","section":"post","summary":"Example on how to use analyzers with the","tags":["elasticsearch","percolator"],"title":"Elasticsearch Percolator and Text Analyzers","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch","percolator"],"content":"If you google How can you match a long query text to a short text field? it will point you to the Stack Overflow page or here where the answer is to use Elasticsearch Percolator.\nMy search items are phrases meaning that it should match all terms in order. Let\u0026rsquo;s create a sample setup in Kibana (v7.5) Dev dashboard.\n Create an index for percolation:  PUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } }  Note on \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot;: this allows Fast Vector Highlighter to highlight combined phrase not just separate qeury terms.\nStore one phrase query:  PUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;bonsai tree\u0026quot; } } }  Percolate a document:  GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  Note on \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot;: this instructs Elasticsearch to use the Fast Vector Highlighter.\nThe query yields:\n{ \u0026quot;took\u0026quot; : 23, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : \u0026quot;bonsai tree\u0026quot; } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  As we see highlighter correctly marker the search phrase.\nStoring additional data with percolator queries Percolation result can be used to connect pieces of information in your system, e.g. store a subscriber_email attribute of the user that wants to be notified when the query matches along with the percolator query.\nPUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;bonsai tree\u0026quot; } }, \u0026quot;subscriber_email\u0026quot;: \u0026quot;subscriber_email@example.com\u0026quot; }  Then query:\nGET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This query yields:\n{ \u0026quot;took\u0026quot; : 10, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : \u0026quot;bonsai tree\u0026quot; } }, \u0026quot;subscriber_email\u0026quot; : \u0026quot;subscriber_email@example.com\u0026quot; }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  Now, take the email under the \u0026quot;subscriber_email\u0026quot; from the response and send an email with the highlight.\n","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"8e98378fbfbaafa1f97ba3a46fd35817","permalink":"https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/blog/post/es-percolator-phrase-highlight/","section":"post","summary":"Investigating the Elasticsearch percolator.","tags":["elasticsearch","search"],"title":"Phrase Highlighting with the Elasticsearch Percolator","type":"post"},{"authors":["Dainius Jocas"],"categories":["clojure","aws","lambda","devops"],"content":"I was writing a Clojure application and the plan was to deploy it as a AWS Lambda. The question I\u0026rsquo;m going to answer in this blog post is: how to build an uberjar for AWS Lambda with Uberdeps?\nTL;DR Add an alias to the deps.edn for uberjar building:\n{:aliases {:uberjar {:extra-deps {uberdeps {:mvn/version \u0026quot;0.1.6\u0026quot;}} :main-opts [\u0026quot;-m\u0026quot; \u0026quot;uberdeps.uberjar\u0026quot;]}}}  Create an executable file compile.clj in the project root folder:\ntouch compile.clj chmod +x compile.clj  Put this code in the compile.clj file:\n Run:\n(rm -rf classes \u0026amp;\u0026amp; \\ mkdir classes \u0026amp;\u0026amp; \\ ./compile.clj \u0026amp;\u0026amp; \\ clojure -A:uberjar --target target/UBERJAR_NAME.jar)  I\u0026rsquo;d advise put that last script into a Makefile ;)\n Introduction To deploy your Clojure code to AWS Lambda you need to package it as an uberjar. If your project is managed with deps.edn, basically you\u0026rsquo;re on your own to find a suitable library to package your code.\nFor some time to build uberjars for deps.edn projects I was using Cambada. It did the job but I was not entirely happy with the library for a couple of reasons:\n the library seems to be no longer maintained; it has various bugs with transitive Git dependencies. I\u0026rsquo;ve found out that these bugs are fixed in a fork of the Cambada and I used it as a git dependency.  Because building an uberjar for deps.edn boils down to just finding a library there is always temptation to try something new.\nEnter Uberdeps For my toy project I wanted to try out Uberdeps. The introduction blog post got me interested and I really liked the main idea:\n Takes deps.edn and packs an uberjar out of it.\n Sounds like exactly what I need.\nTrouble I\u0026rsquo;ve written my application, added all the things needed to deploy it as an AWS Lambda, build an uberjar with Uberdeps, deployed the app with the AWS CloudFormation, but when I\u0026rsquo;ve invoked the Lambda I\u0026rsquo;ve received an error:\n{ \u0026quot;message\u0026quot; : \u0026quot;Internal server error\u0026quot; }  After searching through the AWS CloudWatch logs I\u0026rsquo;ve found:\nClass not found: my.Lambda: java.lang.ClassNotFoundException java.lang.ClassNotFoundException: my.Lambda at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348)  The my.Lambda class was not found.\nAfter taking a look at the contents of the uberjar I\u0026rsquo;ve noticed that the my.Lambda class is indeed not inside the Uberjar. Ah, it seems that AOT (Ahead-of-Time) is not done out of the box. After searching and not finding a flag or some parameter that I need to pass to force the AOT compilation in the Uberdeps README, I\u0026rsquo;ve discovered an already closed pull request: the AOT compilation functionality is not implemented.\nI was in trouble.\nSolution The solution was to manually perform AOT compilation of the relevant namespaces right before building an uberjar and then instruct Uberdeps to put the resulting class files into the uberjar.\nTo do AOT compilation I\u0026rsquo;ve written a Clojure script compile.clj:\n Inspiration on how to write the script was taken from here and here.\nTo instruct Uberdeps to put class files to the uberjar I\u0026rsquo;ve added classes directory to the :paths vector in deps.edn.\nJust for the convenience, in the Makefile I\u0026rsquo;ve put commands for AOT compilation right before the command to build an uberjar:\nuberjar: rm -rf classes mkdir classes ./compile.clj clojure -A:uberjar --target target/my-jar-name.jar  And that is it! I have an uberjar with my.Lambda class and the AWS Lambda runtime is happy.\nDiscussion The solution is not bullet proof because:\n it assumes that the main deps.end file is called deps.edn; compiled classes are put in the classes directory; the alias for which namespaces should be AOT compiled is the default alias.  I hope that when a more generic solution will be needed either the Uberdeps will have an option for AOT compilatoin or I\u0026rsquo;ll be clever enough to deal with the situation and write a follow up blog post with the workaround.\n","date":1573776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573776000,"objectID":"bc08dd6e0a5cfada2a44f4a636a8db0f","permalink":"https://www.jocas.lt/blog/post/uberdeps-for-aws-lambda/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/blog/post/uberdeps-for-aws-lambda/","section":"post","summary":"A guide on how to build an Uberjar for AWS Lambda with `tonsky/uberdeps`","tags":["clojure","devops","aws","lambda"],"title":"Using Uberdeps to Build AWS Lambda Uberjar","type":"post"},{"authors":["Dainius Jocas"],"categories":["clojure","gitlab","ci","devops"],"content":"I want to share my hard-won lessons on how to setup the Gitlab CI for Clojure projects based on tools.deps. I think that the Gitlab CI is a wonderful tool for CI workloads. But when you\u0026rsquo;re going a bit sideways from the documented ways of doing things you have to do a bit of discovery for yourself.\nGitlab CI Cache Setup Usually I want to cache dependencies between all build and all branches. To achieve this I hard-code the cache key at the root of the .gitlab-ci.yml file e.g.:\ncache: key: one-key-to-rule-them-all  When it comes to caching Clojure dependencies we have to be aware that there different types of dependencies. Two most common ones are: Maven and gitlibs.\nThe Gitlab CI cache works only with directories inside the project directory. While local repositories (i.e. cache) for Clojure dependencies by default are stored outside the project directory (~/.m2 and ~/.gitlibs). Therefore, we have to provide parameters for our build tool to change the default directories for storing the dependencies.\nTo specify Maven local repository we can provide :mvn/local-repo parameter e.g.:\nclojure -Sdeps '{:mvn/local-repo \u0026quot;./.m2/repository\u0026quot;}' -A:test  Having configured local maven repository in our gitlab-ci.yml we can specify:\ncache: key: one-key-to-rule-them-all paths: - ./.m2/repository  When it comes to gitlibs there is no public API for changing the default directory in tools.deps. But the underlying tools.gitlibs uses an environment variable to set where to store the gitlibs conveniently named GITLIBS. E.g.\n$ (export GITLIBS=\u0026quot;.gitlibs/\u0026quot; \u0026amp;\u0026amp; clojure -A:test)  Of course, we should not forget to configure the cache:\ncache: key: one-key-to-rule-them-all paths: - ./.gitlibs  To use caching for both types of dependencies:\n(export GITLIBS=\u0026quot;.gitlibs/\u0026quot; \u0026amp;\u0026amp; clojure -Sdeps '{:mvn/local-repo \u0026quot;./.m2/repository\u0026quot;}' -A:test)  And setup the cache:\ncache: key: one-key-to-rule-them-all paths: - ./.m2/repository - ./.gitlibs  If you want to disable cache for a particular job (e.g. you\u0026rsquo;re linting with clj-kondo, which is delivered as a GraalVM compiled native image), just give an empty map for a job\u0026rsquo;s cache setup, e.g.:\nlint: stage: test image: borkdude/clj-kondo cache: {} when: always script: - clj-kondo --lint src test  I\u0026rsquo;ve used the Gitlab CI cache while working on a streaming-text search library Beagle. A full .gitlab-ci.yml file example of the setup can be found here.\nHope this helps!\n","date":1573430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573430400,"objectID":"10dfc9aba13fb40f8ec0be74224e4205","permalink":"https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/","publishdate":"2019-11-11T00:00:00Z","relpermalink":"/blog/post/gitlab-ci-clojure-dependencies/","section":"post","summary":"A guide on how to use Gitlab CI Cache for Clojure Dependencies","tags":["clojure","devops"],"title":"Using Gitlab CI Cache for Clojure Dependencies","type":"post"},{"authors":["Dainius Jocas"],"categories":null,"content":"  Clojure workflow @ TokenMill by Dainius Jocas  The source code of the demo project can be found here.\n","date":1559242800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559242800,"objectID":"86663215daf06a1fb32613517d99226f","permalink":"https://www.jocas.lt/blog/talk/vilnius-clojure-meetup/","publishdate":"2019-11-11T19:00:00Z","relpermalink":"/blog/talk/vilnius-clojure-meetup/","section":"talk","summary":"Sharing Joy of Clojure Programming","tags":["meetup","clojure","lambda","aws","graalvm"],"title":"Clojure Workflow @ TokenMill","type":"talk"},{"authors":null,"categories":null,"content":"Lucene-Grep a.k.a. lmgrep  whoami { \u0026quot;name\u0026quot;: \u0026quot;Dainius Jocas\u0026quot;, \u0026quot;company\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;Vinted\u0026quot;, \u0026quot;mission\u0026quot;: \u0026quot;Make second-hand the first choice worldwide\u0026quot; }, \u0026quot;role\u0026quot;: \u0026quot;Staff Engineer\u0026quot;, \u0026quot;website\u0026quot;: \u0026quot;https://www.jocas.lt\u0026quot;, \u0026quot;twitter\u0026quot;: \u0026quot;@dainius_jocas\u0026quot;, \u0026quot;github\u0026quot;: \u0026quot;dainiusjocas\u0026quot;, \u0026quot;author_of_oss\u0026quot;: [\u0026quot;lucene-grep\u0026quot;, \u0026quot;ket\u0026quot;] }   Agenda  Intro Whats inside Lucene-Grep? Use cases Future work Discussion   Intro  lmgrep is a CLI full-text search tool Interface is similar to grep Based on Lucene Lucene Monitor library is the main building block Compiled with the GraalVM native-image Single binary file, no external dependencies Supports Linux, MacOS, Windows   Origin  Used Elasticsearch Percolator for some basic named entity recognition (NER) Needed to deploy to AWS Lambda, Elasticsearch was not an option However, I really liked the idea of expressing entities as full-text queries Found the Luwak library, deployed on AWS Lambda, however it ran on JVM Gunnar Morling blog post about GraalVM native-image Lucene on AWS Lambda Convinced Red Hat devs to open source and release quarkiverse/quarkus-lucene Hacked Lucene Grep   grep vs lmgrep echo \u0026quot;Lucene is awesome\u0026quot; | grep Lucene  echo \u0026quot;Lucene is awesome\u0026quot; | lmgrep Lucene   Installing the lmgrep brew or a shell script on Linux\nwget https://github.com/dainiusjocas/lucene-grep/releases/download/v2021.05.23/lmgrep-v2021.05.23-linux-static-amd64.zip unzip lmgrep-v2021.05.23-linux-static-amd64.zip mv lmgrep /usr/local/bin  brew on MacOS\nbrew install dainiusjocas/brew/lmgrep  scoop on Windows\nscoop bucket add scoop-clojure https://github.com/littleli/scoop-clojure scoop bucket add extras scoop install lmgrep   Whats inside?  Reading from file(s) Searching for files with GLOB, e.g. \u0026lsquo;**/*.txt' Reading from STDIN Writing to STDOUT in various formats, e.g. JSON Text analysis pipeline Multiple query parsers Text tokenization with **--only-analyze**flag Loading multiple queries from a file Full-text search lmgrep -h for the full list of available options   Text Analysis  The same good ol\u0026rsquo; lucene text analysis 45 predefined analyzers available, e.g. LithuanianAnalyzer 5 character filters 14 tokenizers 113 token filters However, not everything that Lucene provides is available in lmgrep because of limitations of the GraalVM native-image https://github.com/dainiusjocas/lucene-grep/blob/main/docs/analysis-components.md   Custom Text Analysis Issue  At first exposed several CLI flags for text analysis  a problem with order of execution   Lucene analyzers are Java classes For a CLI tool, exposing Java classes is not a good option Something similar to Elasticsearch analysis syntax is needed   Text Analysis Definition { \u0026quot;char-filters\u0026quot;: [ {\u0026quot;name\u0026quot;: \u0026quot;htmlStrip\u0026quot;}, { \u0026quot;name\u0026quot;: \u0026quot;patternReplace\u0026quot;, \u0026quot;args\u0026quot;: { \u0026quot;pattern\u0026quot;: \u0026quot;foo\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;bar\u0026quot; } } ], \u0026quot;tokenizer\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;standard\u0026quot;}, \u0026quot;token-filters\u0026quot;: [ {\u0026quot;name\u0026quot;: \u0026quot;englishMinimalStem\u0026quot;}, {\u0026quot;name\u0026quot;: \u0026quot;uppercase\u0026quot;} ] }   Various Query Parsers --query-parser  --query-parser=classic  The default one When googling for the Lucene query syntax, the first hit  echo \u0026quot;Lucene is awesome\u0026quot; | lmgrep --query-parser=classic \u0026quot;lucene is aweso~\u0026quot;  echo \u0026quot;Lucene is awesome\u0026quot; | lmgrep --query-parser=classic \u0026quot;\\\u0026quot;lucene is\\\u0026quot;\u0026quot;   --query-parser=complex-phrase  similar to the classic query parser but phrase queries are more expressive  echo \u0026quot;jonathann jon peterson\u0026quot; | lmgrep --query-parser=complex-phrase \u0026quot;\\\u0026quot;(john jon jonathan~) peters*\\\u0026quot;\u0026quot;   --query-parser=simple  similar to the classic query parser BUT any errors in the query syntax will be ignored and the parser will attempt to decipher what it can E.g. given term1\\* searches for the term term1* Probably should be the default query parser in lmgrep   --query-parser=standard  Implementation of the Lucene classic query parser using the flexible query parser frameworks There must be a reason why it comes with the default lucene dependency   --query-parser=surround  Constructs span queries that use positional information  echo \u0026quot;Lucene is awesome\u0026quot; | lmgrep --query-parser=surround \u0026quot;2W(lucene, awesome)\u0026quot;   if the term order is NOT important: W-\u0026gt;N  echo \u0026quot;Lucene is awesome\u0026quot; | lmgrep --query-parser=surround \u0026quot;2N(awesome, lucene)\u0026quot;   WARNING: query terms are not analyzed   --only-analyze  Just apply the text analyzer on the input text and output the list(s) of tokens   --only-analyze: basic example echo \u0026quot;Lucene is awesome\u0026quot; | lmgrep --only-analyze   --only-analyze: custom text analysis pipeline echo \u0026quot;\u0026lt;p\u0026gt;foo bars baz\u0026lt;/p\u0026gt;\u0026quot; | lmgrep --only-analyze --analysis=' { \u0026quot;char-filters\u0026quot;: [ {\u0026quot;name\u0026quot;: \u0026quot;htmlStrip\u0026quot;}, { \u0026quot;name\u0026quot;: \u0026quot;patternReplace\u0026quot;, \u0026quot;args\u0026quot;: { \u0026quot;pattern\u0026quot;: \u0026quot;foo\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;bar\u0026quot; } } ], \u0026quot;tokenizer\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;standard\u0026quot;}, \u0026quot;token-filters\u0026quot;: [ {\u0026quot;name\u0026quot;: \u0026quot;englishMinimalStem\u0026quot;}, {\u0026quot;name\u0026quot;: \u0026quot;uppercase\u0026quot;} ] } '  [\u0026quot;BAR\u0026quot;,\u0026quot;BAR\u0026quot;,\u0026quot;BAZ\u0026quot;]   --only-analyze with --explain echo \u0026quot;Dogs and CAt\u0026quot; | lmgrep --only-analyze --explain | jq  [ { \u0026quot;token\u0026quot;: \u0026quot;dog\u0026quot;, \u0026quot;position\u0026quot;: 0, \u0026quot;positionLength\u0026quot;: 1, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;end_offset\u0026quot;: 4, \u0026quot;start_offset\u0026quot;: 0 }, { \u0026quot;end_offset\u0026quot;: 8, \u0026quot;positionLength\u0026quot;: 1, \u0026quot;position\u0026quot;: 1, \u0026quot;start_offset\u0026quot;: 5, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;token\u0026quot;: \u0026quot;and\u0026quot; }, { \u0026quot;position\u0026quot;: 2, \u0026quot;token\u0026quot;: \u0026quot;cat\u0026quot;, \u0026quot;positionLength\u0026quot;: 1, \u0026quot;end_offset\u0026quot;: 12, \u0026quot;type\u0026quot;: \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;start_offset\u0026quot;: 9 } ]   The idea is similar to the Elasticsearch\u0026rsquo;s _analyze API No need to recreate an index on every custom analyzer change   --only-analyze: output for graphviz  TODO   Loading queries from a file echo \u0026quot;I have two dogs\u0026quot; | lmgrep --queries-file=dog-lovers.json  [ { \u0026quot;id\u0026quot;: \u0026quot;german_language\u0026quot;, \u0026quot;query\u0026quot;: \u0026quot;hund\u0026quot;, \u0026quot;stemmer\u0026quot;: \u0026quot;german\u0026quot; }, { \u0026quot;id\u0026quot;: \u0026quot;english_language\u0026quot;, \u0026quot;query\u0026quot;: \u0026quot;dog\u0026quot;, \u0026quot;stemmer\u0026quot;: \u0026quot;english\u0026quot; } ]   load all queries once 100K queries takes about 1s to load on my laptop   Full-text search mkdir demo cd demo echo \u0026quot;Lucene is awesome\u0026quot; \u0026gt; lucene.txt echo \u0026quot;Grep is awesome\u0026quot; \u0026gt; grep.txt lmgrep lucene **.txt   Full-text File Search with Score cd mkdir full-text-search || true cd full-text-search echo \u0026quot;Lucene is awesome\u0026quot; \u0026gt; lucene.txt echo \u0026quot;Lucene Grep is build on Lucene Monitor library\u0026quot; \u0026gt; lucene-grep.txt lmgrep \u0026quot;Lucene\u0026quot; '**.txt' --no-split --with-score --format=json | jq -s -c 'sort_by(.score)[]' | tac | head -3 | jq   Source Code Search  Specify a custom analyzer for you programming language E.g. WordDelimiterGraphFilter that \u0026ldquo;MyFooClass\u0026rdquo; =\u0026gt; [\u0026ldquo;My\u0026rdquo;, \u0026ldquo;Foo\u0026rdquo;, \u0026ldquo;Class\u0026rdquo;] Enable scoring Output hyperlinks in a (supported) terminal emulator to the specific line number   Alternative to Elasticsearch Percolator  Start a lmgrep with open STDIN, STDOUT, and STDERR pipes for inter-process communication  require 'open3' @stdin, @stdout, @stderr, @wait_thr = Open3.popen3(\u0026quot;lmgrep lucene\u0026quot;) @stdin.puts \u0026quot;Lucene is awesome\u0026quot; @stdout.gets   https://github.com/dainiusjocas/lucene-grep/tree/main/examples/ruby-percolator   Future work  Your issues https://github.com/dainiusjocas/lucene-grep/issues Machanism for shared analysis components  now only inlined text analysis config is supported   LMGREP_HOME for keeping all the resources in one place Release analyzer construction code as a standalone library Melt your CPU  Use all CPU cores to the max for as short as possible Do not preserve the input order   Optimize --with-scored-highlights option  Sort output by score   Analysis components with inlined data  E.g. inlines stopwords list, not a file     Discussion This notebook was exported from https://nextjournal.com/a/P3v43aPLhVdSZ3BS8NaX3?change-id=CxhWFVu6LhGvq85956D1fu {:article {:settings {:numbered? false}, :nodes {\u0026quot;00d83f4f-db54-4f94-9c55-c188c0f11cb7\u0026quot; {:id \u0026quot;00d83f4f-db54-4f94-9c55-c188c0f11cb7\u0026quot;, :kind \u0026quot;code\u0026quot;, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;09fccaa9-72f6-490d-8e2d-df19048a611d\u0026quot; {:compute-ref #uuid \u0026quot;f5c9e0e1-e2b7-472f-8af7-c54f043dcb35\u0026quot;, :exec-duration 758, :id \u0026quot;09fccaa9-72f6-490d-8e2d-df19048a611d\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;0d2cde56-d973-4049-ba70-25f56c4c1df7\u0026quot; {:compute-ref #uuid \u0026quot;a22031b2-8c74-49f3-b240-f57854eeb5f2\u0026quot;, :exec-duration 4615, :id \u0026quot;0d2cde56-d973-4049-ba70-25f56c4c1df7\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 436}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;], :stdout-collapsed? true}, \u0026quot;17899a72-33de-491b-901d-e84dc92c9ab9\u0026quot; {:compute-ref #uuid \u0026quot;e3f64704-22b2-41b6-9e86-2a50792e62e8\u0026quot;, :exec-duration 881, :id \u0026quot;17899a72-33de-491b-901d-e84dc92c9ab9\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;2342aefa-edd9-4274-8aee-fa229d27e5eb\u0026quot; {:compute-ref #uuid \u0026quot;68f5460c-5c3d-422a-9a65-2ea459c4566c\u0026quot;, :exec-duration 901, :id \u0026quot;2342aefa-edd9-4274-8aee-fa229d27e5eb\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;30c23b70-7b03-4ed3-8fe4-c3d7aac6c7fa\u0026quot; {:compute-ref #uuid \u0026quot;12efa625-d503-4984-8bd6-79536e1d2c1c\u0026quot;, :exec-duration 799, :id \u0026quot;30c23b70-7b03-4ed3-8fe4-c3d7aac6c7fa\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 27}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;], :stdout-collapsed? true}, \u0026quot;3109d090-1872-4788-ae47-10aed96fd6d7\u0026quot; {:id \u0026quot;3109d090-1872-4788-ae47-10aed96fd6d7\u0026quot;, :kind \u0026quot;code-listing\u0026quot;}, \u0026quot;3da7f5f5-c87b-4ccf-84d2-0486291a2a4d\u0026quot; {:compute-ref #uuid \u0026quot;75ece01b-2eff-4ce7-aa94-7a213cd6cbf8\u0026quot;, :exec-duration 974, :id \u0026quot;3da7f5f5-c87b-4ccf-84d2-0486291a2a4d\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 14}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;400756d4-7904-4fbb-916f-6db143d4fd0f\u0026quot; {:id \u0026quot;400756d4-7904-4fbb-916f-6db143d4fd0f\u0026quot;, :kind \u0026quot;code-listing\u0026quot;}, \u0026quot;4181e7ac-8b90-422d-b65e-de8ecb110259\u0026quot; {:id \u0026quot;4181e7ac-8b90-422d-b65e-de8ecb110259\u0026quot;, :kind \u0026quot;code-listing\u0026quot;}, \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot; {:environment [:environment {:article/nextjournal.id #uuid \u0026quot;5b45dad0-dfdf-4576-9b8c-f90892e74c94\u0026quot;, :change/nextjournal.id #uuid \u0026quot;5df5da3f-c83a-4296-bc41-0e6e394499d4\u0026quot;, :node/id \u0026quot;dab15041-47f1-4ca7-84e2-b4532a4a2f70\u0026quot;}], :id \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;, :kind \u0026quot;runtime\u0026quot;, :language \u0026quot;bash\u0026quot;, :type :nextjournal}, \u0026quot;51a22eb8-4ffd-4e47-8f0e-05ffd18eee2a\u0026quot; {:id \u0026quot;51a22eb8-4ffd-4e47-8f0e-05ffd18eee2a\u0026quot;, :kind \u0026quot;code-listing\u0026quot;}, \u0026quot;6011bd03-c09a-46f6-b664-3b0fd31946e4\u0026quot; {:compute-ref #uuid \u0026quot;1db28be9-449b-417d-b8e4-0b5457056aff\u0026quot;, :exec-duration 1040, :id \u0026quot;6011bd03-c09a-46f6-b664-3b0fd31946e4\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;790cefed-de1e-49fa-85b9-a0151bb9cc6b\u0026quot; {:compute-ref #uuid \u0026quot;a2a52cb4-c860-4b68-b2f9-ff45e568d2f1\u0026quot;, :exec-duration 1069, :id \u0026quot;790cefed-de1e-49fa-85b9-a0151bb9cc6b\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;9ac214c9-b5e9-47b9-8c86-238756e4b33b\u0026quot; {:compute-ref #uuid \u0026quot;d79de0c9-05d6-44d9-9bd8-09e35d3216b8\u0026quot;, :exec-duration 992, :id \u0026quot;9ac214c9-b5e9-47b9-8c86-238756e4b33b\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;9e3a426c-5354-4378-9cd9-a079f7760c23\u0026quot; {:id \u0026quot;9e3a426c-5354-4378-9cd9-a079f7760c23\u0026quot;, :kind \u0026quot;file\u0026quot;}, \u0026quot;af8eba1d-7df2-4260-b96e-ddb022900283\u0026quot; {:compute-ref #uuid \u0026quot;27541092-284e-4136-afd0-046259eb264c\u0026quot;, :exec-duration 2964, :id \u0026quot;af8eba1d-7df2-4260-b96e-ddb022900283\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;], :stdout-collapsed? true}, \u0026quot;b40c55d8-05c3-4a0e-bf77-6f65b8147318\u0026quot; {:compute-ref #uuid \u0026quot;df503be9-fc5a-478e-bf3f-623015c2bf4c\u0026quot;, :exec-duration 1129, :id \u0026quot;b40c55d8-05c3-4a0e-bf77-6f65b8147318\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;cd1d1a54-be1e-4a32-baa4-4d6495c13572\u0026quot; {:compute-ref #uuid \u0026quot;ef429098-d2fe-4484-a681-fdefabacec7f\u0026quot;, :exec-duration 980, :id \u0026quot;cd1d1a54-be1e-4a32-baa4-4d6495c13572\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;], :stdout-collapsed? true}, \u0026quot;d0d84e54-a3b0-4437-8090-b8886b9585ea\u0026quot; {:compute-ref #uuid \u0026quot;d6e6fc75-6f70-4443-8a70-09304cb6180d\u0026quot;, :exec-duration 1100, :id \u0026quot;d0d84e54-a3b0-4437-8090-b8886b9585ea\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;ecb5068e-2d50-4696-ba83-755ef6d198c1\u0026quot; {:id \u0026quot;ecb5068e-2d50-4696-ba83-755ef6d198c1\u0026quot;, :kind \u0026quot;code-listing\u0026quot;}, \u0026quot;ed867748-ecb1-4528-ab4c-05fc6a442561\u0026quot; {:id \u0026quot;ed867748-ecb1-4528-ab4c-05fc6a442561\u0026quot;, :kind \u0026quot;code-listing\u0026quot;}, \u0026quot;f9c1c889-6a12-4a11-8781-319ee9b69a38\u0026quot; {:compute-ref #uuid \u0026quot;f3f310ba-317e-456b-802b-3b089c2553a5\u0026quot;, :exec-duration 811, :id \u0026quot;f9c1c889-6a12-4a11-8781-319ee9b69a38\u0026quot;, :kind \u0026quot;code\u0026quot;, :output-log-lines {:stdout 2}, :runtime [:runtime \u0026quot;4534e627-a3df-4694-97c9-39b3fbbe9f90\u0026quot;]}, \u0026quot;fa294f41-8259-4938-a8ed-a2a12a6cb4cf\u0026quot; {:id \u0026quot;fa294f41-8259-4938-a8ed-a2a12a6cb4cf\u0026quot;, :kind \u0026quot;code-listing\u0026quot;}}, :nextjournal/id #uuid \u0026quot;031428c0-72a8-44c8-a5aa-10fba8f8e34a\u0026quot;, :article/change {:nextjournal/id #uuid \u0026quot;60db64ef-6234-4eef-bec7-a9f972100980\u0026quot;}}}   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18a7dadbdac94cc7278c815587dc7c5c","permalink":"https://www.jocas.lt/blog/slides/london-information-retrieval-meetup-2021-06/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blog/slides/london-information-retrieval-meetup-2021-06/","section":"slides","summary":"Lucene-Grep a.k.a. lmgrep  whoami { \u0026quot;name\u0026quot;: \u0026quot;Dainius Jocas\u0026quot;, \u0026quot;company\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;Vinted\u0026quot;, \u0026quot;mission\u0026quot;: \u0026quot;Make second-hand the first choice worldwide\u0026quot; }, \u0026quot;role\u0026quot;: \u0026quot;Staff Engineer\u0026quot;, \u0026quot;website\u0026quot;: \u0026quot;https://www.jocas.lt\u0026quot;, \u0026quot;twitter\u0026quot;: \u0026quot;@dainius_jocas\u0026quot;, \u0026quot;github\u0026quot;: \u0026quot;dainiusjocas\u0026quot;, \u0026quot;author_of_oss\u0026quot;: [\u0026quot;lucene-grep\u0026quot;, \u0026quot;ket\u0026quot;] }   Agenda  Intro Whats inside Lucene-Grep?","tags":null,"title":"","type":"slides"}]