<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | Dainius Jocas</title><link>https://www.jocas.lt/blog/post/</link><atom:link href="https://www.jocas.lt/blog/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2023 Dainius Jocas</copyright><lastBuildDate>Sun, 29 Jan 2023 00:00:00 +0000</lastBuildDate><image><url>img/map[gravatar:%!s(bool=false) shape:circle]</url><title>Posts</title><link>https://www.jocas.lt/blog/post/</link></image><item><title>Match the Entire Text with a Slop in Elasticsearch</title><link>https://www.jocas.lt/blog/post/elasticsearch-full-phrase-with-slop/</link><pubDate>Sun, 29 Jan 2023 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/elasticsearch-full-phrase-with-slop/</guid><description>&lt;h2 id="intro">Intro&lt;/h2>
&lt;p>Recently I&amp;rsquo;ve done a little Elasticsearch-based demo where there was one requirement I believe is worth sharing: the entire text should match even if two words are swapped.
This post is divided into two parts: requirements for a demo, and several implementation options.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>A simplified version of relevant requirements goes something like this:&lt;/p>
&lt;ol>
&lt;li>Documents are chat app message bodies,&lt;/li>
&lt;li>Match a message body with other messages even if any two words were swapped.&lt;/li>
&lt;/ol>
&lt;p>E.g. a message &lt;code>this is my message&lt;/code> should match &lt;code>is this my message&lt;/code> and vice versa.&lt;/p>
&lt;p>At first, this looks like nothing fancy, just set up an &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-analyzers.html">analyzer&lt;/a> with the required &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html%60">token filters&lt;/a> and do a &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query-phrase.html">phrase_match query&lt;/a> with a &lt;code>slop=2&lt;/code>.&lt;/p>
&lt;p>But then comes the tricky bit: we also need to take into account the &lt;strong>length of the message&lt;/strong>.
This is because a &lt;code>phrase_match&lt;/code> with a short message will match longer messages.
E.g. We don&amp;rsquo;t want that the query &lt;code>is this my message&lt;/code> would match &lt;code>this is my message which is longer&lt;/code>.&lt;/p>
&lt;h2 id="implementation-strategies">Implementation strategies&lt;/h2>
&lt;p>We&amp;rsquo;ll discuss two implementation strategies: fingerprinting based on the token count and a clever trick leveraging the text analysis pipeline.&lt;/p>
&lt;h3 id="fingerprint">Fingerprint&lt;/h3>
&lt;p>An instinctive approach is to &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/fingerprint-processor.html">fingerprint&lt;/a> the message text.
The simplest fingerprint could just be the count of tokens.
Elasticsearch conveniently offers a &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/token-count.html">token count field type&lt;/a>.&lt;/p>
&lt;p>One serious downside of this approach is that at the query time we&amp;rsquo;d have to get the count of tokens of the query.
This means either one additional round-trip to Elasticsearch to get the token count, e.g. using the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html">&lt;code>_analyze&lt;/code> API&lt;/a> or use of a &lt;code>script&lt;/code> in the query.
Let&amp;rsquo;s rule out the round trip to the &lt;code>_analyze&lt;/code> API approach for the demo.
Then how to implement the &lt;code>script&lt;/code> to get query token count?
The &lt;a href="https://chat.openai.com/chat#">Chat GPT&lt;/a> gave me a hint at a possible &lt;code>script&lt;/code> based solution :)&lt;/p>
&lt;p>&lt;img src="query-token-count.png" alt="Script">&lt;/p>
&lt;p>A significant problem with the above approach is that the text is split by a whitespace character (or something that &lt;a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Pattern.html#split%2Djava.lang.CharSequence%2D">&lt;code>split&lt;/code>&lt;/a> accepts).
Any seasoned search engineer immediately notices that the text is not tokenized by the same analyzer that was used to tokenize the text during the index-time for the token counting.
Of, course we could set up both &lt;code>tokenizers&lt;/code> to work the same way, but that approach seems somewhat fragile because part of the logic is in the index mapping and the other part is in the script that lives in the query constructor which is probably inside your application.&lt;/p>
&lt;h3 id="clever-text-analysis">Clever Text Analysis&lt;/h3>
&lt;p>We could also be more clever about how to ensure that the length of the matched texts is equal.
My strategy is to require both conditions to be true:&lt;/p>
&lt;ol>
&lt;li>Match the text &lt;strong>from the beginning of the string&lt;/strong> with a &lt;code>phrase_match&lt;/code> query with a slop=2.&lt;/li>
&lt;li>Match the &lt;strong>reversed&lt;/strong> text from the beginning of the string with a &lt;code>phrase_match&lt;/code> query with a slop=2.&lt;/li>
&lt;/ol>
&lt;p>The strategy requires us to solve 2 puzzles:&lt;/p>
&lt;ol>
&lt;li>How to match exactly from the beginning of the text?&lt;/li>
&lt;li>How to reverse the text?&lt;/li>
&lt;/ol>
&lt;p>To ensure the matching from the beginning we could insert a synthetic &lt;code>PREFIX&lt;/code> token at the position 0.&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/p>
&lt;p>For the text reversal the idea is to:&lt;/p>
&lt;ol>
&lt;li>Use the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-keyword-tokenizer.html">keyword tokenizer&lt;/a>&lt;/li>
&lt;li>Use the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-reverse-tokenfilter.html">reverse&lt;/a> token filter on the entire text,&lt;/li>
&lt;li>Split the text into tokens using the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-word-delimiter-graph-tokenfilter.html#analysis-word-delimiter-graph-tokenfilter">word delimiter graph token filter&lt;/a>.&lt;/li>
&lt;li>Add a synthetic token at the position 0.&lt;/li>
&lt;/ol>
&lt;p>Let&amp;rsquo;s work out an example from requirements: &lt;code>this is my message&lt;/code> should match &lt;code>is this my message&lt;/code> but should not match &lt;code>this is my message which is longer&lt;/code>.&lt;/p>
&lt;p>The &lt;code>messages&lt;/code> index config&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;settings&amp;quot;: {
&amp;quot;number_of_replicas&amp;quot;: 0,
&amp;quot;number_of_shards&amp;quot;: 1,
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;normal_direction_analyzer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;token_splitter&amp;quot;,
&amp;quot;prefixer&amp;quot;
]
},
&amp;quot;backwards_direction_analyzer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;reverse&amp;quot;,
&amp;quot;token_splitter&amp;quot;,
&amp;quot;prefixer&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;token_splitter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;word_delimiter_graph&amp;quot;,
&amp;quot;split_on_case_change&amp;quot;: false,
&amp;quot;split_on_numerics&amp;quot;: false
},
&amp;quot;inject_prefix&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;^(.*)$&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;PREFIX $1&amp;quot;
},
&amp;quot;prefixer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;condition&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;inject_prefix&amp;quot;,
&amp;quot;token_splitter&amp;quot;
],
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.getPosition() == 0&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;body&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;normal_direction&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;normal_direction_analyzer&amp;quot;
},
&amp;quot;backwards_direction&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;backwards_direction_analyzer&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s index 2 documents:&lt;/p>
&lt;pre>&lt;code class="language-text">POST _bulk?refresh=true
{ &amp;quot;index&amp;quot; : { &amp;quot;_index&amp;quot; : &amp;quot;messages&amp;quot;, &amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot; } }
{ &amp;quot;body&amp;quot;: &amp;quot;this is my message&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_index&amp;quot; : &amp;quot;messages&amp;quot;, &amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot; } }
{ &amp;quot;body&amp;quot;: &amp;quot;this is my message which is longer&amp;quot; }
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s query the &lt;code>messages&lt;/code> index:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;query&amp;quot;: {
&amp;quot;bool&amp;quot;: {
&amp;quot;must&amp;quot;: [
{
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;body.normal_direction&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;is this my message&amp;quot;,
&amp;quot;slop&amp;quot;: 2
}
}
},
{
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;body.backwards_direction&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;is this my message&amp;quot;,
&amp;quot;slop&amp;quot;: 2
}
}
}
],
&amp;quot;_name&amp;quot;: &amp;quot;full phrase with accounted length&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The response:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot;: 2,
&amp;quot;timed_out&amp;quot;: false,
&amp;quot;_shards&amp;quot;: {
&amp;quot;total&amp;quot;: 1,
&amp;quot;successful&amp;quot;: 1,
&amp;quot;skipped&amp;quot;: 0,
&amp;quot;failed&amp;quot;: 0
},
&amp;quot;hits&amp;quot;: {
&amp;quot;total&amp;quot;: {
&amp;quot;value&amp;quot;: 1,
&amp;quot;relation&amp;quot;: &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot;: 0.46947598,
&amp;quot;hits&amp;quot;: [
{
&amp;quot;_index&amp;quot;: &amp;quot;messages&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 0.46947598,
&amp;quot;_source&amp;quot;: {
&amp;quot;body&amp;quot;: &amp;quot;this is my message&amp;quot;
},
&amp;quot;matched_queries&amp;quot;: [
&amp;quot;full phrase with accounted length&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The response is exactly as expected: it does match the shorter but not the longer message body.&lt;/p>
&lt;p>Downsides of the approach:&lt;/p>
&lt;ul>
&lt;li>The text is indexed 2 times. But it is acceptable to index and analyze the text in multiple ways, e.g. stemming.&lt;/li>
&lt;li>Analyzers got somewhat complicated. But the complicated bit is isolated only for dealing with the first token. Also, a couple of examples with the &lt;code>_analyze&lt;/code> API should make the analysis pipeline understandable.&lt;/li>
&lt;li>The total text length is limited to the &lt;code>Integer.MAX_VALUE&lt;/code> which is 2147483647 characters. But ~2 GB per message should be enough.&lt;/li>
&lt;li>&amp;ldquo;Tokenization&amp;rdquo; is done with the &lt;code>word_delimiter_graph&lt;/code>. But it is a standard &lt;a href="https://lucene.apache.org">Lucene&lt;/a> feature that you should learn anyway.&lt;/li>
&lt;li>Also, all the gotchas of the &lt;code>slop&lt;/code> are relevant, e.g. if one token was dropped/added from/to the query, then there still would be a match.&lt;/li>
&lt;li>I&amp;rsquo;d suggest you also should take extra care in handling the non-alphanumeric symbols.&lt;/li>
&lt;li>Also, the setup might produce surprising results for shorter (e.g. 1-5 words) strings.&lt;/li>
&lt;/ul>
&lt;p>A nice thing is that this solution is contained within the text analysis pipeline: no ingest pipelines, no scripting in queries, etc.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>In this post we&amp;rsquo;ve defined a problem of matching entire text with some flexibility in terms of token position changes.
We&amp;rsquo;ve discussed 2 approaches: fingerprinting and leveraging text analysis pipeline to account for the text length.
IMO, both approaches are somewhat hacky and have their downsides.
I&amp;rsquo;ve picked to work out the text analysis approach, and I&amp;rsquo;ve got my demo done.&lt;/p>
&lt;p>Let me know how you would approach this problem in the comments below.&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Of course, I&amp;rsquo;ve been thinking about leveraging &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-span-first-query.html">&lt;code>span_first&lt;/code>&lt;/a> query, but it requires another inner term level span query.
Which would make the query construction complicated because tokenization should be done inside your application. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>lowercasing and ascii-folding are omitted for brevity. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Tricks with Elasticsearch Completions Suggesters</title><link>https://www.jocas.lt/blog/post/tricks-with-elasticsearch-completion-suggesters/</link><pubDate>Tue, 13 Sep 2022 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/tricks-with-elasticsearch-completion-suggesters/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>Elasticsearch text analyzers can supercharge search suggesters.&lt;/p>
&lt;h2>Table of Contents&lt;/h2>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>&lt;a href="#tldr">TL;DR&lt;/a>&lt;/li>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#requirements">Requirements&lt;/a>&lt;/li>
&lt;li>&lt;a href="#implementation">Implementation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#baseline-suggestions-setup">Baseline suggestions setup&lt;/a>&lt;/li>
&lt;li>&lt;a href="#suggest-from-the-second-word">Suggest from the second word&lt;/a>&lt;/li>
&lt;li>&lt;a href="#suggest-from-the-second-entity">Suggest from the second entity&lt;/a>&lt;/li>
&lt;li>&lt;a href="#suggest-from-the-last-word">Suggest from the last word&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bonus-1-synonyms-for-suggestions">Bonus 1: Synonyms for suggestions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bonus-2-shingles-for-suggestions">Bonus 2: shingles for suggestions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#fin">Fin&lt;/a>&lt;/li>
&lt;li>&lt;a href="#footnotes">Footnotes&lt;/a>&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>So, you are a &lt;a href="https://opensourceconnections.com/blog/2020/07/16/what-is-a-relevance-engineer/">search engineer&lt;/a> that happily uses &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html#completion-suggester">Elasticsearch Completion Suggester&lt;/a> feature: lightning speed prefix suggestions works just like a charm&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>But one day the product manager comes to you with a requirement: &lt;code>could we also suggest if users start typing a word from the middle of the suggested string?&lt;/code>. Of course, the deadline is yesterday, as always. On top he adds that he doesn&amp;rsquo;t care whether that makes sense from search engineers perspective, it must be done&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. You try to argue that it will take forever to change upstream indexing pipeline because it is owned by another department of your company. PM doesn&amp;rsquo;t blink.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>Your app suggests artist names. The problematic artist currently rocking in the charts and attracting a lot of attention is &lt;code>Britney Spears &amp;amp; Elton John&lt;/code>. The PM specifies that it is needed that artists name and surname should be the source of suggestions. This means and that all &lt;code>b&lt;/code>, &lt;code>s&lt;/code>, &lt;code>e&lt;/code>, &lt;code>j&lt;/code> should suggest that artist.&lt;/p>
&lt;p>The actual song
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8hLtlzkoGPk" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
.&lt;/p>
&lt;h2 id="implementation">Implementation&lt;/h2>
&lt;p>Generating multiple strings upstream is not an option due to the time constraints (because in a week the song will not be in the charts anymore). Also, the team discards generation of multiple strings using an &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest-processors.html">ingest processor&lt;/a> because nobody in our team likes to work with them. In the anemic Elasticsearch &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters.html#_parameters_for_completion_fields_2">documentation&lt;/a> there is a hint that using text analyzers (stopwords are mentioned) it is possible to achieve different entry points for suggestions which sounds like the trick that might work for our case. You decide to go to the rabbit hole of the analyzers.&lt;/p>
&lt;p>After the technical refinement meeting the notes on implementation of use cases look like:&lt;/p>
&lt;ul>
&lt;li>&lt;code>b&lt;/code> -&amp;gt; the classic use case which already works;&lt;/li>
&lt;li>&lt;code>s&lt;/code> -&amp;gt; from the second word; trick is to use a different analyser at the search time;&lt;/li>
&lt;li>&lt;code>e&lt;/code> -&amp;gt; from the second entity;&lt;/li>
&lt;li>&lt;code>j&lt;/code> -&amp;gt; the last word.&lt;/li>
&lt;/ul>
&lt;p>Cool, it&amp;rsquo;s time to open the Kibana dev tools and hack. All examples are worked out and tested with version &lt;code>8.4.1&lt;/code>.&lt;/p>
&lt;h3 id="baseline-suggestions-setup">Baseline suggestions setup&lt;/h3>
&lt;p>Nothing fancy here, copy-paste from the documentation verbatim, for the sake of completeness.&lt;/p>
&lt;pre>&lt;code class="language-text">DELETE music
PUT music
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;b&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>And it returns:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;b&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="suggest-from-the-second-word">Suggest from the second word&lt;/h3>
&lt;p>The idea here is to have a subfield for suggestions that uses different analyzers for indexing and searching. For indexing we want an analyzer that drops the first token. For search time analyzer we want to use a standard analyzers because we should not drop first token from the search string. Do not forget to set &lt;code>preserve_position_increments&lt;/code> as &lt;code>false&lt;/code> for the new field.&lt;/p>
&lt;pre>&lt;code class="language-text">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;remove_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position != 0&amp;quot;
}
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;drop_first_word&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;from_second_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;drop_first_word&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;preserve_position_increments&amp;quot;: false
}
}
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;s&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest.from_second_word&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>It returns:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;s&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Great start!&lt;/p>
&lt;p>What about starting from the 3rd and 4th word? Could we just create a new subfield and change the script for the &lt;code>predicate_token_filter&lt;/code> from &lt;code>token.position != 0&lt;/code> to &lt;code>token.position != 0 &amp;amp;&amp;amp; token.position != 1&lt;/code> and so on? IMO, could work but there should be a &amp;ldquo;better&amp;rdquo; way.&lt;/p>
&lt;h3 id="suggest-from-the-second-entity">Suggest from the second entity&lt;/h3>
&lt;p>In other words we need to suggest text that starts after the separator. The implementation assumes that you have 2 entities. Once again, we will leverage different index and search time analyzers. For the indexing to achieve the required functionality with token filters would be complicated. The hack would be to use &lt;code>char_filter&lt;/code> to get rid of the first &amp;ldquo;entity&amp;rdquo;.&lt;/p>
&lt;pre>&lt;code class="language-text">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;char_filter&amp;quot;: {
&amp;quot;remove_until_separator&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;(.*and )|(.*&amp;amp; )&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;&amp;quot;
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;drop_first_entity&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;char_filter&amp;quot;: [&amp;quot;remove_until_separator&amp;quot;],
&amp;quot;filter&amp;quot;: [ &amp;quot;lowercase&amp;quot;]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;from_second_entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;drop_first_entity&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;preserve_position_increments&amp;quot;: false
}
}
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;e&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest.from_second_entity&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Returns:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;e&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="suggest-from-the-last-word">Suggest from the last word&lt;/h3>
&lt;p>The idea is not to tokenize the string, reverse the string, tokenize the reversed string, and reverse the tokens once again.&lt;/p>
&lt;pre>&lt;code class="language-text">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;last_word&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;word_delimiter_graph&amp;quot;,
&amp;quot;reverse&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;from_last_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;last_word&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;preserve_position_increments&amp;quot;: false
}
}
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;j&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest.from_last_word&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>returns&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;j&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Note that the suggestion for &amp;ldquo;john e&amp;rdquo; also works. Which might be a bit unexpected.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Every separate use case is covered with a dedicated field. To support all of them at once you just need to add all the subfields into index and query all of them with multiple &lt;code>suggest&lt;/code> clauses. How to combine those suggestions is out of scope for this post, but it should be implemented in your app.&lt;/p>
&lt;h2 id="bonus-1-synonyms-for-suggestions">Bonus 1: Synonyms for suggestions&lt;/h2>
&lt;p>After the &amp;ldquo;successful&amp;rdquo; release of the new functionality the very next day PM (under the usual influence of some exotic and probably illegal substances) once again came up with new idea: &amp;ldquo;in search suggestions we need to support variants of artist names&amp;rdquo;. His example was: &amp;ldquo;I&amp;rsquo;ve heard that most babies can&amp;rsquo;t pronounce &lt;code>britney&lt;/code> and she say something like &lt;a href="https://mom.com/baby-names/girl/19714/britney">&lt;code>ditney&lt;/code>&lt;/a>, so to make our product more successful among the baby searchers segment we must support this use case&amp;rdquo;.&lt;/p>
&lt;p>Somewhat reasonable :)&lt;/p>
&lt;p>The idea is to add synonym token filter for the artist names so that the synonym token would be on the 0 position in the token stream.&lt;/p>
&lt;pre>&lt;code class="language-kibana">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;name_synonyms&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym&amp;quot;,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;britney, ditney&amp;quot;
]
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonyms&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;name_synonyms&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;synonyms&amp;quot;
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;d&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Returns:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;artist&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;d&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>It works, amazing! Baby searchers are happy.&lt;/p>
&lt;h2 id="bonus-2-shingles-for-suggestions">Bonus 2: shingles for suggestions&lt;/h2>
&lt;p>After some sleeping on the mind-bending development experience a colleague asked: &amp;ldquo;can&amp;rsquo;t we just use one field for all the requirements?&amp;rdquo;. His reasoning was that we index the same string multiple times and our clusters doesn&amp;rsquo;t have infinite capacity. Also, we&amp;rsquo;ve seen what and how PM is thinking, and it is only a matter of time when we will have to support suggestions from everywhere in the string&amp;quot;. We&amp;rsquo;ve already seen that using synonyms it is doable.&lt;/p>
&lt;p>His idea was to analyze text in such a way that it would produce multiple tokens with the position=0 for all the potential suggestion &amp;ldquo;entry points&amp;rdquo;. To achieve it we could &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-shingle-tokenfilter.html">shingle&lt;/a> the string, take only shingles that has position=0 and take the last word from every shingle. Let&amp;rsquo;s try.&lt;/p>
&lt;pre>&lt;code class="language-kibana">DELETE music
PUT music
{
&amp;quot;settings&amp;quot;: {
&amp;quot;index.max_shingle_diff&amp;quot;: 10,
&amp;quot;analysis&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;after_last_space&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;(.* )&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;&amp;quot;
},
&amp;quot;preserve_only_first&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position == 0&amp;quot;
}
},
&amp;quot;big_shingling&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;shingle&amp;quot;,
&amp;quot;min_shingle_size&amp;quot;: 2,
&amp;quot;max_shingle_size&amp;quot;: 10,
&amp;quot;output_unigrams&amp;quot;: true
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;dark_magic&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;big_shingling&amp;quot;,
&amp;quot;preserve_only_first&amp;quot;,
&amp;quot;after_last_space&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;completion&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;dark_magic&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
PUT music/_doc/1?refresh
{
&amp;quot;suggest&amp;quot; : {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s test how this analyzer works:&lt;/p>
&lt;pre>&lt;code class="language-kibana">POST music/_analyze
{
&amp;quot;explain&amp;quot;: false,
&amp;quot;text&amp;quot;: [&amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;dark_magic&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Returns&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;tokens&amp;quot;: [
{
&amp;quot;token&amp;quot;: &amp;quot;britney&amp;quot;,
&amp;quot;start_offset&amp;quot;: 0,
&amp;quot;end_offset&amp;quot;: 7,
&amp;quot;type&amp;quot;: &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot;: 0
},
{
&amp;quot;token&amp;quot;: &amp;quot;spears&amp;quot;,
&amp;quot;start_offset&amp;quot;: 0,
&amp;quot;end_offset&amp;quot;: 14,
&amp;quot;type&amp;quot;: &amp;quot;shingle&amp;quot;,
&amp;quot;position&amp;quot;: 0,
&amp;quot;positionLength&amp;quot;: 2
},
{
&amp;quot;token&amp;quot;: &amp;quot;elton&amp;quot;,
&amp;quot;start_offset&amp;quot;: 0,
&amp;quot;end_offset&amp;quot;: 22,
&amp;quot;type&amp;quot;: &amp;quot;shingle&amp;quot;,
&amp;quot;position&amp;quot;: 0,
&amp;quot;positionLength&amp;quot;: 3
},
{
&amp;quot;token&amp;quot;: &amp;quot;john&amp;quot;,
&amp;quot;start_offset&amp;quot;: 0,
&amp;quot;end_offset&amp;quot;: 27,
&amp;quot;type&amp;quot;: &amp;quot;shingle&amp;quot;,
&amp;quot;position&amp;quot;: 0,
&amp;quot;positionLength&amp;quot;: 4
}
]
}
&lt;/code>&lt;/pre>
&lt;p>Positions of all the tokens are 0. Good start.&lt;/p>
&lt;p>Let&amp;rsquo;s test the suggestions with all the potential queries in the same request:&lt;/p>
&lt;pre>&lt;code class="language-text">
POST music/_search?filter_path=suggest
{
&amp;quot;suggest&amp;quot;: {
&amp;quot;britney&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;b&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
},
&amp;quot;spears&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;s&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
},
&amp;quot;elton&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;e&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
},
&amp;quot;john&amp;quot;: {
&amp;quot;prefix&amp;quot;: &amp;quot;j&amp;quot;,
&amp;quot;completion&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;suggest&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>It returns:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;suggest&amp;quot;: {
&amp;quot;britney&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;b&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
],
&amp;quot;elton&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;e&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
],
&amp;quot;john&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;j&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
],
&amp;quot;spears&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;s&amp;quot;,
&amp;quot;offset&amp;quot;: 0,
&amp;quot;length&amp;quot;: 1,
&amp;quot;options&amp;quot;: [
{
&amp;quot;text&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;,
&amp;quot;_index&amp;quot;: &amp;quot;music&amp;quot;,
&amp;quot;_id&amp;quot;: &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot;: 1,
&amp;quot;_source&amp;quot;: {
&amp;quot;suggest&amp;quot;: {
&amp;quot;input&amp;quot;: &amp;quot;Britney Spears &amp;amp; Elton John&amp;quot;
}
}
}
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Wow! Note, that a query (that spans several tokens) e.g. &lt;code>britney sp&lt;/code> doesn&amp;rsquo;t match anything. Fixable, but let&amp;rsquo;s leave the fix out of scope for now.&lt;/p>
&lt;h2 id="fin">Fin&lt;/h2>
&lt;p>Thank you and congratulations: You got to the very end of the blog post. Tell me about your craziest adventures with the search suggestions in the comments below?&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Have a look at the impressive engineering of &lt;a href="https://github.com/apache/lucene/blob/main/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingSuggester.java">Lucene&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>All the characters in this story are completely fictional. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Entity Resolution with Opensearch/Elasticsearch</title><link>https://www.jocas.lt/blog/post/entity-resolution/</link><pubDate>Wed, 12 Jan 2022 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/entity-resolution/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>The &lt;a href="https://en.wikipedia.org/wiki/Record_linkage">entity resolution&lt;/a> can be implemented as a search application and if the requirements are not too crazy then Opensearch/Elasticsearch is good enough.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Say that our company have a curated registry of organizations (only organization names with the number of employees) conveniently indexed in the OpenSearch/Elasticsearch.
Our employer acquired a direct competitor with their own nice little registry (only organization names with their addresses), and we were tasked to integrate the new registry with our old registry, i.e. to perform an entity resolution.
Our task is to iterate through the new registry record by record and try to map them to our &amp;ldquo;Golden registry&amp;rdquo;.
Unfortunately, the only overlapping data is organization names and our matching needs to be based mostly on the organization name.
Let&amp;rsquo;s implement organization name matching by text similarity directly with Opensearch/Elasticsearch.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>Say that we were given these organization name similarity rules in the descending order of importance.
Let&amp;rsquo;s have an example query &amp;ldquo;Apple&amp;rdquo; in mind as we go:&lt;/p>
&lt;ol>
&lt;li>Exact match, e.g. &amp;ldquo;Apple&amp;rdquo;&lt;/li>
&lt;li>Exact first word match, e.g. &amp;ldquo;&lt;strong>Apple&lt;/strong> Computers&amp;rdquo;&lt;/li>
&lt;li>Exact not first word match, e.g. &amp;ldquo;Big &lt;strong>Apple&lt;/strong> Company&amp;rdquo;&lt;/li>
&lt;li>Partial first word match - start, e.g. &amp;ldquo;&lt;strong>Apple&lt;/strong>sauce Company&amp;rdquo;&lt;/li>
&lt;li>Partial first word match - end, e.g. &amp;ldquo;Pine&lt;strong>apple&lt;/strong> Manufacturing&amp;rdquo;&lt;/li>
&lt;li>Partial not first word match - end, e.g. &amp;ldquo;Canadian Bake&lt;strong>apple&lt;/strong>&amp;rdquo;&lt;/li>
&lt;li>Fuzzy, e.g. &amp;ldquo;&lt;strong>Apply&lt;/strong>&amp;rdquo;&lt;/li>
&lt;/ol>
&lt;p>On top of that, we should be able to add other signals that could change the ordering,
e.g. the number of employees: the more employees the matching organization has the higher its matching score.&lt;/p>
&lt;p>&amp;ldquo;Applesauce Company&amp;rdquo; with 100 employees should be lower than &amp;ldquo;Pineapple Manufacturing&amp;rdquo; with 100000 employees despite that
the rule of a &lt;em>partial first word match at the start&lt;/em> has a higher name similarity than the &lt;em>partial not first word match at the end&lt;/em>.&lt;/p>
&lt;p>And yes, the entity resolution should be implemented only with features provided by Opensearch/Elasticsearch.&lt;/p>
&lt;p>NOTE: the proposed solution is not production ready, use the examples wisely.&lt;/p>
&lt;h3 id="additional-notes-for-development">Additional notes for development&lt;/h3>
&lt;p>To make the development easier we also want:&lt;/p>
&lt;ul>
&lt;li>to know which rule matched for each hit we can leverage &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-request-named-queries-and-filters.html">named queries&lt;/a> feature (or try the highlighting&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>),&lt;/li>
&lt;li>give each rule a normalized score (&lt;a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">TF-IDF&lt;/a> is hard to normalize),&lt;/li>
&lt;li>matching of strings should be case-insensitive,&lt;/li>
&lt;li>Index (conveniently names &lt;code>organizations&lt;/code>) contains the sample organizations list (extracted from the requirements examples),&lt;/li>
&lt;li>Work out the examples in Kibana/Dashboards,&lt;/li>
&lt;/ul>
&lt;p>The indexing of organizations can be done with this simple command:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations/_bulk
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;3&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Big Apple Company&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;4&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Applesauce Company&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;5&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Pineapple Manufacturing&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;6&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Canadian Bakeapple&amp;quot; }
{ &amp;quot;index&amp;quot; : { &amp;quot;_id&amp;quot; : &amp;quot;7&amp;quot; } }
{ &amp;quot;name&amp;quot; : &amp;quot;Apply&amp;quot; }
&lt;/code>&lt;/pre>
&lt;h2 id="implementation-strategy">Implementation Strategy&lt;/h2>
&lt;p>Let&amp;rsquo;s split the implementation into 3 parts:&lt;/p>
&lt;ul>
&lt;li>implement the organization name matching requirements,&lt;/li>
&lt;li>implement the scoring signal based on the number of employees,&lt;/li>
&lt;li>fine-tune the scoring function.&lt;/li>
&lt;/ul>
&lt;h2 id="string-matching-implementation">String Matching Implementation&lt;/h2>
&lt;p>In the following sections we&amp;rsquo;ll implement all string matching signals with the OpenSearch/Elasticsearch text analysis features.
I&amp;rsquo;ll work out each requirement in isolation.
For each rule we will define a subfield of the &lt;code>name&lt;/code> attribute with an analyzer and a corresponding query clause.&lt;/p>
&lt;h3 id="exact-match">Exact match&lt;/h3>
&lt;p>Exact matching is rather simple to implement, just use &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/keyword.html#keyword-field-type">&lt;code>keyword&lt;/code>&lt;/a> datatype with a &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/normalizer.html">normalizer&lt;/a> that &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenfilter.html">lowercases&lt;/a> the string.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-kibana">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;lowercased_keyword&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;keyword_lowercased&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;lowercased_keyword&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-kibana">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;term&amp;quot;: {
&amp;quot;name.keyword_lowercased&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;exact_match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 7
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Matches exactly one organization:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 3,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 7.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 7.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;exact_match&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="exact-first-word-match">Exact first word match&lt;/h3>
&lt;p>With this requirement we want the query to match the first word of the organization name&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>We can implement the requirement by simply indexing only the first token.
To achieve it we could use the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-limit-token-count-tokenfilter.html">&lt;code>limit&lt;/code>&lt;/a> token filter.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_one_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;first_token&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_one_token_limit&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>We leverage the fact that by default the &lt;code>limit&lt;/code> token filter defaults &lt;code>max_token_count&lt;/code> parameters to 1 token.
And we don&amp;rsquo;t need to define a new token filter (key damage to the keyboard, yay).&lt;/p>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;term&amp;quot;: {
&amp;quot;name.first_token&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Exact first word match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 6
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 6.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 6.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Exact first word match&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 6.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Exact first word match&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Note that not only &amp;ldquo;Apple Computers&amp;rdquo; matched but also &amp;ldquo;Apple&amp;rdquo; matched.
This makes sense because &amp;ldquo;Apple&amp;rdquo; has only one word it is &lt;em>the first word&lt;/em>.&lt;/p>
&lt;p>If we wanted to exclude the &amp;ldquo;Apple&amp;rdquo; match we could combine the &lt;code>Exact first word match&lt;/code> with the &lt;code>Exact match&lt;/code> and construct a bool query, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;bool&amp;quot;: {
&amp;quot;_name&amp;quot;: &amp;quot;Exact first word match&amp;quot;,
&amp;quot;must&amp;quot;: [
{
&amp;quot;term&amp;quot;: {
&amp;quot;name.first_token&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;
}
}
}
],
&amp;quot;must_not&amp;quot;: [
{
&amp;quot;term&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;
}
}
}
]
}
},
&amp;quot;boost&amp;quot;: 6
}
}
}
&lt;/code>&lt;/pre>
&lt;p>For similar situations we could act similarly but let&amp;rsquo;s keep it simple.&lt;/p>
&lt;h3 id="exact-not-first-word-match">Exact not first word match&lt;/h3>
&lt;p>E.g.: query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Big Apple Company&amp;rdquo;.
The interesting bit is how to differentiate this rule from the &lt;code>Exact first word match&lt;/code>?
As an implementation we could create a boolean query that filters out the first word match.
Another way to implement the is not to index the first word of the organization name.
A slight complication with this approach is that the query time analyzer should be different from
the index time analyzer to prevent the removal of the first (and probably the only) word from the query.
Let&amp;rsquo;s proceed with the second approach because I think it is more interesting.&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;tokenized&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;tokenized_without_first_word&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;remove_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position != 0&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;tokenized_without_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;tokenized_without_first_word&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;tokenized&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note, for the sake of simplicity for examples we assume only one word queries.&lt;/p>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.tokenized_without_first_word&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Exact not first word match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 5
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 6.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;3&amp;quot;,
&amp;quot;_score&amp;quot; : 6.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Big Apple Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Exact not first word match&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="partial-first-word-match---start">Partial first word match - start&lt;/h3>
&lt;p>E.g. Query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Applesauce Company&amp;rdquo;.
Probably we could get away with a simple prefix query, but if we want to make it fast for larger indices we should do &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenfilter.html">edge n-grams token filter&lt;/a>.
The idea is to keep only this first word and then apply the edge n-gram token filter on it.
Of course, query analyzer should be specified without edge n-gram token filter to prevent false hits from matching a couple of first characters from the query, e.g. query &amp;ldquo;Apps&amp;rdquo; would match &amp;ldquo;Apple&amp;rdquo; which we don&amp;rsquo;t want.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_first_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_first_token_limit_edge_ngrams&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;1_10_edgegrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;,
&amp;quot;min_gram&amp;quot;: 1,
&amp;quot;max_gram&amp;quot;: 10
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;first_token_edge_ngram&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_first_token_limit_edge_ngrams&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Here the &lt;code>max_ngram&lt;/code> is set to 10 for no big reason, it really depends on your data.&lt;/p>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_token_edge_ngram&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial first word match - start&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 4
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 3,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 4.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 4.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - start&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 4.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - start&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;4&amp;quot;,
&amp;quot;_score&amp;quot; : 4.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Applesauce Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - start&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Note that &amp;ldquo;Apple&amp;rdquo; and &amp;ldquo;Apple Computers&amp;rdquo; also matched along the &amp;ldquo;Applesauce Company&amp;rdquo;.
We could prevent matching the first two by doing the bool query with must_not match on the whole first token.&lt;/p>
&lt;h3 id="partial-first-word-match---end">Partial first word match - end&lt;/h3>
&lt;p>E.g. Query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Pineapple Manufacturing&amp;rdquo;.
In other words, the query string should be the ending of the first word of the organization name.
The idea is to take the first word, reverse it, apply edge n-grams, reverse those n-grams.
The search time analyzer should not have an edge n-grams token filter.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_first_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_one_token_limit_endings&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;reverse&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;1_10_edgegrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;,
&amp;quot;min_gram&amp;quot;: 1,
&amp;quot;max_gram&amp;quot;: 10
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;first_word_end_ngrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_one_token_limit_endings&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_word_end_ngrams&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial first word match - end&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 3
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-text">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 3,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 3.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 3.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 3.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;5&amp;quot;,
&amp;quot;_score&amp;quot; : 3.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Pineapple Manufacturing&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="partial-not-first-word-match---end">Partial not first word match - end&lt;/h3>
&lt;p>E.g. query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Canadian Bakeapple&amp;rdquo;.
The idea is to tokenize, lowercase, remove first word, reverse tokens, create edge n-grams, reverse back.
Search time analyzer should not include edge n-grams.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_first_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_remaining_token_endings&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;reverse&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;1_10_edgegrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;,
&amp;quot;min_gram&amp;quot;: 1,
&amp;quot;max_gram&amp;quot;: 10
},
&amp;quot;remove_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position != 0&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;remaining_words_end_ngrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_remaining_token_endings&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.remaining_words_end_ngrams&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial not first word match - end&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 2
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 2.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;3&amp;quot;,
&amp;quot;_score&amp;quot; : 2.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Big Apple Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial not first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;6&amp;quot;,
&amp;quot;_score&amp;quot; : 2.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Canadian Bakeapple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial not first word match - end&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;h3 id="fuzzy">Fuzzy&lt;/h3>
&lt;p>E.g. query &amp;ldquo;Apple&amp;rdquo; should match &amp;ldquo;Apply&amp;rdquo;.
It is pretty much the exact match but with some &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html#query-dsl-match-query-fuzziness">fuzziness&lt;/a> allowed.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;lowercased_keyword&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;keyword_lowercased&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;lowercased_keyword&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;fuzziness&amp;quot;: 1,
&amp;quot;_name&amp;quot;: &amp;quot;fuzzy_1&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 1
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Hits:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;fuzzy_1&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;7&amp;quot;,
&amp;quot;_score&amp;quot; : 1.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apply&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;fuzzy_1&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>In some circumstances we might allow fuzziness set to more than 1.&lt;/p>
&lt;h2 id="lets-combine-the-pieces-together">Let&amp;rsquo;s combine the pieces together&lt;/h2>
&lt;p>Phew, that was pretty involved.
Let&amp;rsquo;s combine the previous examples into one, so our requirements are satisfied.&lt;/p>
&lt;p>Index configuration is constructed by merging analysis components and adding fields from all the examples:&lt;/p>
&lt;pre>&lt;code class="language-text">PUT organizations
{
&amp;quot;settings&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;lowercased_keyword&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;1_10_edgegrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;edge_ngram&amp;quot;,
&amp;quot;min_gram&amp;quot;: 1,
&amp;quot;max_gram&amp;quot;: 10
},
&amp;quot;remove_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;predicate_token_filter&amp;quot;,
&amp;quot;script&amp;quot;: {
&amp;quot;source&amp;quot;: &amp;quot;token.position != 0&amp;quot;
}
}
},
&amp;quot;analyzer&amp;quot;: {
&amp;quot;standard_one_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;tokenized&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;tokenized_without_first_word&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_first_token_limit_edge_ngrams&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;
]
},
&amp;quot;standard_first_token_limit&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;
]
},
&amp;quot;standard_remaining_token_endings&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;remove_first_word&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;reverse&amp;quot;
]
},
&amp;quot;standard_one_token_limit_endings&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;limit&amp;quot;,
&amp;quot;lowercase&amp;quot;,
&amp;quot;reverse&amp;quot;,
&amp;quot;1_10_edgegrams&amp;quot;,
&amp;quot;reverse&amp;quot;
]
}
}
}
},
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;first_token&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_one_token_limit&amp;quot;
},
&amp;quot;tokenized_without_first_word&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;tokenized_without_first_word&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;tokenized&amp;quot;
},
&amp;quot;first_token_edge_ngram&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_first_token_limit_edge_ngrams&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
},
&amp;quot;first_word_end_ngrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_one_token_limit_endings&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
},
&amp;quot;remaining_words_end_ngrams&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard_remaining_token_endings&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;standard_first_token_limit&amp;quot;
},
&amp;quot;keyword_lowercased&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;lowercased_keyword&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Here&lt;/p>
&lt;p>The query is constructed from all the previous examples simply by adding constant score queries to the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-dis-max-query.html">dis_max&lt;/a> query:&lt;/p>
&lt;pre>&lt;code class="language-text">GET organizations/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;dis_max&amp;quot;: {
&amp;quot;queries&amp;quot;: [
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;term&amp;quot;: {
&amp;quot;name.keyword_lowercased&amp;quot;: {
&amp;quot;value&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;exact_match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 7
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_token&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Exact first word match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 6
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.tokenized_without_first_word&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Exact not first word match&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 5
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_token_edge_ngram&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial first word match - start&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 4
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.first_word_end_ngrams&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial first word match - end&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 3
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name.remaining_words_end_ngrams&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;_name&amp;quot;: &amp;quot;Partial not first word match - end&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 2
}
},
{
&amp;quot;constant_score&amp;quot;: {
&amp;quot;filter&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;name&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;Apple&amp;quot;,
&amp;quot;fuzziness&amp;quot;: 1,
&amp;quot;_name&amp;quot;: &amp;quot;fuzzy_1&amp;quot;
}
}
},
&amp;quot;boost&amp;quot;: 1
}
}
]
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Finally, the list of organizations ranked by the name similarity:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot; : 2,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 7,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 7.0,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 7.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;exact_match&amp;quot;,
&amp;quot;Partial first word match - end&amp;quot;,
&amp;quot;Partial first word match - start&amp;quot;,
&amp;quot;Exact first word match&amp;quot;,
&amp;quot;fuzzy_1&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 6.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apple Computers&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;,
&amp;quot;Partial first word match - start&amp;quot;,
&amp;quot;Exact first word match&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;3&amp;quot;,
&amp;quot;_score&amp;quot; : 5.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Big Apple Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Exact not first word match&amp;quot;,
&amp;quot;Partial not first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;4&amp;quot;,
&amp;quot;_score&amp;quot; : 4.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Applesauce Company&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - start&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;5&amp;quot;,
&amp;quot;_score&amp;quot; : 3.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Pineapple Manufacturing&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;6&amp;quot;,
&amp;quot;_score&amp;quot; : 2.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Canadian Bakeapple&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;Partial not first word match - end&amp;quot;
]
},
{
&amp;quot;_index&amp;quot; : &amp;quot;organizations&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;7&amp;quot;,
&amp;quot;_score&amp;quot; : 1.0,
&amp;quot;_source&amp;quot; : {
&amp;quot;name&amp;quot; : &amp;quot;Apply&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;fuzzy_1&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Nice, from the most similar &amp;ldquo;Exact match&amp;rdquo; all the way down to the &amp;ldquo;fuzzy&amp;rdquo; matches.&lt;/p>
&lt;p>I believe this post got a bit too long to continue working out the remaining requirements.
I&amp;rsquo;ll finish the developments in the future post.
Also, it shouldn&amp;rsquo;t be too hard to load bigger dataset to test our entity resolution algorithm, something like &lt;a href="http://download.companieshouse.gov.uk/en_output.html">Companies House registry&lt;/a> with some real world data.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>I hope this exercise was interesting.
To sum up:&lt;/p>
&lt;ul>
&lt;li>I&amp;rsquo;ve demonstrated how to do the text analysis tricks that would shape the organization name string according to the requirements.&lt;/li>
&lt;li>As a way to normalize search scores I&amp;rsquo;ve used the &lt;code>constant_score&lt;/code> query. To come up with this trick is not very obvious because Elasticsearch scoring by default is unbounded due to text statistics and therefore not very well suited
to satisfy the requirements of matching semi structured short strings such ar organization names.&lt;/li>
&lt;li>Another neat trick was to use the &lt;code>dis_max&lt;/code> query to calculate the score by taking only the most significant similarity signal.
In this way we&amp;rsquo;ve prevented the situations where less similar matches would go up because they matched several less significant
similarity rules and their score were added up.&lt;/li>
&lt;li>Also, by using &lt;code>_name&lt;/code> parameters for each query clause we get which similarity rules matched despite the fact that we&amp;rsquo;ve took the score
of the highest scoring similarity rule. These flags might be used for further rescoring in your application.&lt;/li>
&lt;/ul>
&lt;p>In case of any comments or questions don&amp;rsquo;t hesitate to leave comments under this Github &lt;a href="https://github.com/dainiusjocas/blog/issues/25">issue&lt;/a>.&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Another approach would be to use hits &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/highlighting.html">highlighting&lt;/a>. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Of course, this is a bit unrealistic because organization name might very well contain more than one word.
A hint on how to implement the same requrements for two word organization names would be to use &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-shingle-tokenfilter.html">shingles&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Clojure deftype with `unsynchronized-mutable`</title><link>https://www.jocas.lt/blog/post/deftype-unsynchronized-mutable/</link><pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/deftype-unsynchronized-mutable/</guid><description>&lt;p>In this post let&amp;rsquo;s take a closer look at why the &lt;code>^:unsynchronized-mutable&lt;/code> is used in this Clojure code snippet and learn some Java in the process.&lt;/p>
&lt;pre>&lt;code class="language-clojure">(ns user
(:import (net.thisptr.jackson.jq JsonQuery Scope Output)
(com.fasterxml.jackson.databind JsonNode)))
(definterface IGetter
(^com.fasterxml.jackson.databind.JsonNode getValue []))
(deftype OutputContainer [^:unsynchronized-mutable ^JsonNode container]
Output
(emit [_ output-json-node] (set! container output-json-node))
IGetter
(getValue [_] container))
(defn query-json-node [^JsonNode data ^JsonQuery query]
(let [output-container (OutputContainer. nil)]
(.apply query (Scope/newEmptyScope) data output-container)
(.getValue output-container)))
&lt;/code>&lt;/pre>
&lt;p>Note that this code snippet is a bit trimmed down for clarity.
The full code can be found &lt;a href="https://github.com/dainiusjocas/clj-jq/blob/main/src/jq/core.clj">here&lt;/a>.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>I was working on &lt;a href="https://github.com/dainiusjocas/clj-jq">&lt;code>clj-jq&lt;/code>&lt;/a>:
a Clojure library whose goal is to make it easy to embed a &lt;a href="https://stedolan.github.io/jq/">&lt;code>jq&lt;/code>&lt;/a> processor directly into an application and that this application can be compiled with the GraalVM &lt;code>native-image&lt;/code>.
&lt;code>clj-jq&lt;/code> was intended to be a thin wrapper for the &lt;a href="https://github.com/eiiches/jackson-jq">&lt;code>jackson-jq&lt;/code>&lt;/a> Java library.
The exact use cases of the &lt;code>clj-jq&lt;/code> are left out for future blog posts.&lt;/p>
&lt;h2 id="wrapper">Wrapper&lt;/h2>
&lt;p>The &lt;code>jackson-jq&lt;/code> provides a nice example &lt;a href="https://github.com/eiiches/jackson-jq/blob/develop/1.x/jackson-jq/src/test/java/examples/Usage.java">class&lt;/a> that helped me to start implementing the wrapper.
I&amp;rsquo;ve taken a straightforward approach to create a Clojure library: just &amp;ldquo;translate&amp;rdquo; the necessary Java code to Clojure using the &lt;a href="https://clojure.org/reference/java_interop">Java interop&lt;/a> more or less verbatim, parameterize the hardcoded parts, refactor here and there, and the result would be a new library.&lt;/p>
&lt;p>The &amp;ldquo;translation&amp;rdquo; went as expected until I had to deal with a code that implements the &lt;a href="https://github.com/eiiches/jackson-jq/blob/6bf785ddb29618f53dafe2f336cd80bdf18a6b45/jackson-jq/src/main/java/net/thisptr/jackson/jq/Output.java">&lt;code>Output&lt;/code>&lt;/a> interface:&lt;/p>
&lt;pre>&lt;code class="language-java">final List&amp;lt;JsonNode&amp;gt; out = new ArrayList&amp;lt;&amp;gt;();
q.apply(childScope, in, out::add);
&lt;/code>&lt;/pre>
&lt;p>In the example an instance of &lt;code>ArrayList&lt;/code> and some Java magic was used (we&amp;rsquo;ll talk about what and why in the &lt;a href="#appendix">appendix&lt;/a>).
Since Clojure functions doesn&amp;rsquo;t implement &lt;code>Output&lt;/code> interface I can&amp;rsquo;t pass a function, therefore I&amp;rsquo;ve &amp;ldquo;translated&amp;rdquo; the code using &lt;code>reify&lt;/code> approach as shown below:&lt;/p>
&lt;pre>&lt;code class="language-clojure">(defn query-json-node [^JsonNode data ^JsonQuery query]
(let [array-list (ArrayList.)]
(.apply query (Scope/newChildScope root-scope) data
(reify Output
(emit [this json-node] (.add array-list json-node))))
(.writeValueAsString mapper ^JsonNode (.get array-list 0))))
&lt;/code>&lt;/pre>
&lt;p>it worked, but I wasn&amp;rsquo;t happy about it.&lt;/p>
&lt;p>Using an &lt;code>ArrayList&lt;/code> in such a function might be OK for an example.
But for a library something more specialized is desired because of several reasons:&lt;/p>
&lt;ul>
&lt;li>the &lt;code>ArrayList&lt;/code> will always hold just one value;&lt;/li>
&lt;li>allocating an &lt;code>ArrayList&lt;/code> and &lt;code>reify&lt;/code>ing is not super cheap;&lt;/li>
&lt;/ul>
&lt;p>It got me thinking about how to make something better.&lt;/p>
&lt;h2 id="requirements">Requirements&lt;/h2>
&lt;p>The requirements for the &lt;code>Output&lt;/code> implementation:&lt;/p>
&lt;ul>
&lt;li>a specialized and efficient class;&lt;/li>
&lt;li>that class has to act as a mutable&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> container that holds one value;&lt;/li>
&lt;/ul>
&lt;p>Efficiency is desired because the library is intended to be used in &amp;ldquo;hot spots&amp;rdquo; of an application.&lt;/p>
&lt;p>Before starting to work on &lt;code>clj-jq&lt;/code> I expected that the work would not require writing any Java code because that would complicate the release of the library.
Therefore, I aimed to implement my &lt;code>Output&lt;/code> class in Clojure.
Also, &lt;code>gen-class&lt;/code> feels a bit &amp;ldquo;too much&amp;rdquo; for this little problem.&lt;/p>
&lt;h2 id="implementation">Implementation&lt;/h2>
&lt;p>The &lt;code>Output&lt;/code> interface defines behaviour, therefore I&amp;rsquo;ve chosen the &lt;a href="https://clojuredocs.org/clojure.core/deftype">&lt;code>deftype&lt;/code>&lt;/a>.
The implementation class needs mutable state to hold one value.
&lt;code>deftype&lt;/code> has two options for mutability of fields: &lt;code>volatile-mutable&lt;/code> and &lt;code>unsynchronized-mutable&lt;/code>.
The &lt;code>deftype&lt;/code> docstring advices that the usage of both options is discouraged and are for &amp;ldquo;experts only&amp;rdquo;.
I skipped the first part of the advice because I wanted to learn a good lesson.
For the second part my thinking was that if I used the options &amp;ldquo;correctly&amp;rdquo; I could consider myself an &amp;ldquo;expert&amp;rdquo; :-)&lt;/p>
&lt;p>&lt;img src="grumpy-expert.jpg" alt="Expert">&lt;/p>
&lt;p>What are the implications of those two &lt;code>deftype&lt;/code> field options:&lt;/p>
&lt;ul>
&lt;li>&lt;code>volatile-mutable&lt;/code> means that field is going to be marked as &lt;a href="https://www.baeldung.com/java-volatile">&lt;code>volatile&lt;/code>&lt;/a>.
Volatile in Java means that the variable is stored in the main memory and &lt;strong>not&lt;/strong> in the CPU cache.
This provides a data consistency guarantee that all threads will observe the updated value immediately&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.
Therefore, the &lt;code>volatile-mutable&lt;/code> option is OK to be used when the field is going to be written to by one thread and read from multiple threads.&lt;/li>
&lt;li>The field marked with &lt;code>unsynchronized-mutable&lt;/code> option is backed by a regular Java mutable field.
In contrast to &lt;code>volatile-mutable&lt;/code>, the &lt;code>unsynchronized-mutable&lt;/code> value might be stored in the CPU cache.
Therefore, &lt;code>unsynchronized-mutable&lt;/code> field is OK when used in single-threaded&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> contexts.&lt;/li>
&lt;/ul>
&lt;p>Since the scope in which my container class is going to be used is small and short-lived (can be considered as an internal implementation detail) and not multi-threaded,
I can choose an option that promises a better performance: &lt;code>unsynchronized-mutable&lt;/code>&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="go-back-to-the-code">Go back to the code&lt;/h2>
&lt;p>The most interesting bit of the entire snippet is the &lt;code>deftype&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-clojure">(deftype OutputContainer [^:unsynchronized-mutable ^JsonNode container]
Output
(emit [_ output-json-node] (set! container output-json-node))
IGetter
(getValue [_] container))
&lt;/code>&lt;/pre>
&lt;p>The class generated by &lt;code>deftype&lt;/code> will be named &lt;code>OutputContainer&lt;/code>.
It has one &lt;strong>mutable&lt;/strong> field: &lt;code>container&lt;/code> that is hinted to be of &lt;code>JsonNode&lt;/code> type.
By declaring a method mutable deftype generates the &lt;a href="https://clojuredocs.org/clojure.core/set!">&lt;code>set!&lt;/code>&lt;/a> assignment special form to allow mutation of the field.&lt;/p>
&lt;p>Usage example of the &lt;code>OutputContainer&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-clojure">(defn query-json-node [^JsonNode data ^JsonQuery query]
(let [output-container (OutputContainer. nil)]
(.apply query (Scope/newEmptyScope) data output-container)
(.getValue output-container)))
&lt;/code>&lt;/pre>
&lt;p>In the function &lt;code>query-json-node&lt;/code> we create an instance of the &lt;code>OutputContainer&lt;/code>.
Then pass it to &lt;code>apply&lt;/code> method of the &lt;code>JsonQuery&lt;/code>.
Somewhere in the &lt;code>apply&lt;/code> the &lt;code>Output::emit&lt;/code> will be called&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup>.
Finally, we take out the value of &lt;code>JsonNode&lt;/code> type from the &lt;code>output-container&lt;/code> instance.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>&lt;code>unsynchronized-mutable&lt;/code> is relatively rarely used in the wild: only &lt;a href="https://github.com/search?l=Clojure&amp;amp;q=unsynchronized-mutable&amp;amp;type=Code">776&lt;/a> out of &lt;a href="https://github.com/search?l=Clojure&amp;amp;q=deftype&amp;amp;type=Code">13,362&lt;/a> uses of &lt;code>deftype&lt;/code> according to GitHub code search as of 2021-09-09.
But when we do understand the implications, and we need exactly what the option provides, we can go ahead and use it despite what the docstring says.
And don&amp;rsquo;t forget to test your code!&lt;/p>
&lt;h2 id="a-idappendix-appendix">&lt;a id="appendix" />Appendix&lt;/h2>
&lt;p>Once again, let&amp;rsquo;s look at this piece of code:&lt;/p>
&lt;pre>&lt;code class="language-java">final List&amp;lt;JsonNode&amp;gt; out = new ArrayList&amp;lt;&amp;gt;();
q.apply(childScope, in, out::add);
&lt;/code>&lt;/pre>
&lt;p>Here method reference to &lt;code>add&lt;/code> of a &lt;code>List&amp;lt;JsonNode&amp;gt;&lt;/code> object is used as an argument to a method that expects &lt;code>Output&lt;/code> type.
How does it work?&lt;/p>
&lt;h3 id="a-bit-of-java-theory">A bit of Java theory&lt;/h3>
&lt;p>Any interface in Java with a SAM (Single Abstract Method) is a &lt;a href="https://www.baeldung.com/java-8-functional-interfaces">&lt;strong>functional interface&lt;/strong>&lt;/a>.
And for an argument of a functional interface type one can provide either:&lt;/p>
&lt;ul>
&lt;li>a class object that implements the interface,&lt;/li>
&lt;li>lambda expression.&lt;/li>
&lt;/ul>
&lt;p>A &lt;a href="https://docs.oracle.com/javase/tutorial/java/javaOO/methodreferences.html">method reference&lt;/a> can replace a lambda expression when passed and returned value types match.
You can use whichever is &lt;a href="https://stackoverflow.com/a/24493905/1728133">easier to read&lt;/a>.&lt;/p>
&lt;h3 id="why-does-it-work-anyway">Why does it work anyway?&lt;/h3>
&lt;p>The &lt;a href="https://github.com/eiiches/jackson-jq/blob/6bf785ddb29618f53dafe2f336cd80bdf18a6b45/jackson-jq/src/main/java/net/thisptr/jackson/jq/Output.java">&lt;code>Output&lt;/code>&lt;/a> is an interface that provides just one method &lt;code>emit&lt;/code> (default methods doesn&amp;rsquo;t count).
Therefore, it is a functional interface (however, not annotated as such).&lt;/p>
&lt;p>We can provide a lambda expression where a functional interface type is expected.
Also, in the same place we can provide a method reference whose types match the expected input-output types.&lt;/p>
&lt;p>What are the expected types for &lt;code>Output::emit&lt;/code>?
&lt;code>JsonNode&lt;/code> for input and &lt;code>void&lt;/code> for output.&lt;/p>
&lt;p>When a variable is of type &lt;code>List&amp;lt;JsonNode&amp;gt;&lt;/code> then the &lt;code>add&lt;/code> method accepts &lt;code>JsonNode&lt;/code> type and returns &lt;code>void&lt;/code>.&lt;/p>
&lt;p>We can see that types match, therefore method reference to &lt;code>out::add&lt;/code> can be used when &lt;code>Output&lt;/code> is required. Voila!&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>Mutable and not-shared might be OK. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Note that writes are not atomic for the volatile value. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Consult the &lt;a href="https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.4">Java Memory Model&lt;/a> docs to learn more. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>A proper discussion on &lt;a href="https://stackoverflow.com/questions/21127636/what-are-the-semantic-implications-of-volatile-mutable-versus-unsynchronized-m">&lt;code>deftype&lt;/code> mutability&lt;/a> and &lt;a href="https://stackoverflow.com/questions/3132931/mutable-fields-in-clojure-deftype">here&lt;/a> &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>For example &lt;a href="https://github.com/eiiches/jackson-jq/blob/6bf785ddb29618f53dafe2f336cd80bdf18a6b45/jackson-jq/src/main/java/net/thisptr/jackson/jq/internal/tree/ArrayConstruction.java#L29">here&lt;/a> &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Make Elasticsearch Query Profiling Faster in Kibana</title><link>https://www.jocas.lt/blog/post/kibana-profile-many-shards-hack/</link><pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/kibana-profile-many-shards-hack/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;pre>&lt;code>curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}' | jq ' .profile.shards = [.profile.shards[0]]' | pbcopy
&lt;/code>&lt;/pre>
&lt;p>and paste into the Kibana&amp;rsquo;s Search Profile panel.&lt;/p>
&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>When you profile a complex Elasticsearch query that targets many shards then Kibana might need a very long time (think, minutes) to visualize the profiling data.
It might be due to some bug in the Kibana or maybe you just throw too much data in there and since Javascript is single threaded it just takes time.
Anyway, you want to see the profile data visualization because reading the raw JSON is not your thing.&lt;/p>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;p>A trick you can try is to visualize only a part of the profile data.
What part?
Let&amp;rsquo;s say the profile data from only one shard.&lt;/p>
&lt;p>The Elasticsearch response with the profile data has this shape:&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;took&amp;quot;: 3,
&amp;quot;timed_out&amp;quot;: false,
&amp;quot;_shards&amp;quot;: {
&amp;quot;total&amp;quot;: 66,
&amp;quot;successful&amp;quot;: 66,
&amp;quot;skipped&amp;quot;: 0,
&amp;quot;failed&amp;quot;: 0
},
&amp;quot;hits&amp;quot;: {
&amp;quot;total&amp;quot;: {
&amp;quot;value&amp;quot;: 10000,
&amp;quot;relation&amp;quot;: &amp;quot;gte&amp;quot;
},
&amp;quot;max_score&amp;quot;: null,
&amp;quot;hits&amp;quot;: []
},
&amp;quot;profile&amp;quot;: {
&amp;quot;shards&amp;quot;: [
]
}
}
&lt;/code>&lt;/pre>
&lt;p>And the important bits are under &lt;code>.profile.shards&lt;/code> array (&lt;code>jq&lt;/code> syntax).
Let&amp;rsquo;s create a little &lt;code>jq&lt;/code> script that would transform the response body to include the profile data from only one shard:&lt;/p>
&lt;pre>&lt;code class="language-shell">jq '.profile.shards = [.profile.shards[0]]'
&lt;/code>&lt;/pre>
&lt;p>The output is now much smaller.&lt;/p>
&lt;p>For this script to work its input must be the full body of the Elasticsearch response in JSON.
Luckily for us, a simple &lt;code>curl&lt;/code> (that can be copied directly from Kibana Dev Tools) does the job well, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-shell">curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}'
&lt;/code>&lt;/pre>
&lt;p>Pipe the output of that &lt;code>curl&lt;/code> command to the &lt;code>jq&lt;/code> script:&lt;/p>
&lt;pre>&lt;code class="language-shell">curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}' | jq '.profile.shards = [.profile.shards[0]]'
&lt;/code>&lt;/pre>
&lt;p>Now just select the output with you mouse, copy, and then paste it in Kibana&amp;rsquo;s Search Profile panel and investigate the query profile.&lt;/p>
&lt;h2 id="but-using-the-mouse-in-terminal-is-not-sleek">But using the mouse in terminal is not sleek&lt;/h2>
&lt;p>One aditional trick to make the process sleeker is to send the profile data to the clipboard directly from your terminal and then paste in Kibana.
This can be done by simply piping the output to your clipboard.
Unfortunately, the script is different for different operating systems.&lt;/p>
&lt;p>Example in macOS:&lt;/p>
&lt;pre>&lt;code class="language-shell">curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}' | jq '.profile.shards = [.profile.shards[0]]' | pbcopy
&lt;/code>&lt;/pre>
&lt;p>Example in Kubuntu 21.04:&lt;/p>
&lt;pre>&lt;code class="language-shell">curl &amp;quot;http://localhost:9200/index-with-many-shards/_search&amp;quot; -H 'Content-Type: application/json' -d'{&amp;quot;profile&amp;quot;: true}' | jq '.profile.shards = [.profile.shards[0]]' | xclip -selection clip
&lt;/code>&lt;/pre></description></item><item><title>lmgrep Text Analysis</title><link>https://www.jocas.lt/blog/post/lucene-text-analysis/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/lucene-text-analysis/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>&lt;code>lmgrep&lt;/code> provides an easy way to play with various text analysis options.
Just download the &lt;code>lmgrep&lt;/code> &lt;a href="https://github.com/dainiusjocas/lucene-grep/releases">binary&lt;/a>, run it with &lt;code>--only-analyze&lt;/code>, and observe the list of tokens.&lt;/p>
&lt;pre>&lt;code class="language-shell">echo &amp;quot;Dogs and CATS&amp;quot; | lmgrep \
--only-analyze \
--analysis='
{
&amp;quot;tokenizer&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;standard&amp;quot;},
&amp;quot;token-filters&amp;quot;: [
{&amp;quot;name&amp;quot;: &amp;quot;lowercase&amp;quot;},
{&amp;quot;name&amp;quot;: &amp;quot;englishminimalstem&amp;quot;}
]
}'
# =&amp;gt; [&amp;quot;dog&amp;quot;,&amp;quot;and&amp;quot;,&amp;quot;cat&amp;quot;]
&lt;/code>&lt;/pre>
&lt;h2 id="text-analysis">Text Analysis&lt;/h2>
&lt;p>The &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html">Elasticsearch documentation&lt;/a> describes text analysis as:&lt;/p>
&lt;blockquote>
&lt;p>the process of converting unstructured text into a structured format thatâs optimized for search.&lt;/p>
&lt;/blockquote>
&lt;p>Therefore, to learn how the full-text search works it is important to understand how the text is, well, analyzed.
The remainder of the post focuses on how text analysis is done in &lt;a href="https://lucene.apache.org/">Lucene&lt;/a> which is the library that powers search engines like Elasticsearch and Solr, and what &lt;code>lmgrep&lt;/code> provides to analyze your text.&lt;/p>
&lt;h1 id="lucene">Lucene&lt;/h1>
&lt;p>Text analysis in the Lucene land is defined by 3 types of components:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>list&lt;/strong> of character filters (changes to the text before tokenization, e.g. HTML stripping, character replacement, etc.),&lt;/li>
&lt;li>&lt;strong>one&lt;/strong> tokenizer (splits text into tokens, e.g. at whitespace characters),&lt;/li>
&lt;li>&lt;strong>list&lt;/strong> of token filters (normalizes the tokens, e.g. lowercases all the letters).&lt;/li>
&lt;/ul>
&lt;p>The combination of text analysis components makes an &lt;code>Analyzer&lt;/code>.
You can think that an analyzer is a recipe to convert a string into a list of tokens&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>.&lt;/p>
&lt;h2 id="lmgrep">&lt;code>lmgrep&lt;/code>&lt;/h2>
&lt;p>&lt;code>lmgrep&lt;/code> is a search tool that is based on the &lt;a href="https://lucene.apache.org/core/8_2_0/monitor/org/apache/lucene/monitor/Monitor.html">Lucene Monitor&lt;/a> library.
To do the full-text search it needs to do the same thing that likes of Elasticsearch are doing: to analyze text.
&lt;code>lmgrep&lt;/code> packs many text &lt;a href="https://github.com/dainiusjocas/lucene-grep/blob/v2021.04.23/docs/analysis-components.md">analysis components&lt;/a>.
Also, it provides a list of &lt;a href="https://github.com/dainiusjocas/lucene-grep/blob/v2021.04.23/docs/predefined-analyzers.md">predefined analyzers&lt;/a>.
Nothing special here, the same battle tested and boring Lucene components that gets the job done&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>However, &lt;code>lmgrep&lt;/code> provides one clever twist to text analysis:
a way to specify an analyzer using plain data in JSON, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-shell">echo &amp;quot;&amp;lt;p&amp;gt;foo bars baz&amp;lt;/p&amp;gt;&amp;quot; | \
lmgrep \
--only-analyze \
--analysis='
{
&amp;quot;char-filters&amp;quot;: [
{&amp;quot;name&amp;quot;: &amp;quot;htmlStrip&amp;quot;},
{
&amp;quot;name&amp;quot;: &amp;quot;patternReplace&amp;quot;,
&amp;quot;args&amp;quot;: {
&amp;quot;pattern&amp;quot;: &amp;quot;foo&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;bar&amp;quot;
}
}
],
&amp;quot;tokenizer&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;standard&amp;quot;},
&amp;quot;token-filters&amp;quot;: [
{&amp;quot;name&amp;quot;: &amp;quot;englishMinimalStem&amp;quot;},
{&amp;quot;name&amp;quot;: &amp;quot;uppercase&amp;quot;}
]
}
'
# =&amp;gt; [&amp;quot;BAR&amp;quot;,&amp;quot;BAR&amp;quot;,&amp;quot;BAZ&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>Again, nothing special here, read the docs&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> of an interesting text analysis component, e.g.
character filter &lt;a href="https://lucene.apache.org/core/8_3_0/analyzers-common/org/apache/lucene/analysis/pattern/PatternReplaceCharFilterFactory.html">&lt;code>patternReplace&lt;/code>&lt;/a>,
add its config to the &lt;code>--analysis&lt;/code>, and apply it on your text.&lt;/p>
&lt;p>Conceptually it is very similar to what Elasticsearch or Solr are providing: &lt;code>analysis&lt;/code> part in the index configuration JSON in Elasticsearch, and Solr Schemas in XML.&lt;/p>
&lt;p>&lt;code>lmgrep&lt;/code> analysis component has this structure:&lt;/p>
&lt;pre>&lt;code>{&amp;quot;name&amp;quot;: &amp;quot;COMPONENT_NAME&amp;quot;, &amp;quot;args&amp;quot;: {&amp;quot;ARG_NAME&amp;quot;: &amp;quot;ARG_VALUE&amp;quot;}}
&lt;/code>&lt;/pre>
&lt;p>Notes:&lt;/p>
&lt;ul>
&lt;li>some components, e.g. &lt;code>stop&lt;/code> token filter, expect a file as an argument.
To support such components &lt;code>lmgrep&lt;/code> brutally patched Lucene to load data from arbitrary files while preserving the predefined analyzers with, e.g. their custom stop-word files.&lt;/li>
&lt;li>when a predefined analyzer is provided for text analysis then all other analysis components are silently ignored.&lt;/li>
&lt;li>predefined analyzers do not support the &lt;code>args&lt;/code> as of &lt;a href="https://github.com/dainiusjocas/lucene-grep/issues/82">now&lt;/a>, just the &lt;code>name&lt;/code>.&lt;/li>
&lt;li>&lt;code>lmgrep&lt;/code> as of now doesn&amp;rsquo;t provide a way to &lt;a href="https://github.com/dainiusjocas/lucene-grep/issues/83">share components between analyzers&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>That is pretty much all there is to know about how &lt;code>lmgrep&lt;/code> does text analysis. Try it out and let me know how it goes.&lt;/p>
&lt;h2 id="--only-analyze">&lt;code>--only-analyze&lt;/code>&lt;/h2>
&lt;p>I like the Elasticsearch&amp;rsquo;s &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html">Analyze API&lt;/a>.
It allows me to look at the raw tokens that are either stored in the index or produced out of the search query.&lt;/p>
&lt;p>To make debugging of &lt;code>lmgrep&lt;/code> easier I wanted to expose something similar to Analyze API. The &lt;code>--only-analyze&lt;/code> flag is my humble attempt to do that.&lt;/p>
&lt;p>When the flag is specified then &lt;code>lmgrep&lt;/code> just outputs a list of tokens that is produced by applying an analyzer on the input text, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-shell">echo &amp;quot;the quick brown fox&amp;quot; | lmgrep --only-analyze
# =&amp;gt; [&amp;quot;the&amp;quot;,&amp;quot;quick&amp;quot;,&amp;quot;brown&amp;quot;,&amp;quot;fox&amp;quot;]
&lt;/code>&lt;/pre>
&lt;h3 id="implementation">Implementation&lt;/h3>
&lt;p>The machinery under the &lt;code>--only-analyze&lt;/code> works as follows:&lt;/p>
&lt;ul>
&lt;li>one thread is dedicated to read and decode the text input (either from STDIN or a file),&lt;/li>
&lt;li>one thread is dedicated to write to the STDOUT,&lt;/li>
&lt;li>the remaining CPU cores can be used by a thread pool that analyzes the text (thanks to Lucene Analyzer implementation being thread-safe).&lt;/li>
&lt;/ul>
&lt;p>On my laptop &lt;code>lmgrep&lt;/code> analyzes ~1GB of text in ~11 seconds and consumes maximum 609 MB of RAM. It should result in ~200 GB of text per hour. IMO, not bad. Of course, the more involved the text analysis is the longer it takes.&lt;/p>
&lt;p>&lt;img src="text-analysis.png" alt="Performance">&lt;/p>
&lt;p>Note that the output of &lt;code>--only-analyze&lt;/code> has the same order as the input. IMO, it makes the output a bit easier to understand. However, preserving the order limits the throughput. It is because the time and resources needed to analyze an individual piece of text can vary greatly, and the required coordination introduces some overhead.&lt;/p>
&lt;p>Consider an example of analyzing the text attributes of a book: assume that the first line sent to &lt;code>lmgrep&lt;/code> is the title of the book, the second line contains a full text of the book, and the third line is the summary. The title is relatively small, it is quickly analyzed and immediately written to STDOUT. The summary is a bit longer but still many times smaller than the body. To preserve the order &lt;code>lmgrep&lt;/code> (before writing the tokens of the summary to STDOUT) waits for the analysis on the body to be finished and written to STDOUT and only then tokens of the summary are written out.&lt;/p>
&lt;p>Notes:&lt;/p>
&lt;ul>
&lt;li>the &lt;code>--explain&lt;/code> flag is coming to &lt;code>lmgrep&lt;/code>;&lt;/li>
&lt;li>the output lines are valid JSON (&lt;code>jq&lt;/code> is your friend);&lt;/li>
&lt;li>the positional arguments for &lt;code>--only-analyze&lt;/code> are interpreted as files and when present then STDIN is ignored.&lt;/li>
&lt;/ul>
&lt;h3 id="interesting-bit">Interesting Bit&lt;/h3>
&lt;p>One thing that constantly frustrates me with Elasticsearch&amp;rsquo;s Analysis API is that I can&amp;rsquo;t specify custom char filters, tokenizer, and token filter directly in the body of the request to the Analysis API.
To observe the output of text analysis that involves custom text analysis components first I have to create an index with an analyzer and then call Analyze API that involves that index. &lt;code>lmgrep&lt;/code> avoids this pain point by allowing to declare text analysis components inline.&lt;/p>
&lt;h2 id="post-script">Post Script&lt;/h2>
&lt;p>All this analyzer construction wizardry is possible because of Lucene&amp;rsquo;s &lt;code>AbstractAnalysisFactory&lt;/code> class and features provided by its subclasses. The &lt;a href="https://lucene.apache.org/core/8_3_1/analyzers-common/org/apache/lucene/analysis/custom/CustomAnalyzer.html">&lt;code>CustomAnalyzer&lt;/code>&lt;/a> builder exposes methods that expects a &lt;code>Class&lt;/code> as an argument, e.g. &lt;a href="https://lucene.apache.org/core/8_3_1/analyzers-common/org/apache/lucene/analysis/custom/CustomAnalyzer.Builder.html#addCharFilter-java.lang.Class-java.util.Map-">&lt;code>addCharFilter&lt;/code>&lt;/a>. The trick here is that, e.g. the class &lt;a href="https://lucene.apache.org/core/7_3_1/analyzers-common/org/apache/lucene/analysis/util/TokenFilterFactory.html">&lt;code>TokenFilterFactory&lt;/code>&lt;/a> provides a method &lt;code>availableTokenFilters&lt;/code> that returns a set of &lt;code>names&lt;/code> of token filters and with those &lt;code>names&lt;/code> you can get a &lt;code>Class&lt;/code> object that can be supplied to &lt;code>CustomAnalyzer&lt;/code> builder methods.&lt;/p>
&lt;p>The discovery of available factory classes is based on the classpath analysis, e.g. fetching all classes where name matches a pattern like &lt;code>.*FilterFactory&lt;/code> and are subclasses of a &lt;code>TokenFilterFactory&lt;/code>. However, for the reasons that were beyond my understanding, when I created my own &lt;code>TokenFilterFactory&lt;/code> class it was not discovered by Lucene &lt;code>Â¯\_(ã)_/Â¯&lt;/code>.&lt;/p>
&lt;p>Yeah, great, but &lt;code>lmgrep&lt;/code> is compiled with the GraalVM &lt;code>native-image&lt;/code> which assumes closed-world and throws the dynamism of the JVM out the window. How then does exactly this TokenFilterFactory-thing-class discovery works? Yes, Native images must include all the classes because at run-time it cannot create classes, but it can be worked around by providing the configuration with the classes that are going to be used at run-time, and those interesting classes can be reflectively discovered at run-time. &lt;code>lmgrep&lt;/code> relies on the Java classes being discoverable at compile-time where the reflection works as expected.&lt;/p>
&lt;p>To instruct the &lt;code>native-image&lt;/code> to discover the Java classes from Clojure code you can specify the class under the regular &lt;code>def&lt;/code> because to the &lt;code>native-image&lt;/code> &lt;code>def&lt;/code>s look like constants and are &lt;strong>evaluated&lt;/strong> at compiled-time. So, if &lt;code>lmgrep&lt;/code> misses some awesome Lucene token filter, all it takes is to add it to the hashmap under a &lt;code>def&lt;/code>.&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html">Lucene TokenStreams are actually graphs&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>If something is missing then let me know by creating an issue &lt;a href="https://github.com/dainiusjocas/lucene-grep/issues">here&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Just Google for &amp;ldquo;Lucene &amp;lt;COMPONENT_NAME&amp;gt;&amp;rdquo; &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>Silencing the Lenovo Thinkpad P53 on Linux</title><link>https://www.jocas.lt/blog/post/silencing-p53/</link><pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/silencing-p53/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>Lenovo Thinkpad P53 is a powerful laptop but a regular Linux install is just too loud for me. However, it is just a machine and there must be several control knobs that can make the experience better. In this post, I&amp;rsquo;ll share my setup that makes the laptop to be silent most of the time and performant when needed.&lt;/p>
&lt;h2 id="the-motivation">The motivation&lt;/h2>
&lt;p>To get my daily work done I spin many fat JVMs like Elasticsearch, Kafka, GraalVM native-image tool, &lt;code>docker-compose&lt;/code> clusters, virtual machines, etc. To plough through those tasks a year ago (in early 2020) I&amp;rsquo;ve got a top spec&amp;rsquo;ed Lenovo Thinkpad P53 laptop:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:right">Part&lt;/th>
&lt;th style="text-align:left">Spec&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:right">CPU&lt;/td>
&lt;td style="text-align:left">Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:right">Memory&lt;/td>
&lt;td style="text-align:left">128GB (not a mistake here)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:right">Graphics Processor&lt;/td>
&lt;td style="text-align:left">Nvidia Quadro RTX 4000&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The laptop(!) can be so power-hungry that the charger needs to provide up to 230W(!). Just imagine how loud the fans get when executing computationally intensive tasks. I simply have to leave the room and close the door until the task is finished.&lt;/p>
&lt;p>Anything that is CPU intensive makes the chip hot which spins the fans. By anything here I mean things like regular web browsing (looking at you JIRA), or a simple conference call (looking at you Google Meet and Zoom). I have some doubts that Lenovo has done the best possible job with the fans but I leave that on their conscience.&lt;/p>
&lt;p>Although I run a lot of intensive tasks on a laptop, not all of my tasks are that demanding. Therefore, I want the laptop to be performant when needed (in this case I don&amp;rsquo;t mind it to be loud) and be as silent as possible when that can be achieved. And one additional requirement: I want to work with a Linux machine.&lt;/p>
&lt;h2 id="prehistory">Prehistory&lt;/h2>
&lt;p>When I got the laptop the first thing I&amp;rsquo;ve done was that I&amp;rsquo;ve installed Ubuntu since it is officially supported&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. In a very short time, I&amp;rsquo;ve discovered that at that time it was impossible to silence the laptop to a satisfiable level: Linux just had no drivers that could silence all(!) the fans&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>.&lt;/p>
&lt;p>The hope was lost: I had to boot up Windows, setup WSL, and accommodate my development environment there. All because several drivers were missing (sigh). It&amp;rsquo;s worth to mention that I&amp;rsquo;ve set up a Windows box after more than 10 years of not touching Windows. It was just an OEM install with all the drivers, updates, etc. However, Windows setup had its own problems: for some reason (to me it was not a surprise) Windows 10 could not properly handle the sleep-wake-up cycle (its a laptop after all). But hey, I could disable the Nvidia GPU, then enable it, and everything would be working ok-ish again. I could get stuff done.&lt;/p>
&lt;p>However, updating Windows is a thing that can be postponed only that long and when you do an update with a setup that can be seen as being a little bit exotic you might get a broken system that fails to boot up. Yeah, sure, then just run the system fix scripts and you can continue working.&lt;/p>
&lt;p>Of course, it doesn&amp;rsquo;t help to have a good experience that it turned out that the laptop arrived with a faulty motherboard that caused all kinds of troubles among which the most painful was random shutdowns. Also, after those shutdowns sometimes even booting up was a real struggle. I guess that poor machine had only the best intentions and was telling me to stop torturing it and by failing to work gave me some well-deserved leisure time.&lt;/p>
&lt;p>One day the machine after a Windows update really got stuck during the reboot. A blank screen with fans maxed out and no reaction to any button or to power cable unplugging. Several Google searches away and I&amp;rsquo;ve discovered that it was a known problem with Thinkpad (not only P53). So, the motherboard got replaced and the laptop got a fair bit more stable.&lt;/p>
&lt;p>Also, even Windows that has supposedly good driver support from everyone (manufacturer, OS) involved has the power consumption and fan control options that are not all that powerful after all: there are predefined profiles with no manual tunning available. That makes me wonder how that software got accepted to be released in the first place?&lt;/p>
&lt;h2 id="the-linux-setup">The Linux Setup&lt;/h2>
&lt;p>The news somehow came to me that Linux kernel 5.11 has the support for the second fan that is present on some thinkpads&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. It encouraged me to roll up the sleeves and get the P53 fans under my control.&lt;/p>
&lt;p>The main pieces of the puzzle are the Linux kernel 5.11 and Kubuntu 21.04 pre-released: the kernel has the fans driver out of the box, and Kubuntu 21.04 provides Nvidia drivers that work with that kernel.&lt;/p>
&lt;p>Also, there are a couple helper utilities that I&amp;rsquo;ve used:&lt;/p>
&lt;ul>
&lt;li>Fancontrol GUI&lt;/li>
&lt;li>thinkfan (with GUI)&lt;/li>
&lt;li>CPU Power GUI&lt;/li>
&lt;/ul>
&lt;h3 id="linux-kernel">Linux kernel&lt;/h3>
&lt;p>I&amp;rsquo;ve installed 5.11 on Ubuntu 20.10 and it shouldn&amp;rsquo;t come to you as a surprise that Nvidia drivers were broken and the install script from the Nvidia website failed. So, a failed experiment.&lt;/p>
&lt;p>I&amp;rsquo;ve used the &lt;a href="https://ubuntuhandbook.org/index.php/2020/08/mainline-install-latest-kernel-ubuntu-linux-mint/">Mainline tool&lt;/a> to install the kernel.&lt;/p>
&lt;h3 id="kubuntu-2104">Kubuntu 21.04&lt;/h3>
&lt;p>When all hope was lost and I was about to go back to Windows for another half a year I got the last thing to try: why not upgrade Kubuntu to 21.04 which is not even in BETA? It would come with a newer kernel and most likely with Nvidia driver support.&lt;/p>
&lt;p>It came with kernel 5.10 and Nvidia drivers worked. Unfortunately, the second fan was not detected. Another failed experiment.&lt;/p>
&lt;p>The last hope was to ask Mainline to install kernel 5.11 and pray that Nvidia drivers are fine with it. Install, reboot, and voila: Nvidia driver works, an external monitor is detected (though only when connected with HDMI), and most importantly both fans are detected and controllable! Hurray!&lt;/p>
&lt;h3 id="fancontrol">Fancontrol&lt;/h3>
&lt;p>&lt;a href="https://github.com/Maldela/fancontrol-gui">Fancontrol&lt;/a> is a GUI utility that allows you to control and set up fan profiles by adjusting two dots: for example, the temperature at which to start/stop spinning the fans and when to max them out.&lt;/p>
&lt;p>&lt;img src="fancontrol-system-settings.png" alt="Fancontrol in KDE System Setting">&lt;/p>
&lt;p>Note: I&amp;rsquo;ve installed it from the source code.
Note: there is a standalone GUI but I like the integration into the KDE System Settings.&lt;/p>
&lt;h3 id="thinkfan">Thinkfan&lt;/h3>
&lt;p>Check the set up instructions &lt;a href="https://gist.github.com/Yatoom/1c80b8afe7fa47a938d3b667ce234559">here&lt;/a>.&lt;/p>
&lt;p>There is even the GUI for the &lt;a href="https://github.com/scientifichackers/thinkfan-control-gui">thinkfan&lt;/a>. This will give a window like this:&lt;/p>
&lt;p>&lt;img src="thinkfan-gui.png" alt="thinkfan GUI">&lt;/p>
&lt;p>Note the two lines that starts with &lt;code>Fan&lt;/code>. It means that two Thinkpad fans are detected and controlable.&lt;/p>
&lt;p>Note that with the &lt;code>thinkfan&lt;/code> it is up to you to write the settings into the file.&lt;/p>
&lt;h3 id="cpu-power-gui">CPU Power GUI&lt;/h3>
&lt;p>&lt;a href="https://ubuntuhandbook.org/index.php/2020/11/cpupower-gui-simple-tool-adjust-cpu-frequency/">CPU Power GUI&lt;/a> is a simple graphical utility that allows you to change the frequency limits of your CPU and its governor:
&lt;img src="cpupower-gui.png" alt="CPU frequency settings">&lt;/p>
&lt;p>Install it from the official Ubuntu repositories.&lt;/p>
&lt;pre>&lt;code class="language-bash">sudo apt install cpupower-gui
&lt;/code>&lt;/pre>
&lt;p>The trick here is that high CPU frequency leads to lots of heat which in turn spins the fans. When the frequency is limited not that much of the heat is created which prevents fans from kicking in. I&amp;rsquo;ve noticed that the minimal frequency (800 MHz) is enough to get some work done (e.g. zoom call). The temperature rarely goes up more than 60 degrees celsius.&lt;/p>
&lt;p>Of course, with a limited CPU frequency, the machine is noticeably slower, but hey, it is a high-end CPU. The slowdown is most noticeable when starting up an app. For example, IntelliJ works but feels bad with the 800MHz limit.&lt;/p>
&lt;h3 id="greenwithenvy">GreenWithEnvy&lt;/h3>
&lt;p>For the Nvidia fans the &lt;a href="https://gitlab.com/leinardi/gwe">GreenWithEnvy&lt;/a> looks promissing but I guess that the Quadro cards are not supported.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>It was a year-long development. Given the current setup, the tasks that are not super demanding can be done on a quiet machine where the only things I hear are my keyboard and the little Marshall Emberton speaker playing my favourite &lt;a href="https://www.youtube.com/watch?v=ZkW-K5RQdzo">Rammstein&lt;/a> &lt;a href="https://www.youtube.com/watch?v=NeQM1c-XCDc">tunes&lt;/a>. When my crazy performance test is set up, I allow the fans to spin at full speed and get the task done as fast as it can.&lt;/p>
&lt;h2 id="ps-nvidia-and-linux">P.S. Nvidia and Linux&lt;/h2>
&lt;p>Yeah, a laptop with an Nvidia GPU should be a red flag that prevents installing Linux in the first place. Of course, there are drivers and stuff, but in general good luck to have them properly working. However, Ubuntu gives you the setup that is worked out and works by default.&lt;/p>
&lt;p>Also, there are open source drivers called Nouveau but they have problems of their own like external monitor support.&lt;/p>
&lt;p>What is worse with this P53 is that &lt;strong>ALL&lt;/strong> video outputs are attached to the discrete GPU (what engineers in their right mind would do it?). This means that if drivers are not properly set-up Linux will able to output only 1 FPS while (for some reason) the mouse and some other windows (sometimes?) works without any lag.&lt;/p>
&lt;p>Sure, things like &lt;a href="https://www.nvidia.com/en-us/geforce/technologies/optimus/technology/">Optimus&lt;/a>, &lt;a href="https://forums.developer.nvidia.com/t/the-all-new-outputsink-feature-aka-reverse-prime/129828">reverse PRIME&lt;/a> exist but I can only wish you good luck setting up these.&lt;/p>
&lt;p>In summary, I wish there was a modification of the laptop that had only the integrated graphics.&lt;/p>
&lt;h2 id="pss-setup-on-the-table">P.S.S. Setup on the table&lt;/h2>
&lt;p>P53 is a big and fat laptop. Since most of the time I work from home due to covid-19, I don&amp;rsquo;t need to move or touch the machine that often. Therefore, it can stand on the table as a &amp;ldquo;triangle&amp;rdquo; (see a picture). This position allows air to circulate a little better. Also, in this way, it takes somewhat less space on the table. On the other hand, I can&amp;rsquo;t use the laptop screen and the keyword.&lt;/p>
&lt;p>&lt;img src="thinkpad-setup.jpg" alt="My Table setup">&lt;/p>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="https://certification.ubuntu.com/hardware/202002-27747">https://certification.ubuntu.com/hardware/202002-27747&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>I&amp;rsquo;ve tried several other distros (like Fedora 33) but without any luck. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>&lt;a href="https://www.phoronix.com/scan.php?page=news_item&amp;amp;px=ThinkPad-Dual-Fan-Control-5.8">https://www.phoronix.com/scan.php?page=news_item&amp;amp;px=ThinkPad-Dual-Fan-Control-5.8&lt;/a> &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>lmgrep - Lucene Based grep-like Utility</title><link>https://www.jocas.lt/blog/post/intro-to-lucene-grep/</link><pubDate>Sat, 30 Jan 2021 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/intro-to-lucene-grep/</guid><description>&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>What if &lt;code>grep&lt;/code> supported the functionality of a proper search engine like &lt;a href="https://www.elastic.co/elasticsearch/">Elasticsearch&lt;/a> without a need to install any servers or index the files before searching?
&lt;a href="https://github.com/dainiusjocas/lucene-grep">&lt;code>lmgrep&lt;/code>&lt;/a> aims to provide you just that.
It is installed as just one executable file without any dependencies, provides a command-line interface, starts-up instantly, and works on macOS, Linux, and, yes, even Windows.&lt;/p>
&lt;p>See the source code &lt;a href="https://github.com/dainiusjocas/lucene-grep">here&lt;/a>.&lt;/p>
&lt;h2 id="my-motivation">My motivation&lt;/h2>
&lt;p>Have you ever wished that &lt;code>grep&lt;/code> supported &lt;a href="https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation">tokenization&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Stemming">stemming&lt;/a>, etc, so that you don&amp;rsquo;t have to write wildcard &lt;a href="https://en.wikipedia.org/wiki/Regular_expression">regular expressions&lt;/a> all the time? I&amp;rsquo;ve also shared that question and on a one nice day, I&amp;rsquo;ve tried to scratch that itch by exposing the &lt;a href="https://lucene.apache.org/">Lucene&lt;/a> query syntax as a CLI utility. &lt;code>lmgep&lt;/code> is the result of my effort. &lt;a href="https://github.com/dainiusjocas/lucene-grep">Give it a try&lt;/a> and let me know how it goes.&lt;/p>
&lt;h2 id="full-text-search-vs-grep">Full-text Search vs. &lt;code>grep&lt;/code>&lt;/h2>
&lt;p>I&amp;rsquo;m perfectly aware that comparing Lucene and &lt;code>grep&lt;/code> is like comparing apples to oranges. However, I think that &lt;code>lmgrep&lt;/code> is best compared with the very tool that inspired it, namely &lt;code>grep&lt;/code>.&lt;/p>
&lt;p>Anyway, what does &lt;code>grep&lt;/code> do? &lt;code>grep&lt;/code> reads a line from &lt;code>stdin&lt;/code>, examines the line to see if it should be forwarded to &lt;code>stdout&lt;/code>, and repeats until stdin is exhausted&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. &lt;code>lmgrep&lt;/code> tries to mimick exactly that functionality. Of course, there are many more options to &lt;code>grep&lt;/code> but it is the essence of the tool.&lt;/p>
&lt;p>Several notable advantages of &lt;code>lmgrep&lt;/code> over &lt;code>grep&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Lucene query syntax is better suited for full-text search;&lt;/li>
&lt;li>Boolean operators allow to construct complex, well-designed queries;&lt;/li>
&lt;li>Text analysis can be customized to the language of the documents;&lt;/li>
&lt;li>Fuzzy text searches;&lt;/li>
&lt;li>Flexible text analysis pipeline that includes, lowercasing, &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-asciifolding-tokenfilter.html">ASCII-folding&lt;/a>, stemming, etc;&lt;/li>
&lt;li>regular expressions can be combined with other Lucene query components;&lt;/li>
&lt;li>Search matches can span multiple lines, i.e. search is not line-oriented.&lt;/li>
&lt;/ul>
&lt;p>Several notable limitations of &lt;code>lmgrep&lt;/code> when compared to &lt;code>grep&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>grep&lt;/code> is faster when it comes to raw speed for large text files;&lt;/li>
&lt;li>&lt;code>grep&lt;/code> has a smaller memory footprint;&lt;/li>
&lt;li>Not all options of &lt;code>grep&lt;/code> are supported;&lt;/li>
&lt;/ul>
&lt;h2 id="why-lucene">Why Lucene?&lt;/h2>
&lt;p>Lucene is a Java library that provides indexing and search features. Lucene has been more than 20 years in development and it is the library that powers many search applications. Also, many developers are already familiar with the Lucene query syntax and know how to leverage it to solve complicated information retrieval problems.&lt;/p>
&lt;p>However powerful Lucene is, it is not well-suited for CLI application. The main problem is the startup time of JVM. To reduce the startup time I&amp;rsquo;ve compiled &lt;code>lmgrep&lt;/code> with the &lt;code>native-image&lt;/code> tool provided by &lt;a href="https://www.graalvm.org/">GraalVM&lt;/a>. In this way, the startup time is around 0.01s for Linux, macOS, and Windows.&lt;/p>
&lt;h2 id="how-does-lmgrep-work">How does &lt;code>lmgrep&lt;/code> work?&lt;/h2>
&lt;p>&lt;code>lmgrep&lt;/code> by default expects two parameters: a search query and a &lt;a href="https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystem.html#getPathMatcher-java.lang.String-">GLOB pattern&lt;/a> (similar to regexp) to find files to execute &lt;code>lmgrep&lt;/code> on. I assume that the dear reader doesn&amp;rsquo;t want to be tortured by reading the explanation on how the file names are being matched with GLOB, so I&amp;rsquo;ll skip it. Instead, I&amp;rsquo;ll focus on explaining how the search works within a file.&lt;/p>
&lt;p>&lt;code>lmgrep&lt;/code> creates a &lt;a href="https://lucene.apache.org/core/8_7_0/monitor/org/apache/lucene/monitor/Monitor.html">Lucene Monitor (Monitor)&lt;/a> object from the provided search query. Then text file is split into lines&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Each line of text is passed to the Monitor for searching. The Monitor then creates an in-memory Lucene index with a single document created out of the line of text. Then the Monitor runs the search query on that in-memory index in the good ol' Lucene way&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>. &lt;code>lmgrep&lt;/code> takes the hits, formats them, and sends results to &lt;code>STDOUT&lt;/code>. That is how &lt;code>lmgrep&lt;/code> does the full-text search.&lt;/p>
&lt;p>The overall searching approach is similar to the one of &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-percolate-query.html">Percolator&lt;/a> in Elasticsearch. &lt;code>lmgrep&lt;/code> just limits the number of stored search queries to one and treats every text line as a document. A cool thing compared with the Percolator is that &lt;code>lmgrep&lt;/code> provides exact offsets of the matched terms while Elasticsearch does not expose offsets when highlighting.&lt;/p>
&lt;p>The described procedure seems to be somewhat inefficient. However, the &lt;strong>query parsing&lt;/strong> for &lt;strong>all&lt;/strong> the lines (and files) is done only once. Also, the searching itself is efficient thanks to Lucene in general and when queries are complicated thanks to the &lt;a href="https://lucene.apache.org/core/8_2_0/monitor/org/apache/lucene/monitor/Presearcher.html">Presearcher&lt;/a> of the Lucene Monitor in particular. Presearcher extracts terms from the search query and if none of these terms are in the index then a full query is not executed at all. Of course, many optimizations can be (and will be) implemented for &lt;code>lmgrep&lt;/code> such as batching of the documents. In general, the performance is limited by the Lucene Monitor.&lt;/p>
&lt;p>What about the text analysis pipeline? By default, &lt;code>lmgrep&lt;/code> uses the &lt;code>StandardTokenizer&lt;/code> to tokenize text. Then the tokens are passed through several token filters in the following order: &lt;code>LowerCaseFilter&lt;/code>, &lt;code>ASCIIFoldingFilter&lt;/code>, and &lt;code>SnowballFilter&lt;/code> which is given the &lt;code>EnglishStemmer&lt;/code>. The same analysis pipeline is used for both the indexing and querying. All the components of the analysis pipeline are configurable via CLI flags, see the &lt;a href="https://github.com/dainiusjocas/lucene-grep/blob/main/README.md#supported-tokenizers">README&lt;/a>. However, the order of the token filters, as of now, is not configurable. Moreover, various filters are not exposed at all (e.g. &lt;code>StopwordsFilter&lt;/code>, or &lt;a href="https://lucene.apache.org/core/7_4_0/analyzers-common/org/apache/lucene/analysis/miscellaneous/WordDelimiterGraphFilter.html">WordDelimiterGraphFilter&lt;/a>, etc.). Supporting a more flexible analysis pipeline configuration is left out for future releases. The more users the tool has the faster new features will be implemented ;)&lt;/p>
&lt;h2 id="prehistory-of-the-lmgrep">Prehistory of the &lt;code>lmgrep&lt;/code>&lt;/h2>
&lt;p>Almost every NLP project that I&amp;rsquo;ve worked on had the component called &lt;strong>dictionary annotator&lt;/strong>. Also, the vast majority of the projects used Elasticsearch in one way or another. The more familiar I&amp;rsquo;ve got with Elasticsearch I&amp;rsquo;ve got, the more of my NLP workload shifted towards implementing it inside Elasticsearch. One day I&amp;rsquo;ve discovered a tool called &lt;a href="https://github.com/flaxsearch/luwak">Luwak&lt;/a> (a cool name isn&amp;rsquo;t it?) and read &lt;a href="https://web.archive.org/web/20201124175132/https://www.flax.co.uk/blog/2016/03/08/helping-bloomberg-build-real-time-news-search-engine/">more about it&lt;/a>. It kind of opened my eyes: the dictionary annotator can be implemented using Elasticsearch and the dictionary entries can be expressed as Elasticsearch queries. Thankfully, Elasticsearch has Percolator that hides all the complexity of managing temporary indices, batching search requests, etc.&lt;/p>
&lt;p>Then I was given was an NLP project where one of the requirements was to implement data analysis using AWS serverless stuff: Lambda for text processing and Dynamo DB for storage. Of course, one of the required NLP components was a dictionary annotator. Since Elasticsearch was not available (because it is not serverless) I still wanted to continue working with dictionary entries as search queries, I&amp;rsquo;ve decided to leverage the Luwak library. From experiences of that project, the &lt;a href="https://web.archive.org/web/20201124175132/https://www.flax.co.uk/blog/2016/03/08/helping-bloomberg-build-real-time-news-search-engine/">Beagle&lt;/a> library was born. &lt;code>lmgrep&lt;/code> is loosely based on Beagle.&lt;/p>
&lt;p>When thinking about how to implement &lt;code>lmgrep&lt;/code> I wanted it to be based on Lucene because of the full-text search features. To provide a good experience the start-up time must be small. To achieve it, &lt;code>lmgrep&lt;/code> had to be compiled with the &lt;code>native-image&lt;/code> tool of the GraalVM. I&amp;rsquo;ve tried but the &lt;code>native-image&lt;/code> doesn&amp;rsquo;t support &lt;a href="https://web.archive.org/web/20201124175132/https://www.flax.co.uk/blog/2016/03/08/helping-bloomberg-build-real-time-news-search-engine/">Method Handles&lt;/a> that Lucene uses. Some more hacking was needed. I was lucky when I&amp;rsquo;ve discovered a &lt;a href="https://web.archive.org/web/2/https://www.morling.dev/blog/how-i-built-a-serverless-search-for-my-blog/">toy project&lt;/a> where the blog search was implemented on AWS Lambda that was backed by Lucene which was compiled by the &lt;code>native-image&lt;/code> tool. I&amp;rsquo;ve cloned the repo, &lt;code>mvnw install&lt;/code>, then included the artefacts to the dependencies list, and &lt;code>lmgrep&lt;/code> compiled with the &lt;code>native-image&lt;/code> tool successfully.&lt;/p>
&lt;p>Then the most complicated part was to prepare executable binaries for different operating systems. Plenty of CPU, RAM, VirtualBox with Windows and macOS virtual machines, and &lt;a href="https://github.com/dainiusjocas/lucene-grep/releases/tag/v2021.01.24">here we go&lt;/a>.&lt;/p>
&lt;p>Did I say how much I enjoyed trying to get stuff done on Windows? None at all. How come that multiple different(!) command prompts are needed to get GraalVM to compile an executable? Now I know that it would a lot better to suffer the pain and to set up the Github Actions pipeline to compile the binaries and upload them to release pages.&lt;/p>
&lt;h2 id="what-is-missing">What is missing?&lt;/h2>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> The analysis pipeline is not as flexible as I&amp;rsquo;d like to (UPDATE 2021-04-24: &lt;a href="https://github.com/dainiusjocas/lucene-grep/pull/81">implemented&lt;/a>);&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Leverage the multicore CPUs by executing the search in parallel;&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Batch documents for matching;&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Let me know if any?&lt;/li>
&lt;/ul>
&lt;h2 id="what-would-be-cool-ways-to-use-lmgrep">What would be cool ways to use &lt;code>lmgrep&lt;/code>?&lt;/h2>
&lt;ul>
&lt;li>&lt;code>tail&lt;/code> logs to &lt;code>lmgrep&lt;/code> and raise alerts;&lt;/li>
&lt;li>Give an alias for &lt;code>lmgrep&lt;/code> with various options tailored for the code search (Java Example);&lt;/li>
&lt;li>Why not expose &lt;a href="https://github.com/borkdude/sci">sci&lt;/a> script as TokenFilter?&lt;/li>
&lt;li>Why not &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenfilter.html">ngrams token filter&lt;/a> then the search would be resilient to the typing errors?&lt;/li>
&lt;li>Static website search, like AWS Lambda that has lmgrep and goes through all files on demand without upfront indexing.&lt;/li>
&lt;/ul>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>&lt;code>lmgrep&lt;/code> scratched my itch. It was exciting to get it working. I hope that you&amp;rsquo;ll also find it interesting and maybe useful. Give it a try, let me know how it was for you, and most importantly any feedback welcome on how to improve &lt;code>lmgrep&lt;/code>.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://web.archive.org/web/20210116173133/https://swtch.com/~rsc/regexp/regexp4.html">https://web.archive.org/web/20210116173133/https://swtch.com/~rsc/regexp/regexp4.html&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://web.archive.org/web/20161018234331/http://www.techrepublic.com/article/graduating-from-grep-powerful-text-file-searching-with-isearch/">https://web.archive.org/web/20161018234331/http://www.techrepublic.com/article/graduating-from-grep-powerful-text-file-searching-with-isearch/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="footnotes">Footnotes&lt;/h2>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>&lt;a href="https://ideolalia.com/essays/composition-is-interpretation.html">https://ideolalia.com/essays/composition-is-interpretation.html&lt;/a> &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>there is no necessity to split text files into lines, it is just to mimik how &lt;code>grep&lt;/code> operates. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>of course, the description is over-simplified, but it is accurate enough to get the overall idea. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>How to Prevent Data Corruption in Elasticsearch When Using Kafka Connect Elasticsearch Sink Connector</title><link>https://www.jocas.lt/blog/post/kc_es_data_consistency/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/kc_es_data_consistency/</guid><description>&lt;h3 id="tldr">TL;DR&lt;/h3>
&lt;p>When the &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch">Elasticsearch indexer&lt;/a> is highly concurrent, Kafka record keys are used as Elasticsearch document IDs, and indexer is set to delete records on &lt;code>null&lt;/code> values, then Kafka Connect Elasticsearch Sink Connector might corrupt your data: documents that should not be deleted end up being deleted, or documents that should be deleted end up still being present in the index. The fix is to use external versioning for deletes in bulk requests as it is proposed in this &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/pull/422">Github Pull Request&lt;/a>.&lt;/p>
&lt;h3 id="the-problem">The problem&lt;/h3>
&lt;p>NOTE: as of version 6.0.0 of the Confluent Platform (last checked on 2020-10-02) the bug that might lead to data corruption is still present.&lt;/p>
&lt;p>Let&amp;rsquo;s focus on a use case where Kafka record key is used as an Elasticsearch document ID&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>. I would consider this to be a proper practice when the documents represent a catalog of things.&lt;/p>
&lt;p>Elasticsearch uses &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/optimistic-concurrency-control.html">optimistic concurrency control&lt;/a>. The job of this concurrency mechanism is to ensure that older version of the document doesn&amp;rsquo;t override a newer version. By default, order of arrival of the operation is applied, but the behaviour can be overriden in &lt;a href="https://www.elastic.co/blog/elasticsearch-versioning-support">several ways&lt;/a> depending on the version of Elasticsearch. In this post we focus on concurrent bulk requests, and with a concurrency that involves a network, requests will sometimes arrive out of order.&lt;/p>
&lt;p>To help Elasticsearch resolve the out-of-order indexing requests Kafka Connect Elasticsearch Sink Connector (from here on Kafka Connect for short) leverages the &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/blob/7256e9473cea690c373058b88fffd1111870cfe6/src/main/java/io/confluent/connect/elasticsearch/jest/JestElasticsearchClient.java#L564">&lt;code>external&lt;/code> document&lt;/a> versioning&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>. Using external versions in Kafka Connect makes sense because we already have versioning in place: Kafka topic partition offsets. If Kafka Connect applies changes to Elasticsearch indices in order of the topic offset, then any update ordering problems would be problems in the upstream system. This is a good guarantee to have.&lt;/p>
&lt;p>Let&amp;rsquo;s add to the mix delete operations. Kafka Connect supports a setting &lt;code>BEHAVIOR_ON_NULL_VALUES_CONFIG&lt;/code> to &lt;code>&amp;quot;delete&amp;quot;&lt;/code>. This setting instructs the Kafka Connect that a document in Elasticsearch with an ID of the kafka record key with &lt;code>null&lt;/code> value (a tombstone message) is going to be deleted. But for some strange reason the deletes &lt;strong>does not use external versioning&lt;/strong>! The line responsible for the described behaviour&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> can be found &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/blob/7256e9473cea690c373058b88fffd1111870cfe6/src/main/java/io/confluent/connect/elasticsearch/jest/JestElasticsearchClient.java#L554">here&lt;/a>. This means that for deletes the order-of-arrival wins. Let&amp;rsquo;s increase the concurrency of bulk requests with the param &lt;code>MAX_IN_FLIGHT_REQUESTS_CONFIG&lt;/code> to a largish number, and the data consistency problems is just round the corner for data that has some largish update ratio.&lt;/p>
&lt;p>The issue is even more pronounced when you re-index data into Elasticsearch and you want to do it as fast as possible, which means doing the indexing concurrently.&lt;/p>
&lt;h3 id="the-example">The Example&lt;/h3>
&lt;p>The code that demonstrated the faulty behaviour can be found in this &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/pull/422">Pull Request&lt;/a>.&lt;/p>
&lt;p>The test case is for testing the case when document should be present in Elasticsearch gets deleted.&lt;/p>
&lt;p>Let&amp;rsquo;s have a little walk over the code snippet:&lt;/p>
&lt;pre>&lt;code class="language-java">Collection&amp;lt;SinkRecord&amp;gt; records = new ArrayList&amp;lt;&amp;gt;();
for (int i = 0; i &amp;lt; numOfRecords - 1 ; i++) {
if (i % 2 == 0) {
SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, null, i);
records.add(sinkRecord);
} else {
record.put(&amp;quot;message&amp;quot;, Integer.toString(i));
SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, i);
records.add(sinkRecord);
}
}
record.put(&amp;quot;message&amp;quot;, Integer.toString(numOfRecords));
SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, numOfRecords);
records.add(sinkRecord);
task.put(records);
task.flush(null);
&lt;/code>&lt;/pre>
&lt;p>Here we send &lt;code>numOfRecords&lt;/code> (which larger than 2) to a Kafka topic. Every second record has &lt;code>null&lt;/code> body (delete operation), and the rest of the records have a sequence number as a &lt;code>message&lt;/code> value. The very last record is &lt;strong>always&lt;/strong> a non-null record with a &lt;code>message&lt;/code> value of &lt;code>numOfRecords&lt;/code>.&lt;/p>
&lt;p>Let&amp;rsquo;s setup a connector:&lt;/p>
&lt;pre>&lt;code class="language-java">KEY_IGNORE_CONFIG = &amp;quot;false&amp;quot;;
MAX_IN_FLIGHT_REQUESTS_CONFIG = Integer.toString(numOfRecords)
BATCH_SIZE_CONFIG = &amp;quot;1&amp;quot;
LINGER_MS_CONFIG = &amp;quot;1&amp;quot;
BEHAVIOR_ON_NULL_VALUES_CONFIG = &amp;quot;delete&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Here we set a connector to use Kafka record key as id &lt;code>KEY_IGNORE_CONFIG = &amp;quot;false&amp;quot;&lt;/code>, set the indexer concurrency to the &lt;code>numOfRecords&lt;/code>; set the indexing batch size to 1 (this creates as many requests to Elasticsearch as there are records in the Kafka topic); set indexer to send requests immediately with &lt;code>LINGER_MS_CONFIG = &amp;quot;1&amp;quot;&lt;/code>; and record with a &lt;code>null&lt;/code> value represents a delete operation.&lt;/p>
&lt;p>With this setup after the indexing is done we expect that in the index we have a document with ID and whose &lt;code>message&lt;/code> value is &lt;code>numOfRecords&lt;/code>. But when ordering of bulk requests is out-of-order then at the end we might have a situation where there is no document in the index at all: the bulk index request with &lt;code>message = numOfRecords&lt;/code> arrived before one of the bulk requests with a delete operation!&lt;/p>
&lt;p>The situation might seem to be a bit far-fetched but for applications like e-commerce where you have a catalog that is frequently updated (e.g. the catalog item should be available in search or not) and updates are modelled as document deletes it happens a bit more often than it might be expected.&lt;/p>
&lt;h3 id="the-fix">The fix&lt;/h3>
&lt;p>The fix is simple: use the same external versioning that is already being used by the indexing requests also for delete requests:&lt;/p>
&lt;pre>&lt;code class="language-java">if (record.version != null) {
req.setParameter(&amp;quot;version_type&amp;quot;, &amp;quot;external&amp;quot;).setParameter(&amp;quot;version&amp;quot;, record.version);
}
&lt;/code>&lt;/pre>
&lt;p>The full code can be found &lt;a href="ttps://github.com/confluentinc/kafka-connect-elasticsearch/pull/422">here&lt;/a>. Let&amp;rsquo;s hope that Confluent developers will find some time to merge that PR.&lt;/p>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>Thank you for reading and leave your feedback &lt;a href="https://github.com/dainiusjocas/blog/issues/12">here&lt;/a>.&lt;/p>
&lt;h3 id="ps">P.S.&lt;/h3>
&lt;p>Of course, this is not the only situation when data can get &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/issues">corrupted&lt;/a>, e.g. changing the number of partitions; when you delete the topic, repopulate it with up-to-date data (also, you skip deletes) then restarting the indexing might pretty much nothing, because all the versions are earlier &lt;code>external version&lt;/code> because offsets are smaller.&lt;/p>
&lt;h3 id="heading">&lt;/h3>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>When Kafka Record keys are not used as Elasticsearch document IDs versioning is not a problem because every Elasticsearch ID is constructed as &lt;code>{topic}+{partition}+{offset}&lt;/code> which creates a new document for every Kafka record, i.e. no versioning is needed. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>Elasticsearch 7 supports the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/breaking-changes-7.0.html#_internal_versioning_is_no_longer_supported_for_optimistic_concurrency_control">external versioning&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>Yes, it is a comment, and it means that the developers were not sure whether to use external versioning for delete operations. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>How to Use Elasticsearch Ingest Pipelines with Kafka Connect Elasticsearch Sink Connector</title><link>https://www.jocas.lt/blog/post/ingest_pipeline_kafka_connect/</link><pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/ingest_pipeline_kafka_connect/</guid><description>&lt;h3 id="tldr">TL;DR&lt;/h3>
&lt;p>Specify your pipeline with the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#dynamic-index-settings">&lt;code>index.default_pipeline&lt;/code>&lt;/a> setting in the index (or index template) settings.&lt;/p>
&lt;h3 id="the-problem">The Problem&lt;/h3>
&lt;p>We need to index the log data into the &lt;a href="https://www.elastic.co/">Elasticsearch&lt;/a> cluster using a &lt;a href="https://docs.confluent.io/current/connect/kafka-connect-elasticsearch/index.html">Kafka Connect Elasticsearch Sink Connector&lt;/a> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>, the data should be split into daily indices, and we need to specify the Elasticsearch ingest pipeline.&lt;/p>
&lt;p>The &lt;a href="https://docs.confluent.io/current/connect/kafka-connect-elasticsearch/configuration_options.html">documentation of the connector&lt;/a> doesn&amp;rsquo;t mention anything about ingest pipelines. After a quick consultation with the Internet you discover that there is an open &lt;a href="https://github.com/confluentinc/kafka-connect-elasticsearch/issues/72">issue&lt;/a> that Kafka Connect Elasticsearch Sink Connector doesn&amp;rsquo;t support specifying an Elasticsearch ingest pipeline. WAT?&lt;/p>
&lt;h3 id="the-workaround">The Workaround&lt;/h3>
&lt;p>Say&lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup>, our pipeline&lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup> just renames an attribute, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-shell">PUT _ingest/pipeline/my_pipeline_id
{
&amp;quot;description&amp;quot; : &amp;quot;renames the field name&amp;quot;,
&amp;quot;processors&amp;quot; : [
{
&amp;quot;rename&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;original_field_name&amp;quot;,
&amp;quot;target_field&amp;quot;: &amp;quot;target_field_name&amp;quot;
}
}
]
}
&lt;/code>&lt;/pre>
&lt;p>The Elasticsearch ingest pipeline for indexing can be specified in several ways:&lt;/p>
&lt;ol>
&lt;li>for each index request as a &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html">URL parameter&lt;/a>,&lt;/li>
&lt;li>per bulk index request as a URL parameter,&lt;/li>
&lt;li>for every &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html#docs-bulk-api-query-params">bulk index request operation&lt;/a>,&lt;/li>
&lt;li>index settings (&lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules.html#dynamic-index-settings">a dynamic attribute&lt;/a>),&lt;/li>
&lt;li>index template.&lt;/li>
&lt;/ol>
&lt;p>First three options are not supported by Kafka Connect. The fourth option is not convenient in our case because the data should be split into time-based (e.g. daily) indices and we don&amp;rsquo;t want to do repetitive tasks&lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup>. The natural option to follow is to define an &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-templates.html">index template&lt;/a>. In the index template we can specify the &lt;code>index.default_pipeline&lt;/code> parameter, e.g.&lt;/p>
&lt;pre>&lt;code class="language-shell">PUT _index_template/template_1
{
&amp;quot;index_patterns&amp;quot;: [&amp;quot;daily_log*&amp;quot;],
&amp;quot;template&amp;quot;: {
&amp;quot;settings&amp;quot;: {
&amp;quot;index.default_pipeline&amp;quot;: &amp;quot;my_pipeline_id&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note, that for indexing not to fail, we should create the Elasticsearch ingest pipeline&lt;sup id="fnref:5">&lt;a href="#fn:5" class="footnote-ref" role="doc-noteref">5&lt;/a>&lt;/sup> &lt;strong>before&lt;/strong> setting up the index template.&lt;/p>
&lt;p>That is it, now when Kafka Connect will create a new daily index the Elasticsearch ingest pipeline is going to be applied to every document without any issues, for free, and in no time.&lt;/p>
&lt;h3 id="bonus">Bonus&lt;/h3>
&lt;p>One thing to note is that only one pipeline can be specified for &lt;code>index.default_pipeline&lt;/code> which might sound a bit limiting. A clever trick to overcome that limitation is to use a series of &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/pipeline-processor.html">pipeline processors&lt;/a> that can invoke other pipelines in the specified order, i.e. pipeline of pipelines.&lt;/p>
&lt;p>Also, there is an index setting called &lt;code>index.final_pipeline&lt;/code> that if specified is going to be executed after all other pipelines.&lt;/p>
&lt;p>Testing pipelines can be done using the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/simulate-pipeline-api.html">&lt;code>_simulate&lt;/code> API&lt;/a>.&lt;/p>
&lt;h3 id="fin">Fin&lt;/h3>
&lt;p>Thanks for reading and leave comments or any other feedback on this blog post in the &lt;a href="https://github.com/dainiusjocas/blog/issues/9">Github issue&lt;/a>. Examples were tested to work with Elasticsearch and Kibana 7.8.1.&lt;/p>
&lt;section class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1" role="doc-endnote">
&lt;p>or any other technology that doesn&amp;rsquo;t support, or it is just not possible to specify the Elasticsearch ingest pipeline. &lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2" role="doc-endnote">
&lt;p>yes, I know that the same job can be done with the &lt;a href="https://docs.confluent.io/current/connect/transforms/index.html">Kafka Connect Transformations&lt;/a>. &lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3" role="doc-endnote">
&lt;p>let&amp;rsquo;s leave out the Kafka Connector setup. &lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4" role="doc-endnote">
&lt;p>set &lt;code>index.default_pipeline=my_pipeline_id&lt;/code> for every new daily index with, say, a cron-job at midnight. &lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:5" role="doc-endnote">
&lt;p>technically, before an index is created that matches the template pattern. &lt;a href="#fnref:5" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/section></description></item><item><title>A Neat Trick with Elasticsearch Normalizers</title><link>https://www.jocas.lt/blog/post/elasticsearch-normlizers/</link><pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/elasticsearch-normlizers/</guid><description>&lt;p>To analyze the textual data Elasticsearch uses &lt;strong>analyzers&lt;/strong> while for the keyword analysis there is a thing called a &lt;strong>normalizer&lt;/strong>. In this article I&amp;rsquo;ll explain what the normalizer is and show it&amp;rsquo;s use case for &lt;strong>normalizing&lt;/strong> URLs.&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>A neat use case for keyword normalizers is to extract a specific part of the URL with a char_filter of the &lt;code>pattern_replace&lt;/code> type.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In Elasticsearch the textual data is represented with two data types: &lt;code>text&lt;/code> and &lt;code>keyword&lt;/code>. The &lt;code>text&lt;/code> type is meant to be used for full-text search use cases while &lt;code>keyword&lt;/code> is mean for filtering, sorting, and aggregation.&lt;/p>
&lt;h3 id="tldr-about-analyzers">TL;DR About Analyzers&lt;/h3>
&lt;p>To make a better use of &lt;code>text&lt;/code> data you can setup the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer-anatomy.html">analyzer&lt;/a> which is a combination of three components:&lt;/p>
&lt;ul>
&lt;li>exactly one &lt;strong>tokenizer&lt;/strong>,&lt;/li>
&lt;li>zero or more &lt;strong>character filters&lt;/strong>,&lt;/li>
&lt;li>zero or more &lt;strong>token filters&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>Basically, an analyzer transforms a single &lt;em>string&lt;/em> into &lt;em>words&lt;/em>, e.g. &lt;code>&amp;quot;This is my text&amp;quot;&lt;/code> can be transformed into &lt;code>[&amp;quot;this&amp;quot;, &amp;quot;my&amp;quot;, &amp;quot;text&amp;quot;]&lt;/code> which you can read as:&lt;/p>
&lt;ul>
&lt;li>text is split into tokens by tokenizer,&lt;/li>
&lt;li>each token is lowercased with the a token filter,&lt;/li>
&lt;li>stopwords are removed with another token filter.&lt;/li>
&lt;/ul>
&lt;h3 id="normalizers">Normalizers&lt;/h3>
&lt;p>The &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-normalizers.html">documentation&lt;/a> says that:&lt;/p>
&lt;blockquote>
&lt;p>Normalizers are similar to analyzers except that they may only emit a single token.&lt;/p>
&lt;/blockquote>
&lt;p>Normalizers can only be applied to the &lt;code>keyword&lt;/code> datatype. The cannonical use case is to lowercase structured content such as IDs, email addresses, e.g. a database stores emails in whatever case but searching for emails should be case insensitive. Note that only a subset of available filters can be used by a normalizer: all filters must work on a &lt;strong>per-character basis&lt;/strong>, i.e. no stopwords or stemmers.&lt;/p>
&lt;h3 id="normalizers-for-normalizing-url-data">Normalizers for Normalizing URL Data&lt;/h3>
&lt;p>Storing a URL in a &lt;code>keyword&lt;/code> field allows to filter, sort, and aggregate your data per URL. But what if you need to filter, sort, and aggregate by just one part of the URL and you have little to no control over the upstream data source? You have a couple of options:&lt;/p>
&lt;ul>
&lt;li>convince upstream to extract that one part in their code and send it to you,&lt;/li>
&lt;li>setup a &lt;code>text&lt;/code> field with an analyzer that produces just that one token and enable field data (not a default setup and can get expensive).&lt;/li>
&lt;li>setup a &lt;code>keyword&lt;/code> field with a normalizer with a &lt;code>char_filter&lt;/code>.&lt;/li>
&lt;li>give up.&lt;/li>
&lt;/ul>
&lt;p>I want to explore the &lt;code>keyword&lt;/code> option. In the next section I&amp;rsquo;ll show how to setup normalizers for Elasticsearch URLs.&lt;/p>
&lt;h3 id="the-not-so-synthetic-problem">The not so Synthetic Problem&lt;/h3>
&lt;p>We have a list URLs without a hostname that were used to query Elasticsearch, e.g.: &lt;code>/my_search_index/_search?q=elasticsearch&lt;/code> and we need to split URLs into parts such as: index, operation endpoint, e.g.: &lt;code>_search&lt;/code> or &lt;code>_count&lt;/code>, query filters, etc. In the following example I&amp;rsquo;ll focus on the extracting the index part of the URL.&lt;/p>
&lt;p>Let&amp;rsquo;s create an index:&lt;/p>
&lt;pre>&lt;code>PUT elasticsearch_url_index
{
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;normalizer&amp;quot;: {
&amp;quot;index_extractor_normalizer&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;custom&amp;quot;,
&amp;quot;char_filter&amp;quot;: [
&amp;quot;index_name_extractor&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;index_name_extractor&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;pattern_replace&amp;quot;,
&amp;quot;pattern&amp;quot;: &amp;quot;/(.+)/.*&amp;quot;,
&amp;quot;replacement&amp;quot;: &amp;quot;$1&amp;quot;
}
}
}
}
},
&amp;quot;mappings&amp;quot; : {
&amp;quot;properties&amp;quot;: {
&amp;quot;url&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;fields&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;normalizer&amp;quot;: &amp;quot;index_extractor_normalizer&amp;quot;
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Here we setup the index with a normalizer &lt;code>index_extractor_normalizer&lt;/code> that has a char filter &lt;code>index_name_extractor&lt;/code> that uses a regex &lt;code>pattern_replace&lt;/code> to extract characters between the first and the second slashes. The mappings have a property &lt;code>url&lt;/code> which is of the &lt;code>keyword&lt;/code> type and have a field &lt;code>index&lt;/code> which is also of the &lt;code>keyword&lt;/code> type and is set up to use the normalizer &lt;code>index_extractor_normalizer&lt;/code>.&lt;/p>
&lt;p>Since the normalizer is basically a collection of filters we can use our good old friend &lt;code>_analyze&lt;/code> API to test how it works.&lt;/p>
&lt;pre>&lt;code>POST elasticsearch_url_index/_analyze
{
&amp;quot;char_filter&amp;quot;: [&amp;quot;index_name_extractor&amp;quot;],
&amp;quot;text&amp;quot;: [&amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;my_search_index&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 40,
&amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
&amp;quot;position&amp;quot; : 0
}
]
}
&lt;/code>&lt;/pre>
&lt;p>Good, exactly as we wanted: &lt;code>/my_search_index/_search?q=elasticsearch&lt;/code> =&amp;gt; &lt;code>my_search_index&lt;/code>.&lt;/p>
&lt;p>Let&amp;rsquo;s index some data:&lt;/p>
&lt;pre>&lt;code>PUT elasticsearch_url_index/_doc/0
{
&amp;quot;url&amp;quot;: &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s try to filter URLs by the index name:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url:my_search_index
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 0,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No results? What? Oh! Wrong field: &lt;code>url&lt;/code> was used instead of &lt;code>url.index&lt;/code>. Let&amp;rsquo;s try once again:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url.index:my_search_index
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.2876821,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;elasticsearch_url_index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;0&amp;quot;,
&amp;quot;_score&amp;quot; : 0.2876821,
&amp;quot;_source&amp;quot; : {
&amp;quot;url&amp;quot; : &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected. Cool.&lt;/p>
&lt;h3 id="bonus-a-trick-with-the-docvalue_fields">Bonus: a Trick with the &lt;code>docvalue_fields&lt;/code>&lt;/h3>
&lt;p>Another neat trick is that we can get out the &lt;code>index&lt;/code> part of the URL from an Elasticsearch index using the &lt;code>docvalue_fields&lt;/code> option in a request ,e.g.:&lt;/p>
&lt;pre>&lt;code>GET elasticsearch_url_index/_search?q=url.index:my_search_index
{
&amp;quot;docvalue_fields&amp;quot;: [&amp;quot;url.index&amp;quot;]
}
&lt;/code>&lt;/pre>
&lt;p>Produces:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.2876821,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;elasticsearch_url_index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;0&amp;quot;,
&amp;quot;_score&amp;quot; : 0.2876821,
&amp;quot;_source&amp;quot; : {
&amp;quot;url&amp;quot; : &amp;quot;/my_search_index/_search?q=elasticsearch&amp;quot;
},
&amp;quot;fields&amp;quot; : {
&amp;quot;url.index&amp;quot; : [
&amp;quot;my_search_index&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The important part is this one:&lt;/p>
&lt;pre>&lt;code>&amp;quot;fields&amp;quot; : {
&amp;quot;url.index&amp;quot; : [
&amp;quot;my_search_index&amp;quot;
]
}
&lt;/code>&lt;/pre>
&lt;p>A neat thing about the &lt;code>docvalue_fields&lt;/code> is that in the example above the &lt;code>my_search_index&lt;/code> value is not comming from the &lt;code>_source&lt;/code> of the document. This means that we can use &lt;code>keywords&lt;/code> and by extension normalized &lt;code>keywords&lt;/code> to fetch an exact value from the Elasticsearch index and not necessarily the one that was sent to Elasticsearch which somewhat solves our dependency from the upstream systems.&lt;/p>
&lt;h2 id="notes">Notes&lt;/h2>
&lt;p>The setup is done in the Kibana Dev Tools with the Elasticsearch 7.7.0.&lt;/p>
&lt;p>The pattern &lt;code>&amp;quot;/(.+)/.*&amp;quot;&lt;/code> is a bit simplified purely for presentation purposes and doesn&amp;rsquo;t work as expected for URLs with more than 2 slashes, e.g.: &lt;code>/index/type/_search&lt;/code> would produce &lt;code>index/type&lt;/code>. You need something a bit more involved like &lt;code>&amp;quot;/([^/]+)/.*&amp;quot;&lt;/code>.&lt;/p>
&lt;h2 id="fin">Fin&lt;/h2>
&lt;p>That is all I wanted to show you today. Hope it might be useful/interesting to someone down the line. Leave comments on the Github issue &lt;a href="https://github.com/dainiusjocas/blog/issues/7">here&lt;/a>. Cheers!&lt;/p></description></item><item><title>Deploy babashka script to AWS Lambda</title><link>https://www.jocas.lt/blog/post/babashka-aws-lambda/</link><pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/babashka-aws-lambda/</guid><description>&lt;p>TL;DR&lt;/p>
&lt;p>I&amp;rsquo;ve managed to package a simple &lt;a href="https://github.com/borkdude/babashka">babashka&lt;/a> script to an AWS Lambda Custom Runtime. &lt;a href="https://github.com/dainiusjocas/babashka-lambda">Here&lt;/a> is the code, try for yourself.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Wouldn&amp;rsquo;t it be great to deploy little Clojure code snippets to Custom Lambda Runtime? The main benefits would be:&lt;/p>
&lt;ul>
&lt;li>you would not suffer from java cold-start problems;&lt;/li>
&lt;li>you wouldn&amp;rsquo;t need to compile your project with GraalVM &lt;code>native-image&lt;/code> tool which is time consuming and for anything more advanced is not likely to work anyway;&lt;/li>
&lt;li>babashka supports scripting with a subset of Clojure, which might do the work for you.&lt;/li>
&lt;/ul>
&lt;h2 id="the-plan">The plan&lt;/h2>
&lt;p>I know what it takes to deploy to Lambda Custom Runtime. Last year I&amp;rsquo;ve created a Clojure project template for deploying &lt;a href="https://github.com/tokenmill/clojure-graalvm-aws-lambda-template">GraalVM compiled AWS Lambda Custom Runtime&lt;/a>. And babashka is just another self contained binary. It should be too hard to bring two things together and get it working? Challenge accepted.&lt;/p>
&lt;h2 id="packaging">Packaging&lt;/h2>
&lt;p>I like to build software inside Docker containers. In this experiment, for the first attempt I&amp;rsquo;ve used this Dockerfile:&lt;/p>
&lt;pre>&lt;code>FROM borkdude/babashka:latest as BABASHKA
FROM clojure:tools-deps-alpine as BUILDER
RUN apk add --no-cache zip
WORKDIR /var/task
COPY --from=BABASHKA /usr/local/bin/bb bb
ENV GITLIBS=&amp;quot;.gitlibs/&amp;quot;
COPY lambda/bootstrap bootstrap
COPY deps.edn deps.edn
RUN clojure -Sdeps '{:mvn/local-repo &amp;quot;./.m2/repository&amp;quot;}' -Spath &amp;gt; cp
COPY src/ src/
COPY resources/ resources/
RUN zip -q -r function.zip bb cp bootstrap .gitlibs/ .m2/ src/ resources/ deps.edn
&lt;/code>&lt;/pre>
&lt;p>Here:&lt;/p>
&lt;ul>
&lt;li>copy &lt;code>bb&lt;/code> binary from babashka Docker image,&lt;/li>
&lt;li>download the dependencies for babashka script using &lt;code>clojure&lt;/code> (both, maven and git dependencies are supported, like is described &lt;a href="https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/">here&lt;/a>),&lt;/li>
&lt;li>write a classpath to the &lt;code>cp&lt;/code> file,&lt;/li>
&lt;li>copy all source code,&lt;/li>
&lt;li>zip the required contents to the &lt;code>function.zip&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Every line of this dockerfile is packed with details but I&amp;rsquo;ll leave it for the future posts.&lt;/p>
&lt;p>I&amp;rsquo;ve packaged all dependencies for lambda into &lt;code>function.zip&lt;/code>. The contents of the archive are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>bb&lt;/code>: babashka binary&lt;/li>
&lt;li>&lt;code>bootstrap&lt;/code>: AWS Lambda entry point script&lt;/li>
&lt;li>&lt;code>cp&lt;/code>: generated classpath text file&lt;/li>
&lt;li>&lt;code>deps.edn&lt;/code>&lt;/li>
&lt;li>&lt;code>.gitlibs&lt;/code>: directory with gitlibs&lt;/li>
&lt;li>&lt;code>.m2&lt;/code>: directory with Maven dependencies&lt;/li>
&lt;li>&lt;code>resources&lt;/code>:&lt;/li>
&lt;li>&lt;code>src&lt;/code>: directory with babashka scripts&lt;/li>
&lt;/ul>
&lt;h2 id="custom-runtime-discoveries">Custom runtime discoveries&lt;/h2>
&lt;p>Finally, having all dependencies packaged up, I&amp;rsquo;ve deployed the &lt;code>function.zip&lt;/code> to AWS Lambda. The first error message was not very &lt;a href="https://gist.github.com/dainiusjocas/feafeef5653ff2c6e8c7b2d9627a831d">encouraging&lt;/a>:&lt;/p>
&lt;pre>&lt;code class="language-text">Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.create() failed. errno: 38 Function not implemented
Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed.
JavaFrameAnchor dump:
No anchors
TopFrame info:
TotalFrameSize in CodeInfoTable 32
VMThreads info:
VMThread 0000000003042750 STATUS_IN_JAVA (safepoints disabled) java.lang.Thread@0x264fa98
VM Thread State for current thread 0000000003042750:
0 (8 bytes): com.oracle.svm.jni.JNIThreadLocalEnvironment.jniFunctions = (bytes)
0000000003042750: 0000000002293a88
8 (32 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.regularTLAB = (bytes)
0000000003042758: 00007f7809500000 00007f7809600000
0000000003042768: 00007f7809507160 0000000000000000
40 (8 bytes): com.oracle.svm.core.heap.NoAllocationVerifier.openVerifiers = (Object) null
48 (8 bytes): com.oracle.svm.core.jdk.IdentityHashCodeSupport.hashCodeGeneratorTL = (Object) null
56 (8 bytes): com.oracle.svm.core.snippets.SnippetRuntime.currentException = (Object) null
64 (8 bytes): com.oracle.svm.core.thread.JavaThreads.currentThread = (Object) java.lang.Thread 000000000264fa98
72 (8 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.activeTimer = (Object) null
80 (8 bytes): com.oracle.svm.jni.JNIObjectHandles.handles = (Object) com.oracle.svm.core.handles.ThreadLocalHandles 00007f7809501558
88 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPendingException.pendingException = (Object) null
96 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPinnedObjects.pinnedObjectsListHead = (Object) null
104 (8 bytes): com.oracle.svm.jni.JNIThreadOwnedMonitors.ownedMonitors = (Object) null
112 (8 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.freeList = (Word) 0 0000000000000000
120 (8 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.stackBoundaryTL = (Word) 1 0000000000000001
128 (8 bytes): com.oracle.svm.core.stack.JavaFrameAnchors.lastAnchor = (Word) 0 0000000000000000
136 (8 bytes): com.oracle.svm.core.thread.VMThreads.IsolateTL = (Word) 25636864 0000000001873000
144 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadHandleTL = (Word) 50477184 0000000003023880
152 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadIdTL = (Word) 50477184 0000000003023880
160 (8 bytes): com.oracle.svm.core.thread.VMThreads.nextTL = (Word) 0 0000000000000000
168 (4 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.yellowZoneStateTL = (int) -16843010 fefefefe
172 (4 bytes): com.oracle.svm.core.snippets.ImplicitExceptions.implicitExceptionsAreFatal = (int) 0 00000000
176 (4 bytes): com.oracle.svm.core.thread.Safepoint.safepointRequested = (int) 2147473200 7fffd730
180 (4 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.currentPauseDepth = (int) 0 00000000
184 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.safepointsDisabledTL = (int) 1 00000001
188 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.statusTL = (int) 1 00000001
VMOperation dump:
No VMOperation in progress
Dump Counters:
Raw Stacktrace:
00007ffeb8e0a940: 000000000186e776 000000000207b9d0
00007ffeb8e0a950: 0000000001873000 000000000085b37c
00007ffeb8e0a960: 000000000084540a 00000000008454ca
00007ffeb8e0a970: 000000000264f128 000000000264ef58
00007ffeb8e0a980: 00007f78095018d8 0000000002650640
00007ffeb8e0a990: 000000000264f128 0000002602650c18
00007ffeb8e0a9a0: 0000000000845444 00007ffeb8e0a970
00007ffeb8e0a9b0: 0000000000000000 0000000000845f6e
00007ffeb8e0a9c0: 0000000002650e18 0000000002650c18
00007ffeb8e0a9d0: 0000000002650e18 0000000002070c60
00007ffeb8e0a9e0: 00000000021f48f8 00000000012b77e6
00007ffeb8e0a9f0: 0000000002650e18 0000000002650c18
00007ffeb8e0aa00: 0000001000000000 0000000002070c60
00007ffeb8e0aa10: 00007f7809507138 0000000000477f69
00007ffeb8e0aa20: 00007f7809503b88 00007f7809501910
00007ffeb8e0aa30: 00007f7809507138 00000000004831b4
00007ffeb8e0aa40: 0000000000000010 000000000085d16d
00007ffeb8e0aa50: 000000000000003b 00000000008b4bdb
00007ffeb8e0aa60: 000000000291e970 00007f7809504828
00007ffeb8e0aa70: 0000000100000007 0000000001079a70
00007ffeb8e0aa80: 00007f78095070b8 00007f7809507080
00007ffeb8e0aa90: 0000000001873000 000000000291e970
00007ffeb8e0aaa0: 00007f7809506f78 00007f78095070b8
00007ffeb8e0aab0: 0000000000000008 0000000000000010
00007ffeb8e0aac0: 0000000000000010 00000000008144a1
00007ffeb8e0aad0: 0000000000000007 0000000000cd7c2e
00007ffeb8e0aae0: 00007f7809504938 0000000001873000
00007ffeb8e0aaf0: 0000000002205088 00007f78095070b8
00007ffeb8e0ab00: 00007f7809507080 0000000cc0001000
00007ffeb8e0ab10: 0000000000000000 0000000000cd73eb
00007ffeb8e0ab20: 00007f7809503b58 00007f78095070b8
00007ffeb8e0ab30: 00007f7809507080 00007f78095038e0
00007ffeb8e0ab40: 00007f7807c8e388 000000000205e900
00007ffeb8e0ab50: 00007f7809501350 000000240000000c
00007ffeb8e0ab60: 000000000000000c 00007f78095038e0
00007ffeb8e0ab70: d15c483b00000000 00000000004830e5
00007ffeb8e0ab80: 0000000000000007 00007f78095038e0
00007ffeb8e0ab90: 00007f78095038e0 00000000006f2b33
00007ffeb8e0aba0: 000000000205e900 0000000002070448
00007ffeb8e0abb0: 00007f78095070b8 0000000000cd8b3d
00007ffeb8e0abc0: 00000000020864c8 0000000000cbffc1
00007ffeb8e0abd0: 0000000002070448 00007f78095070b8
00007ffeb8e0abe0: 0000000c00000000 00007f7809505ef8
00007ffeb8e0abf0: 00007f78095070d8 00007f7809504840
00007ffeb8e0ac00: 7cab467402070d98 0000000000fbfc08
00007ffeb8e0ac10: 0000000002634470 00007f7809507020
00007ffeb8e0ac20: 0000000001873000 00007f78095070d8
00007ffeb8e0ac30: 00007f7809504840 0000000000cc187e
00007ffeb8e0ac40: 0000000000000000 0000000000000000
00007ffeb8e0ac50: 00007f7807c91840 00007f7809504840
00007ffeb8e0ac60: 0000000002070d98 0000000000cc17b9
00007ffeb8e0ac70: 0000000000c848f0 00007f78095038e0
00007ffeb8e0ac80: 0000000002b33a78 0000000100cc4f83
00007ffeb8e0ac90: 0000000000483140 00000000004b5713
00007ffeb8e0aca0: 0000000002070d98 0000000000cdae9a
00007ffeb8e0acb0: 000000000209a600 00007f78095038e0
00007ffeb8e0acc0: 0000000002b33a78 000000000047c576
00007ffeb8e0acd0: 000000000209a600 000000000209a630
00007ffeb8e0ace0: 0000000002a1b8d8 0000000002a1b408
00007ffeb8e0acf0: 000000000209a600 00000000017acc23
00007ffeb8e0ad00: 0000000000000001 0000000000001000
00007ffeb8e0ad10: 0000000000000000 0000000000000000
00007ffeb8e0ad20: 0000000000000000 0000000000000000
00007ffeb8e0ad30: 0000000000000000 0000000000000000
Stacktrace Stage0:
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 FrameSize 32
RSP 00007ffeb8e0a960 RIP 000000000085b37c FrameSize 16
RSP 00007ffeb8e0a970 RIP 00000000008454ca FrameSize 80
RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e FrameSize 48
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 FrameSize 48
RSP 00007ffeb8e0aa20 RIP 0000000000477f69 FrameSize 32
RSP 00007ffeb8e0aa40 RIP 00000000004831b4 FrameSize 320
RSP 00007ffeb8e0ab80 RIP 00000000004830e5 FrameSize 32
RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 FrameSize 256
RSP 00007ffeb8e0aca0 RIP 00000000004b5713 FrameSize 48
RSP 00007ffeb8e0acd0 RIP 000000000047c576 FrameSize 160
RSP 00007ffeb8e0ad70 RIP 000000000047c285 FrameSize 32
RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 FrameSize 256
RSP 00007ffeb8e0ae90 RIP 000000000048f162 FrameSize 32
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c FrameSize 1
Stacktrace Stage1:
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a960 RIP 000000000085b37c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a970 RIP 00000000008454ca com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aa20 RIP 0000000000477f69 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aa40 RIP 00000000004831b4 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ab80 RIP 00000000004830e5 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aca0 RIP 00000000004b5713 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0acd0 RIP 000000000047c576 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ad70 RIP 000000000047c285 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0ae90 RIP 000000000048f162 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code
Full Stacktrace:
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.jdk.VMErrorSubstitutions.shutdown(VMErrorSubstitutions.java:111)
RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:74)
RSP 00007ffeb8e0a960 RIP 000000000085b37c [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:59)
RSP 00007ffeb8e0a970 RIP 00000000008454ca [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.ensureInitialized(SunMiscSubstitutions.java:176)
RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.numberFromName(SunMiscSubstitutions.java:223)
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.findSignal(Signal.java:78)
RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.&amp;lt;init&amp;gt;(Signal.java:140)
RSP 00007ffeb8e0aa20 RIP 0000000000477f69 [image code] babashka.impl.pipe_signal_handler$handle_pipe_BANG_.invokeStatic(pipe_signal_handler.clj:11)
RSP 00007ffeb8e0aa40 RIP 00000000004831b4 [image code] babashka.main$main.invokeStatic(main.clj:282)
RSP 00007ffeb8e0ab80 RIP 00000000004830e5 [image code] babashka.main$main.doInvoke(main.clj:282)
RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137)
RSP 00007ffeb8e0aca0 RIP 00000000004b5713 [image code] clojure.core$apply.invokeStatic(core.clj:665)
RSP 00007ffeb8e0acd0 RIP 000000000047c576 [image code] babashka.main$_main.invokeStatic(main.clj:442)
RSP 00007ffeb8e0ad70 RIP 000000000047c285 [image code] babashka.main$_main.doInvoke(main.clj:437)
RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137)
RSP 00007ffeb8e0ae90 RIP 000000000048f162 [image code] babashka.main.main(Unknown Source)
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.runCore(JavaMainWrapper.java:151)
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:186)
RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.code.IsolateEnterStub.JavaMainWrapper_run_5087f5482cc9a6abc971913ece43acb471d2631b(IsolateEnterStub.java:0)
[Native image heap boundaries:
ReadOnly Primitives: 0x1873008 .. 0x206f048
ReadOnly References: 0x206ff78 .. 0x24fc9f8
Writable Primitives: 0x24fd000 .. 0x26343e0
Writable References: 0x2634470 .. 0x2ba42c0]
[Heap:
[Young generation:
[youngSpace:
aligned: 0/0 unaligned: 0/0]]
[Old generation:
[fromSpace:
aligned: 0/0 unaligned: 0/0]
[toSpace:
aligned: 0/0 unaligned: 0/0]
]
[Unused:
aligned: 0/0]]
Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed.
RequestId: 263ff1be-425d-4dcb-9ea5-67020dc3041b Error: Runtime exited with error: exit status 99
Runtime.ExitError
&lt;/code>&lt;/pre>
&lt;h2 id="the-fight">The fight&lt;/h2>
&lt;p>After some Googling I&amp;rsquo;ve discovered several related clues &lt;a href="https://github.com/oracle/graal/issues/841">here&lt;/a> and &lt;a href="https://github.com/quarkusio/quarkus/issues/4262">here&lt;/a>. They say that signals are not supported in AWS lambda. So, why not to disable signals for babashka and see what happens? I&amp;rsquo;ve forked the repo, made a flag that disables PIPE signal handling, deployed babashka to the &lt;a href="https://hub.docker.com/r/dainiusjocas/babashka">docker hub&lt;/a> and tried to deploy lambda once again.&lt;/p>
&lt;p>And? It worked:&lt;/p>
&lt;pre>&lt;code class="language-shell">make function-name=$(make get-function-name) invoke-function
=&amp;gt;
{&amp;quot;test&amp;quot;:&amp;quot;test914&amp;quot;}{
&amp;quot;StatusCode&amp;quot;: 200,
&amp;quot;ExecutedVersion&amp;quot;: &amp;quot;$LATEST&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>&lt;a href="https://github.com/dainiusjocas/babashka-lambda">Here&lt;/a> is the example of babashka script that can be deployed to AWS Lambda.&lt;/p>
&lt;ul>
&lt;li>The &lt;code>function.zip&lt;/code> weights just 18MB.&lt;/li>
&lt;li>The cold startup of the Lambda that is given 128MB of RAM is ~400ms. Subsequent calls ranges from 4ms and 120ms. The more RAM you give the faster lambda gets.&lt;/li>
&lt;li>I can develop the code in Cursive as the structure is like of an ordinary Clojure deps.edn project (and it can be used on the JVM).&lt;/li>
&lt;li>I made a &lt;a href="https://github.com/borkdude/babashka/pull/305">PR to babashka&lt;/a> and I&amp;rsquo;ve got accepted.&lt;/li>
&lt;/ul>
&lt;h2 id="next-steps">Next Steps&lt;/h2>
&lt;ul>
&lt;li>Fix Problem building on macos (&lt;code>/tmp&lt;/code> dir is not writable).&lt;/li>
&lt;li>Get rid of AWS CloudFormation part.&lt;/li>
&lt;li>Work a bit more to support AWS API Gateway.&lt;/li>
&lt;li>Create a template for such projects.&lt;/li>
&lt;/ul></description></item><item><title>Using Search Templates in Elasticsearch</title><link>https://www.jocas.lt/blog/post/on-search-templates/</link><pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/on-search-templates/</guid><description>&lt;p>I want to take a look at &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html">Search Templates&lt;/a> for Elasticsearch. Let&amp;rsquo;s apply them to examples from &lt;a href="https://www.jocas.lt/blog/post/synonym-graph-phrase-search/">previous post on Synonym Graphs&lt;/a>.&lt;/p>
&lt;h2 id="setup">Setup&lt;/h2>
&lt;p>I&amp;rsquo;m using Elasticsearch 7.5.1.&lt;/p>
&lt;p>Index configuration:&lt;/p>
&lt;pre>&lt;code>DELETE test_index-1
PUT /test_index-1
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;descrition&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
}
},
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonym_graph_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;my_synonym_graph&amp;quot;
]
},
&amp;quot;lowercase_keyword_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
],
&amp;quot;char_filter&amp;quot;: [
&amp;quot;spaces_to_undescores_filter&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;spaces_to_undescores_filter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;,
&amp;quot;mappings&amp;quot;: [
&amp;quot; \\u0020 =&amp;gt; _&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;my_synonym_graph&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym_graph&amp;quot;,
&amp;quot;lenient&amp;quot;: true,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;
]
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Index the document:&lt;/p>
&lt;pre>&lt;code>PUT test_index-1/_doc/1
{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>prefix very important thing suffix&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="templates">Templates&lt;/h2>
&lt;p>I&amp;rsquo;m very interested in one particular use of the search templates: how flexible is the management of stored seach templates? Can I update a search template while receiving queries?&lt;/p>
&lt;p>Add a template:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Try to run the search:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Exactly as expected.&lt;/p>
&lt;p>When using a stored search template the Elasticsearch client doesn&amp;rsquo;t need to handle the complex query construction.&lt;/p>
&lt;h2 id="templates-are-updateable">Templates are updateable&lt;/h2>
&lt;p>Let&amp;rsquo;s try to update the template with a higher boost value:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 5
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Works.&lt;/p>
&lt;p>Now let&amp;rsquo;s run the same query:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 4,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>The scores are 0.5753642 and 1.4384103 that is ~2/5. Cool! This means that without changing (and redeploying) the Elasticsearch client we can change the querying logic, making the query an more dynamic.&lt;/p>
&lt;h2 id="corner-cases">Corner Cases&lt;/h2>
&lt;p>What if we run query has more attributes, e.g.:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: &amp;quot;suffix very important thing prefix&amp;quot;,
&amp;quot;new_attr&amp;quot;: &amp;quot;123&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>Works as expected!&lt;/p>
&lt;p>When &lt;code>query_string&lt;/code> is &lt;code>null&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: null
}
}
&lt;/code>&lt;/pre>
&lt;p>Works!&lt;/p>
&lt;p>What if the param is not provided:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;new_attr&amp;quot;: &amp;quot;value&amp;quot;
}
}
&lt;/code>&lt;/pre>
&lt;p>No error!&lt;/p>
&lt;p>What if we provide a list instead of a string:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search/template
{
&amp;quot;id&amp;quot;: &amp;quot;synonym-graph-search&amp;quot;,
&amp;quot;params&amp;quot;: {
&amp;quot;query_string&amp;quot;: [&amp;quot;this&amp;quot;, &amp;quot;Very Important Thing&amp;quot;]
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Instead of profiding one value we can replace it with a list. Good!&lt;/p>
&lt;h2 id="metadata-of-the-search-template">Metadata of the search template&lt;/h2>
&lt;p>It would be great to be able to store some metadata with the search template script, e.g. Git commit SHA of the query. I couldn&amp;rsquo;t find a way to do this. A workaround might be to &lt;code>_name&lt;/code> attribute of the query. E.g.:&lt;/p>
&lt;pre>&lt;code>POST _scripts/synonym-graph-search
{
&amp;quot;script&amp;quot;: {
&amp;quot;lang&amp;quot;: &amp;quot;mustache&amp;quot;,
&amp;quot;source&amp;quot;: {
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;_name&amp;quot;: &amp;quot;GIT COMMIT SHA&amp;quot;,
&amp;quot;query&amp;quot;: &amp;quot;{{query_string}}&amp;quot;,
&amp;quot;boost&amp;quot;: 5
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>The response:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.4384103,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.4384103,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
},
&amp;quot;matched_queries&amp;quot; : [
&amp;quot;GIT COMMIT SHA&amp;quot;
]
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Not great but might be useful.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;ul>
&lt;li>Templates doesn&amp;rsquo;t support search index specification.&lt;/li>
&lt;li>Field names can be parameterized, this feature alows to start/stop using a new/old field.&lt;/li>
&lt;li>Search template can be tested in (even in production cluster) independently.&lt;/li>
&lt;li>We can run our query against &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-search-template.html">multiple search templates&lt;/a>. Combine this with the Profile API and performance can be compared. Explain API also is supported.&lt;/li>
&lt;/ul></description></item><item><title>Phrase Search with Synonym Graph Token Filter in Elasticsearch</title><link>https://www.jocas.lt/blog/post/synonym-graph-phrase-search/</link><pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/synonym-graph-phrase-search/</guid><description>&lt;p>I&amp;rsquo;ve &lt;a href="https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/">written&lt;/a> that if you google for &lt;code>How can you match a long query text to a short text field?&lt;/code> you&amp;rsquo;re advised to use Elasticsearch Percolator. Today I&amp;rsquo;ll show an alternative way of solving the same problem with Elasticsearch.&lt;/p>
&lt;p>The main idea is to use &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/master/analysis-synonym-graph-tokenfilter.html">Synonym Graph Token Filter&lt;/a> with some data preparation.&lt;/p>
&lt;h2 id="problem-statement">Problem Statement&lt;/h2>
&lt;p>Say that we learned how extract some entity from free form text with techniques such as NER, dictionary annotations, or some fancy Machine Learning. And when this entity is mentioned in the search query we want to boost documents that mention this entity. Also, say you&amp;rsquo;ve ruled out using Elasticsearch Percolator because it increases network latency because it requires additional call to Elasticsearch.&lt;/p>
&lt;p>For further discussion our unstructured text is going to be &lt;code>This description is about a Very Important Thing and something else.&lt;/code> and the extracted entity &lt;code>Very Important Thing&lt;/code>. Our test document looks like :&lt;/p>
&lt;pre>&lt;code class="language-json">{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>prefix very important thing suffix&lt;/code>&lt;/li>
&lt;li>&lt;code>prefix very important another thing suffix&lt;/code>&lt;/li>
&lt;li>&lt;code>prefix thing suffix&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>All examples are tested on Elasticsearch 7.5.1.&lt;/p>
&lt;h3 id="naive-setup">Naive Setup&lt;/h3>
&lt;p>Let&amp;rsquo;s create an index for our documents:&lt;/p>
&lt;pre>&lt;code>PUT /test_index-2
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;description&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Entity field is of type &lt;code>text&lt;/code> because we want it to be searchable. &lt;code>keyword&lt;/code> type won&amp;rsquo;t work because it does only exact matches and out query most likely will be longer than our entity string.&lt;/p>
&lt;p>Index our document:&lt;/p>
&lt;pre>&lt;code>PUT test_index-2/_doc/1
{
&amp;quot;description&amp;quot;: &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot;: &amp;quot;Very Important Thing&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Search the index with the query that mentions our &lt;code>very important thing&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.7260926,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.7260926,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Cool, we found what we we looking for.&lt;/p>
&lt;p>Let&amp;rsquo;s try another query, this time with a mention of &lt;code>very important another thing&lt;/code>:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important another thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 1.7260926,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 1.7260926,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Oh, the results are the same as with the previous query despite the fact that we mention &lt;code>Another Thing&lt;/code> here. But it still might be OK because we matched all the terms of the entity.&lt;/p>
&lt;p>Let&amp;rsquo;s try another query:&lt;/p>
&lt;pre>&lt;code>GET test_index-2/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-2&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Oh no, we still matched our &lt;code>Very Important Thing&lt;/code> while only &lt;code>thing&lt;/code> term is present in the query. But at least this time the score is lower than with previous twoqueries, 0.5753642 vs. 1.7260926. Here we clearly see the problem: we are matching short strings with long strings and partial matches raises problems.&lt;/p>
&lt;h2 id="proposed-solution">Proposed Solution&lt;/h2>
&lt;p>Let&amp;rsquo;s leverage Synonym Graph Token Filter to solve our problem.&lt;/p>
&lt;pre>&lt;code>PUT /test_index-1
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;descrition&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;
},
&amp;quot;entity&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
}
},
&amp;quot;settings&amp;quot;: {
&amp;quot;index&amp;quot;: {
&amp;quot;analysis&amp;quot;: {
&amp;quot;analyzer&amp;quot;: {
&amp;quot;synonym_graph_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;,
&amp;quot;my_synonym_graph&amp;quot;
]
},
&amp;quot;lowercase_keyword_analyzer&amp;quot;: {
&amp;quot;tokenizer&amp;quot;: &amp;quot;keyword&amp;quot;,
&amp;quot;filter&amp;quot;: [
&amp;quot;lowercase&amp;quot;
],
&amp;quot;char_filter&amp;quot;: [
&amp;quot;spaces_to_undescores_filter&amp;quot;
]
}
},
&amp;quot;char_filter&amp;quot;: {
&amp;quot;spaces_to_undescores_filter&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;mapping&amp;quot;,
&amp;quot;mappings&amp;quot;: [
&amp;quot; \\u0020 =&amp;gt; _&amp;quot;
]
}
},
&amp;quot;filter&amp;quot;: {
&amp;quot;my_synonym_graph&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;synonym_graph&amp;quot;,
&amp;quot;lenient&amp;quot;: true,
&amp;quot;synonyms&amp;quot;: [
&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;
]
}
}
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s decompose this large index configuration piece by piece:&lt;/p>
&lt;ol>
&lt;li>The &lt;code>entity&lt;/code> attribute now has separate analyzers for both index and search phases.&lt;/li>
&lt;li>The &lt;code>lowercase_keyword_analyzer&lt;/code> uses keyword tokenizer which means that tokenization will result in the sequence of token of size 1, then it normalizes tokens by lowercasing them and finally &lt;code>spaces_to_undescores_filter&lt;/code>, replaces spaces to underscores. E.g. a string &lt;code>&amp;quot;Very Important Thing&amp;quot;&lt;/code> is transformed into list of tokens &lt;code>[&amp;quot;very_important_thing&amp;quot;]&lt;/code>. Or use out friend &lt;code>_analyze&lt;/code> API:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>POST test_index-1/_analyze
{
&amp;quot;text&amp;quot;: [&amp;quot;Very Important Thing&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;lowercase_keyword_analyzer&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;very_important_thing&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 20,
&amp;quot;type&amp;quot; : &amp;quot;word&amp;quot;,
&amp;quot;position&amp;quot; : 0
}
]
}
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>The &lt;code>synonym_graph_analyzer&lt;/code> use standard tokenizer, which is followed by the &lt;code>lowercase&lt;/code> filter, and then the &lt;code>my_synonym_graph&lt;/code> token filter is applied. We&amp;rsquo;ve set up one synonym &lt;code>&amp;quot;very important thing =&amp;gt; very_important_thing&amp;quot;&lt;/code>. E.g.&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>POST test_index-1/_analyze
{
&amp;quot;text&amp;quot;: [&amp;quot;prefix very important thing suffix&amp;quot;],
&amp;quot;analyzer&amp;quot;: &amp;quot;synonym_graph_analyzer&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;tokens&amp;quot; : [
{
&amp;quot;token&amp;quot; : &amp;quot;prefix&amp;quot;,
&amp;quot;start_offset&amp;quot; : 0,
&amp;quot;end_offset&amp;quot; : 6,
&amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot; : 0
},
{
&amp;quot;token&amp;quot; : &amp;quot;very_important_thing&amp;quot;,
&amp;quot;start_offset&amp;quot; : 7,
&amp;quot;end_offset&amp;quot; : 27,
&amp;quot;type&amp;quot; : &amp;quot;SYNONYM&amp;quot;,
&amp;quot;position&amp;quot; : 1
},
{
&amp;quot;token&amp;quot; : &amp;quot;suffix&amp;quot;,
&amp;quot;start_offset&amp;quot; : 28,
&amp;quot;end_offset&amp;quot; : 34,
&amp;quot;type&amp;quot; : &amp;quot;&amp;lt;ALPHANUM&amp;gt;&amp;quot;,
&amp;quot;position&amp;quot; : 2
}
]
}
&lt;/code>&lt;/pre>
&lt;p>After analysis we have 3 tokens &lt;code>[&amp;quot;prefix&amp;quot;, &amp;quot;very_important_thing&amp;quot;, &amp;quot;suffix&amp;quot;]&lt;/code>. Notice &lt;code>&amp;quot;very_important_thing&amp;quot;&lt;/code> token: this is equal to the right-hand-side from our synonym definitions. Now let&amp;rsquo;s run queries from the previous section:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.5753642,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;test_index-1&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.5753642,
&amp;quot;_source&amp;quot; : {
&amp;quot;description&amp;quot; : &amp;quot;This description is about a Very Important Thing and something else.&amp;quot;,
&amp;quot;entity&amp;quot; : &amp;quot;Very Important Thing&amp;quot;
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: exact match -&amp;gt; hit.&lt;/p>
&lt;p>Another query:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix very important another thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 0,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits! Good! The document is not going to be boosted despite the fact that all tokens match.&lt;/p>
&lt;p>And the last one:&lt;/p>
&lt;pre>&lt;code>GET test_index-1/_search
{
&amp;quot;query&amp;quot;: {
&amp;quot;match&amp;quot;: {
&amp;quot;entity&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;prefix thing suffix&amp;quot;,
&amp;quot;boost&amp;quot;: 2
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits! Good. This means that also substring doesn&amp;rsquo;t match.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>Synonym Graph Token Filter can &amp;ldquo;replace&amp;rdquo; a sequence of tokens (e.g. a phrase) with another sequence of tokens. In this particular example: many tokens were replaced with one token.&lt;/p>
&lt;ol>
&lt;li>One field can have only one analyzer pair for index and search phases. If we want another analysis pipeline for the &lt;code>entity&lt;/code> attribute we have to create another field with the analyzers specified, e.g. stemmed phrase with lower boost.&lt;/li>
&lt;li>The synonym list must be prepared before the index creation.&lt;/li>
&lt;li>Management of the synonym list might complicate index management, e.g. you use templates for your index management.&lt;/li>
&lt;li>The overal solution in general might look a bit too complicated.&lt;/li>
&lt;/ol></description></item><item><title>Elasticsearch Percolator and Text Analyzers</title><link>https://www.jocas.lt/blog/post/percolator-phrase-analyzers/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/percolator-phrase-analyzers/</guid><description>&lt;p>This time I need to percolate texts with different analyzers for index and search analyzers.&lt;/p>
&lt;p>Let&amp;rsquo;s elaborate a bit on &lt;a href="https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/">previous article&lt;/a> and explicitly declare analyzers to use.&lt;/p>
&lt;p>Define index:&lt;/p>
&lt;pre>&lt;code>PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Then define 2 slightly different percolator queries (notice the difference between &lt;code>&amp;quot;bonsai tree&amp;quot;&lt;/code> and &lt;code>&amp;quot;bonsai, tree&amp;quot;&lt;/code>).&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
PUT /my-index/_doc/2?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s percolate:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 80,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 2,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;2&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
},
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: 2 documents matched.&lt;/p>
&lt;p>But now lets change the analyzer of the second percolation query to &lt;code>whitespace&lt;/code>:&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/2?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai, tree&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;whitespace&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Run the percolator:&lt;/p>
&lt;pre>&lt;code>
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 5,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai tree&amp;quot;,
&amp;quot;analyzer&amp;quot; : &amp;quot;standard&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As expected: only 1 percolator query matched our input.&lt;/p>
&lt;h2 id="phrases-with-stopwords">Phrases with Stopwords&lt;/h2>
&lt;p>Say, we have a phrase &lt;code>&amp;quot;bonsai is tree&amp;quot;&lt;/code> and we percolate text &lt;code>A new bonsai in tree in the office&lt;/code> with the &lt;code>standard&lt;/code> analyzer for indexing and &lt;code>english&lt;/code> for search analyzer. There should be no matches. Let&amp;rsquo;s try:&lt;/p>
&lt;pre>&lt;code>DELETE my-index
PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;english&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai is tree&amp;quot;
}
}
}
}
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai in tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>And, surprisingly, this yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 2,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai is tree&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>We have a match! Also notice that the highlighter is broken!&lt;/p>
&lt;p>The problem that these two analyzers have different stopword lists (no stopwords for &lt;code>standard&lt;/code> and several English stopwords for &lt;code>english&lt;/code> analyzer) and the phrase contains a stopword that is not shared between analyzers.&lt;/p>
&lt;p>Let&amp;rsquo;s fix this surprise with &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html#search-quote-analyzer">&lt;code>search_quote_analyzer&lt;/code>&lt;/a>.&lt;/p>
&lt;pre>&lt;code>DELETE my-index
PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;search_analyzer&amp;quot;: &amp;quot;english&amp;quot;,
&amp;quot;search_quote_analyzer&amp;quot;: &amp;quot;standard&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;query&amp;quot;: &amp;quot;bonsai is tree&amp;quot;
}
}
}
}
GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai in tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 1,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 0,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : null,
&amp;quot;hits&amp;quot; : [ ]
}
}
&lt;/code>&lt;/pre>
&lt;p>No hits, as expected.&lt;/p>
&lt;p>Let&amp;rsquo;s check if the expected behaviour is still there:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai is tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 4,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.39229375,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.39229375,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : {
&amp;quot;query&amp;quot; : &amp;quot;bonsai is tree&amp;quot;
}
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai is tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Good. Even the highlighting works.&lt;/p></description></item><item><title>Phrase Highlighting with the Elasticsearch Percolator</title><link>https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/</link><pubDate>Wed, 18 Dec 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/</guid><description>&lt;p>If you google &lt;code>How can you match a long query text to a short text field?&lt;/code> it will point you to the &lt;a href="https://stackoverflow.com/questions/51865747/elasticsearch-match-long-query-text-to-short-field">Stack Overflow page&lt;/a> &lt;a href="https://discuss.elastic.co/t/match-long-query-text-to-short-field/144584/3">or here&lt;/a> where the answer is to use &lt;a href="">Elasticsearch Percolator&lt;/a>.&lt;/p>
&lt;p>My search items are phrases meaning that it should match all terms in order. Let&amp;rsquo;s create a sample setup in Kibana (v7.5) Dev dashboard.&lt;/p>
&lt;ol>
&lt;li>Create an index for percolation:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>PUT /my-index
{
&amp;quot;mappings&amp;quot;: {
&amp;quot;properties&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,
&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;
},
&amp;quot;query&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;percolator&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note on &lt;code>&amp;quot;term_vector&amp;quot;: &amp;quot;with_positions_offsets&amp;quot;&lt;/code>: this allows &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.6/search-request-highlighting.html#fast-vector-highlighter">Fast Vector Highlighter&lt;/a> to highlight combined phrase not just separate qeury terms.&lt;/p>
&lt;ol start="2">
&lt;li>Store one phrase query:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;bonsai tree&amp;quot;
}
}
}
&lt;/code>&lt;/pre>
&lt;ol start="3">
&lt;li>Percolate a document:&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>Note on &lt;code>&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;&lt;/code>: this instructs Elasticsearch to use the Fast Vector Highlighter.&lt;/p>
&lt;p>The query yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 23,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : &amp;quot;bonsai tree&amp;quot;
}
}
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>As we see highlighter correctly marker the search phrase.&lt;/p>
&lt;h2 id="storing-additional-data-with-percolator-queries">Storing additional data with percolator queries&lt;/h2>
&lt;p>Percolation result can be used to connect pieces of information in your system, e.g. store a &lt;code>subscriber_email&lt;/code> attribute of the user that wants to be notified when the query matches along with the percolator query.&lt;/p>
&lt;pre>&lt;code>PUT /my-index/_doc/1?refresh
{
&amp;quot;query&amp;quot;: {
&amp;quot;match_phrase&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;bonsai tree&amp;quot;
}
},
&amp;quot;subscriber_email&amp;quot;: &amp;quot;subscriber_email@example.com&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>Then query:&lt;/p>
&lt;pre>&lt;code>GET /my-index/_search?
{
&amp;quot;query&amp;quot;: {
&amp;quot;percolate&amp;quot;: {
&amp;quot;field&amp;quot;: &amp;quot;query&amp;quot;,
&amp;quot;document&amp;quot;: {
&amp;quot;message&amp;quot;: &amp;quot;A new bonsai tree in the office&amp;quot;
}
}
},
&amp;quot;highlight&amp;quot;: {
&amp;quot;fields&amp;quot;: {
&amp;quot;message&amp;quot;: {
&amp;quot;type&amp;quot;: &amp;quot;fvh&amp;quot;
}
}
}
}
&lt;/code>&lt;/pre>
&lt;p>This query yields:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;took&amp;quot; : 10,
&amp;quot;timed_out&amp;quot; : false,
&amp;quot;_shards&amp;quot; : {
&amp;quot;total&amp;quot; : 1,
&amp;quot;successful&amp;quot; : 1,
&amp;quot;skipped&amp;quot; : 0,
&amp;quot;failed&amp;quot; : 0
},
&amp;quot;hits&amp;quot; : {
&amp;quot;total&amp;quot; : {
&amp;quot;value&amp;quot; : 1,
&amp;quot;relation&amp;quot; : &amp;quot;eq&amp;quot;
},
&amp;quot;max_score&amp;quot; : 0.26152915,
&amp;quot;hits&amp;quot; : [
{
&amp;quot;_index&amp;quot; : &amp;quot;my-index&amp;quot;,
&amp;quot;_type&amp;quot; : &amp;quot;_doc&amp;quot;,
&amp;quot;_id&amp;quot; : &amp;quot;1&amp;quot;,
&amp;quot;_score&amp;quot; : 0.26152915,
&amp;quot;_source&amp;quot; : {
&amp;quot;query&amp;quot; : {
&amp;quot;match_phrase&amp;quot; : {
&amp;quot;message&amp;quot; : &amp;quot;bonsai tree&amp;quot;
}
},
&amp;quot;subscriber_email&amp;quot; : &amp;quot;subscriber_email@example.com&amp;quot;
},
&amp;quot;fields&amp;quot; : {
&amp;quot;_percolator_document_slot&amp;quot; : [
0
]
},
&amp;quot;highlight&amp;quot; : {
&amp;quot;message&amp;quot; : [
&amp;quot;A new &amp;lt;em&amp;gt;bonsai tree&amp;lt;/em&amp;gt; in the office&amp;quot;
]
}
}
]
}
}
&lt;/code>&lt;/pre>
&lt;p>Now, take the email under the &lt;code>&amp;quot;subscriber_email&amp;quot;&lt;/code> from the response and send an email with the highlight.&lt;/p></description></item><item><title>Using Uberdeps to Build AWS Lambda Uberjar</title><link>https://www.jocas.lt/blog/post/uberdeps-for-aws-lambda/</link><pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/uberdeps-for-aws-lambda/</guid><description>&lt;p>I was writing a Clojure application and the plan was to deploy it as a AWS Lambda. The question I&amp;rsquo;m going to answer in this blog post is: how to build an uberjar for AWS Lambda with &lt;a href="https://github.com/tonsky/uberdeps">Uberdeps&lt;/a>?&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>Add an alias to the &lt;code>deps.edn&lt;/code> for uberjar building:&lt;/p>
&lt;pre>&lt;code>{:aliases {:uberjar
{:extra-deps {uberdeps {:mvn/version &amp;quot;0.1.6&amp;quot;}}
:main-opts [&amp;quot;-m&amp;quot; &amp;quot;uberdeps.uberjar&amp;quot;]}}}
&lt;/code>&lt;/pre>
&lt;p>Create an executable file &lt;code>compile.clj&lt;/code> in the project root folder:&lt;/p>
&lt;pre>&lt;code class="language-bash">touch compile.clj
chmod +x compile.clj
&lt;/code>&lt;/pre>
&lt;p>Put this code in the &lt;code>compile.clj&lt;/code> file:&lt;/p>
&lt;script src="https://gist.github.com/dainiusjocas/e9b154d7a1cbdca8558cd7c5d730d5d0.js">&lt;/script>
&lt;p>Run:&lt;/p>
&lt;pre>&lt;code class="language-bash">(rm -rf classes &amp;amp;&amp;amp; \
mkdir classes &amp;amp;&amp;amp; \
./compile.clj &amp;amp;&amp;amp; \
clojure -A:uberjar --target target/UBERJAR_NAME.jar)
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;d advise put that last script into a &lt;code>Makefile&lt;/code> ;)&lt;/p>
&lt;hr>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>To deploy your Clojure code to AWS Lambda you need to package it as an uberjar. If your project is managed with &lt;code>deps.edn&lt;/code>, basically you&amp;rsquo;re on your own to find a suitable library to package your code.&lt;/p>
&lt;p>For some time to build uberjars for &lt;code>deps.edn&lt;/code> projects I was using &lt;a href="https://github.com/luchiniatwork/cambada">Cambada&lt;/a>. It did the job but I was not entirely happy with the library for a couple of reasons:&lt;/p>
&lt;ul>
&lt;li>the library seems to be no longer maintained;&lt;/li>
&lt;li>it has various &lt;a href="https://github.com/luchiniatwork/cambada/issues">bugs&lt;/a> with transitive Git dependencies. I&amp;rsquo;ve found out that these bugs are fixed in a &lt;a href="https://github.com/xfthhxk/cambada">fork&lt;/a> of the Cambada and I used it as a git dependency.&lt;/li>
&lt;/ul>
&lt;p>Because building an uberjar for &lt;code>deps.edn&lt;/code> boils down to just finding a library there is always temptation to try something new.&lt;/p>
&lt;h2 id="enter-uberdeps">Enter Uberdeps&lt;/h2>
&lt;p>For my toy project I wanted to try out &lt;a href="https://github.com/tonsky/uberdeps">Uberdeps&lt;/a>. The introduction &lt;a href="https://tonsky.me/blog/uberdeps/">blog post&lt;/a> got me interested and I really liked the main idea:&lt;/p>
&lt;blockquote>
&lt;p>Takes deps.edn and packs an uberjar out of it.&lt;/p>
&lt;/blockquote>
&lt;p>Sounds like exactly what I need.&lt;/p>
&lt;h2 id="trouble">Trouble&lt;/h2>
&lt;p>I&amp;rsquo;ve written my application, added all the things needed to deploy it as an AWS Lambda, build an uberjar with Uberdeps, deployed the app with the AWS CloudFormation, but when I&amp;rsquo;ve invoked the Lambda I&amp;rsquo;ve received an error:&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;message&amp;quot; : &amp;quot;Internal server error&amp;quot;
}
&lt;/code>&lt;/pre>
&lt;p>After searching through the AWS CloudWatch logs I&amp;rsquo;ve found:&lt;/p>
&lt;pre>&lt;code>Class not found: my.Lambda: java.lang.ClassNotFoundException
java.lang.ClassNotFoundException: my.Lambda
at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:348)
&lt;/code>&lt;/pre>
&lt;p>The &lt;code>my.Lambda&lt;/code> class was not found.&lt;/p>
&lt;p>After taking a look at the contents of the uberjar I&amp;rsquo;ve noticed that the &lt;code>my.Lambda&lt;/code> class is indeed not inside the Uberjar. Ah, it seems that AOT (Ahead-of-Time) is not done out of the box. After searching and not finding a flag or some parameter that I need to pass to force the AOT compilation in the Uberdeps README, I&amp;rsquo;ve discovered an already closed &lt;a href="https://github.com/tonsky/uberdeps/pull/11">pull request&lt;/a>: the AOT compilation functionality is not implemented.&lt;/p>
&lt;p>I was in trouble.&lt;/p>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;p>The solution was to manually perform AOT compilation of the relevant namespaces right before building an uberjar and then instruct Uberdeps to put the resulting class files into the uberjar.&lt;/p>
&lt;p>To do AOT compilation I&amp;rsquo;ve written a Clojure script &lt;code>compile.clj&lt;/code>:&lt;/p>
&lt;script src="https://gist.github.com/dainiusjocas/e9b154d7a1cbdca8558cd7c5d730d5d0.js">&lt;/script>
&lt;p>Inspiration on how to write the script was taken from &lt;a href="https://www.reddit.com/r/Clojure/comments/8ltsrs/standalone_script_with_clj_including_dependencies/">here&lt;/a> and &lt;a href="https://github.com/tonsky/datascript/blob/master/release.clj">here&lt;/a>.&lt;/p>
&lt;p>To instruct Uberdeps to put class files to the uberjar I&amp;rsquo;ve added &lt;code>classes&lt;/code> directory to the &lt;code>:paths&lt;/code> vector in &lt;code>deps.edn&lt;/code>.&lt;/p>
&lt;p>Just for the convenience, in the Makefile I&amp;rsquo;ve put commands for AOT compilation right before the command to build an uberjar:&lt;/p>
&lt;pre>&lt;code>uberjar:
rm -rf classes
mkdir classes
./compile.clj
clojure -A:uberjar --target target/my-jar-name.jar
&lt;/code>&lt;/pre>
&lt;p>And that is it! I have an uberjar with &lt;code>my.Lambda&lt;/code> class and the AWS Lambda runtime is happy.&lt;/p>
&lt;h2 id="discussion">Discussion&lt;/h2>
&lt;p>The solution is not bullet proof because:&lt;/p>
&lt;ul>
&lt;li>it assumes that the main &lt;code>deps.end&lt;/code> file is called &lt;code>deps.edn&lt;/code>;&lt;/li>
&lt;li>compiled classes are put in the &lt;code>classes&lt;/code> directory;&lt;/li>
&lt;li>the alias for which namespaces should be AOT compiled is the default alias.&lt;/li>
&lt;/ul>
&lt;p>I hope that when a more generic solution will be needed either the Uberdeps will have an option for AOT compilatoin or I&amp;rsquo;ll be clever enough to deal with the situation and write a follow up blog post with the workaround.&lt;/p></description></item><item><title>Using Gitlab CI Cache for Clojure Dependencies</title><link>https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/</link><pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate><guid>https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/</guid><description>&lt;p>I want to share my hard-won lessons on how to setup the Gitlab CI for Clojure projects based on tools.deps. I think that the Gitlab CI is a wonderful tool for CI workloads. But when you&amp;rsquo;re going a bit sideways from the documented ways of doing things you have to do a bit of discovery for yourself.&lt;/p>
&lt;h2 id="gitlab-ci-cachesetup">Gitlab CI CacheÂ Setup&lt;/h2>
&lt;p>Usually I want to cache dependencies between all build and all branches. To achieve this I hard-code the cache key at the root of theÂ &lt;code>.gitlab-ci.yml&lt;/code> file e.g.:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
&lt;/code>&lt;/pre>
&lt;p>When it comes to caching Clojure dependencies we have to be aware that there different types of dependencies. Two most common ones are: Maven and gitlibs.&lt;/p>
&lt;p>The Gitlab CI cache works &lt;strong>only&lt;/strong> with directories &lt;strong>inside the project directory&lt;/strong>. While local repositories (i.e. cache) for Clojure dependencies &lt;strong>by default&lt;/strong> are stored &lt;strong>outside the project directory&lt;/strong> (&lt;code>~/.m2&lt;/code> and &lt;code>~/.gitlibs&lt;/code>). Therefore, we have to provide parameters for our build tool to change the default directories for storing the dependencies.&lt;/p>
&lt;p>To specify Maven local repository we can provideÂ &lt;code>:mvn/local-repo&lt;/code> parameter e.g.:&lt;/p>
&lt;pre>&lt;code class="language-yaml">clojure -Sdeps '{:mvn/local-repo &amp;quot;./.m2/repository&amp;quot;}' -A:test
&lt;/code>&lt;/pre>
&lt;p>Having configured local maven repository in our &lt;code>gitlab-ci.yml&lt;/code> we can specify:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
paths:
- ./.m2/repository
&lt;/code>&lt;/pre>
&lt;p>When it comes to gitlibs there is no public API for changing the default directory in &lt;code>tools.deps&lt;/code>. But the underlying &lt;code>tools.gitlibs&lt;/code> uses an environment variable to set where to store the &lt;a href="https://github.com/clojure/tools.gitlibs/blob/b7acb151b97952409103094794f5fc6f4d7d3840/src/main/clojure/clojure/tools/gitlibs.clj#L23">gitlibs conveniently named &lt;strong>GITLIBS&lt;/strong>&lt;/a>. E.g.&lt;/p>
&lt;pre>&lt;code class="language-bash">$ (export GITLIBS=&amp;quot;.gitlibs/&amp;quot; &amp;amp;&amp;amp; clojure -A:test)
&lt;/code>&lt;/pre>
&lt;p>Of course, we should not forget to configure the cache:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
paths:
- ./.gitlibs
&lt;/code>&lt;/pre>
&lt;p>To use caching for both types of dependencies:&lt;/p>
&lt;pre>&lt;code class="language-bash">(export GITLIBS=&amp;quot;.gitlibs/&amp;quot; &amp;amp;&amp;amp; clojure -Sdeps '{:mvn/local-repo &amp;quot;./.m2/repository&amp;quot;}' -A:test)
&lt;/code>&lt;/pre>
&lt;p>And setup the cache:&lt;/p>
&lt;pre>&lt;code class="language-yaml">cache:
key: one-key-to-rule-them-all
paths:
- ./.m2/repository
- ./.gitlibs
&lt;/code>&lt;/pre>
&lt;p>If you want to disable cache for a particular job (e.g. you&amp;rsquo;re linting with &lt;a href="https://github.com/borkdude/clj-kondo">clj-kondo&lt;/a>, which is delivered as a &lt;a href="https://www.graalvm.org/">GraalVM&lt;/a> compiled &lt;a href="https://www.graalvm.org/docs/reference-manual/native-image/">native image&lt;/a>), just give an empty map for a job&amp;rsquo;s cache setup, e.g.:&lt;/p>
&lt;pre>&lt;code class="language-yaml">lint:
stage: test
image: borkdude/clj-kondo
cache: {}
when: always
script:
- clj-kondo --lint src test
&lt;/code>&lt;/pre>
&lt;p>I&amp;rsquo;ve used the Gitlab CI cache while working on a streaming-text search library &lt;a href="https://github.com/tokenmill/beagle">Beagle&lt;/a>. A fullÂ .gitlab-ci.yml file example of the setup can be found &lt;a href="https://github.com/tokenmill/beagle/blob/master/.gitlab-ci.yml">here&lt;/a>.&lt;/p>
&lt;p>Hope this helps!&lt;/p></description></item></channel></rss>