[{"authors":["admin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://www.jocas.lt/blog/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blog/authors/admin/","section":"authors","summary":"","tags":null,"title":"","type":"authors"},{"authors":["Dainius Jocas"],"categories":["Elasticsearch","Kafka Connect"],"content":"TL;DR When the Elasticsearch indexer is highly concurrent, Kafka record keys are used as Elasticsearch document IDs, and indexer is set to delete records on null values, then Kafka Connect Elasticsearch Sink Connector might corrupt your data: documents that should not be deleted end up being deleted, or documents that should be deleted end up still being present in the index. The fix is to use external versioning for deletes in bulk requests as it is proposed in this Github Pull Request.\nThe problem NOTE: as of version 6.0.0 of the Confluent Platform (last checked on 2020-10-02) the bug that might lead to data corruption is still present.\nLet\u0026rsquo;s focus on a use case where Kafka record key is used as an Elasticsearch document ID1. I would consider this to be a proper practice when the documents represent a catalog of things.\nElasticsearch uses optimistic concurrency control. The job of this concurrency mechanism is to ensure that older version of the document doesn\u0026rsquo;t override a newer version. By default, order of arrival of the operation is applied, but the behaviour can be overriden in several ways depending on the version of Elasticsearch. In this post we focus on concurrent bulk requests, and with a concurrency that involves a network, requests will sometimes arrive out of order.\nTo help Elasticsearch resolve the out-of-order indexing requests Kafka Connect Elasticsearch Sink Connector (from here on Kafka Connect for short) leverages the external document versioning2. Using external versions in Kafka Connect makes sense because we already have versioning in place: Kafka topic partition offsets. If Kafka Connect applies changes to Elasticsearch indices in order of the topic offset, then any update ordering problems would be problems in the upstream system. This is a good guarantee to have.\nLet\u0026rsquo;s add to the mix delete operations. Kafka Connect supports a setting BEHAVIOR_ON_NULL_VALUES_CONFIG to \u0026quot;delete\u0026quot;. This setting instructs the Kafka Connect that a document in Elasticsearch with an ID of the kafka record key with null value (a tombstone message) is going to be deleted. But for some strange reason the deletes does not use external versioning! The line responsible for the described behaviour3 can be found here. This means that for deletes the order-of-arrival wins. Let\u0026rsquo;s increase the concurrency of bulk requests with the param MAX_IN_FLIGHT_REQUESTS_CONFIG to a largish number, and the data consistency problems is just round the corner for data that has some largish update ratio.\nThe issue is even more pronounced when you re-index data into Elasticsearch and you want to do it as fast as possible, which means doing the indexing concurrently.\nThe Example The code that demonstrated the faulty behaviour can be found in this Pull Request.\nThe test case is for testing the case when document should be present in Elasticsearch gets deleted.\nLet\u0026rsquo;s have a little walk over the code snippet:\nCollection\u0026lt;SinkRecord\u0026gt; records = new ArrayList\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; numOfRecords - 1 ; i++) { if (i % 2 == 0) { SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, null, i); records.add(sinkRecord); } else { record.put(\u0026quot;message\u0026quot;, Integer.toString(i)); SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, i); records.add(sinkRecord); } } record.put(\u0026quot;message\u0026quot;, Integer.toString(numOfRecords)); SinkRecord sinkRecord = new SinkRecord(TOPIC, PARTITION, Schema.STRING_SCHEMA, key, schema, record, numOfRecords); records.add(sinkRecord); task.put(records); task.flush(null);  Here we send numOfRecords (which larger than 2) to a Kafka topic. Every second record has null body (delete operation), and the rest of the records have a sequence number as a message value. The very last record is always a non-null record with a message value of numOfRecords.\nLet\u0026rsquo;s setup a connector:\nKEY_IGNORE_CONFIG = \u0026quot;false\u0026quot;; MAX_IN_FLIGHT_REQUESTS_CONFIG = Integer.toString(numOfRecords) BATCH_SIZE_CONFIG = \u0026quot;1\u0026quot; LINGER_MS_CONFIG = \u0026quot;1\u0026quot; BEHAVIOR_ON_NULL_VALUES_CONFIG = \u0026quot;delete\u0026quot;  Here we set a connector to use Kafka record key as id KEY_IGNORE_CONFIG = \u0026quot;false\u0026quot;, set the indexer concurrency to the numOfRecords; set the indexing batch size to 1 (this creates as many requests to Elasticsearch as there are records in the Kafka topic); set indexer to send requests immediately with LINGER_MS_CONFIG = \u0026quot;1\u0026quot;; and record with a null value represents a delete operation.\nWith this setup after the indexing is done we expect that in the index we have a document with ID and whose message value is numOfRecords. But when ordering of bulk requests is out-of-order then at the end we might have a situation where there is no document in the index at all: the bulk index request with message = numOfRecords arrived before one of the bulk requests with a delete operation!\nThe situation might seem to be a bit far-fetched but for applications like e-commerce where you have a catalog that is frequently updated (e.g. the catalog item should be available in search or not) and updates are modelled as document deletes it happens a bit more often than it might be expected.\nThe fix The fix is simple: use the same external versioning that is already being used by the indexing requests also for delete requests:\nif (record.version != null) { req.setParameter(\u0026quot;version_type\u0026quot;, \u0026quot;external\u0026quot;).setParameter(\u0026quot;version\u0026quot;, record.version); }  The full code can be found here. Let\u0026rsquo;s hope that Confluent developers will find some time to merge that PR.\nConclusion Thank you for reading and leave your feedback here.\nP.S. Of course, this is not the only situation when data can get corrupted, e.g. changing the number of partitions; when you delete the topic, repopulate it with up-to-date data (also, you skip deletes) then restarting the indexing might pretty much nothing, because all the versions are earlier external version because offsets are smaller.\n   When Kafka Record keys are not used as Elasticsearch document IDs versioning is not a problem because every Elasticsearch ID is constructed as {topic}+{partition}+{offset} which creates a new document for every Kafka record, i.e. no versioning is needed. \u0026#x21a9;\u0026#xfe0e;\n Elasticsearch 7 supports the external versioning. \u0026#x21a9;\u0026#xfe0e;\n Yes, it is a comment, and it means that the developers were not sure whether to use external versioning for delete operations. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1601683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601683200,"objectID":"98b4170bae1f25c6aeaa33eab0ec010f","permalink":"https://www.jocas.lt/blog/post/kc_es_data_consistency/","publishdate":"2020-10-03T00:00:00Z","relpermalink":"/blog/post/kc_es_data_consistency/","section":"post","summary":"A shout-out about a lurking bug in the Kafka Connect Elasticsearch Sink connector","tags":["Elasticsearch","Kafka Connect"],"title":"How to Prevent Data Corruption in Elasticsearch When Using Kafka Connect Elasticsearch Sink Connector","type":"post"},{"authors":["Dainius Jocas"],"categories":["Elasticsearch","Kafka Connect"],"content":"TL;DR Specify your pipeline with the index.default_pipeline setting in the index (or index template) settings.\nThe Problem We need to index the log data into the Elasticsearch cluster using a Kafka Connect Elasticsearch Sink Connector 1, the data should be split into daily indices, and we need to specify the Elasticsearch ingest pipeline.\nThe documentation of the connector doesn\u0026rsquo;t mention anything about ingest pipelines. After a quick consultation with the Internet you discover that there is an open issue that Kafka Connect Elasticsearch Sink Connector doesn\u0026rsquo;t support specifying an Elasticsearch ingest pipeline. WAT?\nThe Workaround Say2, our pipeline3 just renames an attribute, e.g.:\nPUT _ingest/pipeline/my_pipeline_id { \u0026quot;description\u0026quot; : \u0026quot;renames the field name\u0026quot;, \u0026quot;processors\u0026quot; : [ { \u0026quot;rename\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;original_field_name\u0026quot;, \u0026quot;target_field\u0026quot;: \u0026quot;target_field_name\u0026quot; } } ] }  The Elasticsearch ingest pipeline for indexing can be specified in several ways:\n for each index request as a URL parameter, per bulk index request as a URL parameter, for every bulk index request operation, index settings ( a dynamic attribute), index template.  First three options are not supported by Kafka Connect. The fourth option is not convenient in our case because the data should be split into time-based (e.g. daily) indices and we don\u0026rsquo;t want to do repetitive tasks4. The natural option to follow is to define an index template. In the index template we can specify the index.default_pipeline parameter, e.g.\nPUT _index_template/template_1 { \u0026quot;index_patterns\u0026quot;: [\u0026quot;daily_log*\u0026quot;], \u0026quot;template\u0026quot;: { \u0026quot;settings\u0026quot;: { \u0026quot;index.default_pipeline\u0026quot;: \u0026quot;my_pipeline_id\u0026quot; } } }  Note, that for indexing not to fail, we should create the Elasticsearch ingest pipeline5 before setting up the index template.\nThat is it, now when Kafka Connect will create a new daily index the Elasticsearch ingest pipeline is going to be applied to every document without any issues, for free, and in no time.\nBonus One thing to note is that only one pipeline can be specified for index.default_pipeline which might sound a bit limiting. A clever trick to overcome that limitation is to use a series of pipeline processors that can invoke other pipelines in the specified order, i.e. pipeline of pipelines.\nAlso, there is an index setting called index.final_pipeline that if specified is going to be executed after all other pipelines.\nTesting pipelines can be done using the _simulate API.\nFin Thanks for reading and leave comments or any other feedback on this blog post in the Github issue. Examples were tested to work with Elasticsearch and Kibana 7.8.1.\n  or any other technology that doesn\u0026rsquo;t support, or it is just not possible to specify the Elasticsearch ingest pipeline. \u0026#x21a9;\u0026#xfe0e;\n yes, I know that the same job can be done with the Kafka Connect Transformations. \u0026#x21a9;\u0026#xfe0e;\n let\u0026rsquo;s leave out the Kafka Connector setup. \u0026#x21a9;\u0026#xfe0e;\n set index.default_pipeline=my_pipeline_id for every new daily index with, say, a cron-job at midnight. \u0026#x21a9;\u0026#xfe0e;\n technically, before an index is created that matches the template pattern. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1596326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596326400,"objectID":"06ca0d3a6f9d9bff5e50afb4cab089bd","permalink":"https://www.jocas.lt/blog/post/ingest_pipeline_kafka_connect/","publishdate":"2020-08-02T00:00:00Z","relpermalink":"/blog/post/ingest_pipeline_kafka_connect/","section":"post","summary":"A workaround on how to leverage the Elasticsearch Ingest Pipelines when using Kafka Connect","tags":["Elasticsearch","Kafka Connect"],"title":"How to Use Elasticsearch Ingest Pipelines with Kafka Connect Elasticsearch Sink Connector","type":"post"},{"authors":["Dainius Jocas"],"categories":["Elasticsearch"],"content":"To analyze the textual data Elasticsearch uses analyzers while for the keyword analysis there is a thing called a normalizer. In this article I\u0026rsquo;ll explain what the normalizer is and show it\u0026rsquo;s use case for normalizing URLs.\nTL;DR A neat use case for keyword normalizers is to extract a specific part of the URL with a char_filter of the pattern_replace type.\nIntroduction In Elasticsearch the textual data is represented with two data types: text and keyword. The text type is meant to be used for full-text search use cases while keyword is mean for filtering, sorting, and aggregation.\nTL;DR About Analyzers To make a better use of text data you can setup the analyzer which is a combination of three components:\n exactly one tokenizer, zero or more character filters, zero or more token filters.  Basically, an analyzer transforms a single string into words, e.g. \u0026quot;This is my text\u0026quot; can be transformed into [\u0026quot;this\u0026quot;, \u0026quot;my\u0026quot;, \u0026quot;text\u0026quot;] which you can read as:\n text is split into tokens by tokenizer, each token is lowercased with the a token filter, stopwords are removed with another token filter.  Normalizers The documentation says that:\n Normalizers are similar to analyzers except that they may only emit a single token.\n Normalizers can only be applied to the keyword datatype. The cannonical use case is to lowercase structured content such as IDs, email addresses, e.g. a database stores emails in whatever case but searching for emails should be case insensitive. Note that only a subset of available filters can be used by a normalizer: all filters must work on a per-character basis, i.e. no stopwords or stemmers.\nNormalizers for Normalizing URL Data Storing a URL in a keyword field allows to filter, sort, and aggregate your data per URL. But what if you need to filter, sort, and aggregate by just one part of the URL and you have little to no control over the upstream data source? You have a couple of options:\n convince upstream to extract that one part in their code and send it to you, setup a text field with an analyzer that produces just that one token and enable field data (not a default setup and can get expensive). setup a keyword field with a normalizer with a char_filter. give up.  I want to explore the keyword option. In the next section I\u0026rsquo;ll show how to setup normalizers for Elasticsearch URLs.\nThe not so Synthetic Problem We have a list URLs without a hostname that were used to query Elasticsearch, e.g.: /my_search_index/_search?q=elasticsearch and we need to split URLs into parts such as: index, operation endpoint, e.g.: _search or _count, query filters, etc. In the following example I\u0026rsquo;ll focus on the extracting the index part of the URL.\nLet\u0026rsquo;s create an index:\nPUT elasticsearch_url_index { \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;normalizer\u0026quot;: { \u0026quot;index_extractor_normalizer\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;custom\u0026quot;, \u0026quot;char_filter\u0026quot;: [ \u0026quot;index_name_extractor\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;index_name_extractor\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;pattern_replace\u0026quot;, \u0026quot;pattern\u0026quot;: \u0026quot;/(.+)/.*\u0026quot;, \u0026quot;replacement\u0026quot;: \u0026quot;$1\u0026quot; } } } } }, \u0026quot;mappings\u0026quot; : { \u0026quot;properties\u0026quot;: { \u0026quot;url\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;fields\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;normalizer\u0026quot;: \u0026quot;index_extractor_normalizer\u0026quot; } } } } } }  Here we setup the index with a normalizer index_extractor_normalizer that has a char filter index_name_extractor that uses a regex pattern_replace to extract characters between the first and the second slashes. The mappings have a property url which is of the keyword type and have a field index which is also of the keyword type and is set up to use the normalizer index_extractor_normalizer.\nSince the normalizer is basically a collection of filters we can use our good old friend _analyze API to test how it works.\nPOST elasticsearch_url_index/_analyze { \u0026quot;char_filter\u0026quot;: [\u0026quot;index_name_extractor\u0026quot;], \u0026quot;text\u0026quot;: [\u0026quot;/my_search_index/_search?q=elasticsearch\u0026quot;] }  Produces:\n{ \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;my_search_index\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 40, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 0 } ] }  Good, exactly as we wanted: /my_search_index/_search?q=elasticsearch =\u0026gt; my_search_index.\nLet\u0026rsquo;s index some data:\nPUT elasticsearch_url_index/_doc/0 { \u0026quot;url\u0026quot;: \u0026quot;/my_search_index/_search?q=elasticsearch\u0026quot; }  Let\u0026rsquo;s try to filter URLs by the index name:\nGET elasticsearch_url_index/_search?q=url:my_search_index  Produces:\n{ \u0026quot;took\u0026quot; : 0, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No results? What? Oh! Wrong field: url was used instead of url.index. Let\u0026rsquo;s try once again:\nGET elasticsearch_url_index/_search?q=url.index:my_search_index  Produces:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.2876821, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;elasticsearch_url_index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;0\u0026quot;, \u0026quot;_score\u0026quot; : 0.2876821, \u0026quot;_source\u0026quot; : { \u0026quot;url\u0026quot; : \u0026quot;/my_search_index/_search?q=elasticsearch\u0026quot; } } ] } }  As expected. Cool.\nBonus: a Trick with the docvalue_fields Another neat trick is that we can get out the index part of the URL from an Elasticsearch index using the docvalue_fields option in a request ,e.g.:\nGET elasticsearch_url_index/_search?q=url.index:my_search_index { \u0026quot;docvalue_fields\u0026quot;: [\u0026quot;url.index\u0026quot;] }  Produces:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.2876821, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;elasticsearch_url_index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;0\u0026quot;, \u0026quot;_score\u0026quot; : 0.2876821, \u0026quot;_source\u0026quot; : { \u0026quot;url\u0026quot; : \u0026quot;/my_search_index/_search?q=elasticsearch\u0026quot; }, \u0026quot;fields\u0026quot; : { \u0026quot;url.index\u0026quot; : [ \u0026quot;my_search_index\u0026quot; ] } } ] } }  The important part is this one:\n\u0026quot;fields\u0026quot; : { \u0026quot;url.index\u0026quot; : [ \u0026quot;my_search_index\u0026quot; ] }  A neat thing about the docvalue_fields is that in the example above the my_search_index value is not comming from the _source of the document. This means that we can use keywords and by extension normalized keywords to fetch an exact value from the Elasticsearch index and not necessarily the one that was sent to Elasticsearch which somewhat solves our dependency from the upstream systems.\nNotes The setup is done in the Kibana Dev Tools with the Elasticsearch 7.7.0.\nThe pattern \u0026quot;/(.+)/.*\u0026quot; is a bit simplified purely for presentation purposes and doesn\u0026rsquo;t work as expected for URLs with more than 2 slashes, e.g.: /index/type/_search would produce index/type. You need something a bit more involved like \u0026quot;/([^/]+)/.*\u0026quot;.\nFin That is all I wanted to show you today. Hope it might be useful/interesting to someone down the line. Leave comments on the Github issue here. Cheers!\n","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"997b41d22469f11e9b603503b55089b9","permalink":"https://www.jocas.lt/blog/post/elasticsearch-normlizers/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/blog/post/elasticsearch-normlizers/","section":"post","summary":"In this article I'll explain what the normalizer is and show it's use case for **normalizing** URLs.","tags":["Elasticsearch"],"title":"A Neat Trick with Elasticsearch Normalizers","type":"post"},{"authors":["Dainius Jocas"],"categories":["AWS Lambda","babashka","Clojure","GraalVM"],"content":"TL;DR\nI\u0026rsquo;ve managed to package a simple babashka script to an AWS Lambda Custom Runtime. Here is the code, try for yourself.\nMotivation Wouldn\u0026rsquo;t it be great to deploy little Clojure code snippets to Custom Lambda Runtime? The main benefits would be:\n you would not suffer from java cold-start problems; you wouldn\u0026rsquo;t need to compile your project with GraalVM native-image tool which is time consuming and for anything more advanced is not likely to work anyway; babashka supports scripting with a subset of Clojure, which might do the work for you.  The plan I know what it takes to deploy to Lambda Custom Runtime. Last year I\u0026rsquo;ve created a Clojure project template for deploying GraalVM compiled AWS Lambda Custom Runtime. And babashka is just another self contained binary. It should be too hard to bring two things together and get it working? Challenge accepted.\nPackaging I like to build software inside Docker containers. In this experiment, for the first attempt I\u0026rsquo;ve used this Dockerfile:\nFROM borkdude/babashka:latest as BABASHKA FROM clojure:tools-deps-alpine as BUILDER RUN apk add --no-cache zip WORKDIR /var/task COPY --from=BABASHKA /usr/local/bin/bb bb ENV GITLIBS=\u0026quot;.gitlibs/\u0026quot; COPY lambda/bootstrap bootstrap COPY deps.edn deps.edn RUN clojure -Sdeps '{:mvn/local-repo \u0026quot;./.m2/repository\u0026quot;}' -Spath \u0026gt; cp COPY src/ src/ COPY resources/ resources/ RUN zip -q -r function.zip bb cp bootstrap .gitlibs/ .m2/ src/ resources/ deps.edn  Here:\n copy bb binary from babashka Docker image, download the dependencies for babashka script using clojure (both, maven and git dependencies are supported, like is described here), write a classpath to the cp file, copy all source code, zip the required contents to the function.zip.  Every line of this dockerfile is packed with details but I\u0026rsquo;ll leave it for the future posts.\nI\u0026rsquo;ve packaged all dependencies for lambda into function.zip. The contents of the archive are:\n bb: babashka binary bootstrap: AWS Lambda entry point script cp: generated classpath text file deps.edn .gitlibs: directory with gitlibs .m2: directory with Maven dependencies resources: src: directory with babashka scripts  Custom runtime discoveries Finally, having all dependencies packaged up, I\u0026rsquo;ve deployed the function.zip to AWS Lambda. The first error message was not very encouraging:\nUtil_sun_misc_Signal.ensureInitialized: CSunMiscSignal.create() failed. errno: 38 Function not implemented Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed. JavaFrameAnchor dump: No anchors TopFrame info: TotalFrameSize in CodeInfoTable 32 VMThreads info: VMThread 0000000003042750 STATUS_IN_JAVA (safepoints disabled) java.lang.Thread@0x264fa98 VM Thread State for current thread 0000000003042750: 0 (8 bytes): com.oracle.svm.jni.JNIThreadLocalEnvironment.jniFunctions = (bytes) 0000000003042750: 0000000002293a88 8 (32 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.regularTLAB = (bytes) 0000000003042758: 00007f7809500000 00007f7809600000 0000000003042768: 00007f7809507160 0000000000000000 40 (8 bytes): com.oracle.svm.core.heap.NoAllocationVerifier.openVerifiers = (Object) null 48 (8 bytes): com.oracle.svm.core.jdk.IdentityHashCodeSupport.hashCodeGeneratorTL = (Object) null 56 (8 bytes): com.oracle.svm.core.snippets.SnippetRuntime.currentException = (Object) null 64 (8 bytes): com.oracle.svm.core.thread.JavaThreads.currentThread = (Object) java.lang.Thread 000000000264fa98 72 (8 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.activeTimer = (Object) null 80 (8 bytes): com.oracle.svm.jni.JNIObjectHandles.handles = (Object) com.oracle.svm.core.handles.ThreadLocalHandles 00007f7809501558 88 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPendingException.pendingException = (Object) null 96 (8 bytes): com.oracle.svm.jni.JNIThreadLocalPinnedObjects.pinnedObjectsListHead = (Object) null 104 (8 bytes): com.oracle.svm.jni.JNIThreadOwnedMonitors.ownedMonitors = (Object) null 112 (8 bytes): com.oracle.svm.core.genscavenge.ThreadLocalAllocation.freeList = (Word) 0 0000000000000000 120 (8 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.stackBoundaryTL = (Word) 1 0000000000000001 128 (8 bytes): com.oracle.svm.core.stack.JavaFrameAnchors.lastAnchor = (Word) 0 0000000000000000 136 (8 bytes): com.oracle.svm.core.thread.VMThreads.IsolateTL = (Word) 25636864 0000000001873000 144 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadHandleTL = (Word) 50477184 0000000003023880 152 (8 bytes): com.oracle.svm.core.thread.VMThreads.OSThreadIdTL = (Word) 50477184 0000000003023880 160 (8 bytes): com.oracle.svm.core.thread.VMThreads.nextTL = (Word) 0 0000000000000000 168 (4 bytes): com.oracle.svm.core.graal.snippets.StackOverflowCheckImpl.yellowZoneStateTL = (int) -16843010 fefefefe 172 (4 bytes): com.oracle.svm.core.snippets.ImplicitExceptions.implicitExceptionsAreFatal = (int) 0 00000000 176 (4 bytes): com.oracle.svm.core.thread.Safepoint.safepointRequested = (int) 2147473200 7fffd730 180 (4 bytes): com.oracle.svm.core.thread.ThreadingSupportImpl.currentPauseDepth = (int) 0 00000000 184 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.safepointsDisabledTL = (int) 1 00000001 188 (4 bytes): com.oracle.svm.core.thread.VMThreads$StatusSupport.statusTL = (int) 1 00000001 VMOperation dump: No VMOperation in progress Dump Counters: Raw Stacktrace: 00007ffeb8e0a940: 000000000186e776 000000000207b9d0 00007ffeb8e0a950: 0000000001873000 000000000085b37c 00007ffeb8e0a960: 000000000084540a 00000000008454ca 00007ffeb8e0a970: 000000000264f128 000000000264ef58 00007ffeb8e0a980: 00007f78095018d8 0000000002650640 00007ffeb8e0a990: 000000000264f128 0000002602650c18 00007ffeb8e0a9a0: 0000000000845444 00007ffeb8e0a970 00007ffeb8e0a9b0: 0000000000000000 0000000000845f6e 00007ffeb8e0a9c0: 0000000002650e18 0000000002650c18 00007ffeb8e0a9d0: 0000000002650e18 0000000002070c60 00007ffeb8e0a9e0: 00000000021f48f8 00000000012b77e6 00007ffeb8e0a9f0: 0000000002650e18 0000000002650c18 00007ffeb8e0aa00: 0000001000000000 0000000002070c60 00007ffeb8e0aa10: 00007f7809507138 0000000000477f69 00007ffeb8e0aa20: 00007f7809503b88 00007f7809501910 00007ffeb8e0aa30: 00007f7809507138 00000000004831b4 00007ffeb8e0aa40: 0000000000000010 000000000085d16d 00007ffeb8e0aa50: 000000000000003b 00000000008b4bdb 00007ffeb8e0aa60: 000000000291e970 00007f7809504828 00007ffeb8e0aa70: 0000000100000007 0000000001079a70 00007ffeb8e0aa80: 00007f78095070b8 00007f7809507080 00007ffeb8e0aa90: 0000000001873000 000000000291e970 00007ffeb8e0aaa0: 00007f7809506f78 00007f78095070b8 00007ffeb8e0aab0: 0000000000000008 0000000000000010 00007ffeb8e0aac0: 0000000000000010 00000000008144a1 00007ffeb8e0aad0: 0000000000000007 0000000000cd7c2e 00007ffeb8e0aae0: 00007f7809504938 0000000001873000 00007ffeb8e0aaf0: 0000000002205088 00007f78095070b8 00007ffeb8e0ab00: 00007f7809507080 0000000cc0001000 00007ffeb8e0ab10: 0000000000000000 0000000000cd73eb 00007ffeb8e0ab20: 00007f7809503b58 00007f78095070b8 00007ffeb8e0ab30: 00007f7809507080 00007f78095038e0 00007ffeb8e0ab40: 00007f7807c8e388 000000000205e900 00007ffeb8e0ab50: 00007f7809501350 000000240000000c 00007ffeb8e0ab60: 000000000000000c 00007f78095038e0 00007ffeb8e0ab70: d15c483b00000000 00000000004830e5 00007ffeb8e0ab80: 0000000000000007 00007f78095038e0 00007ffeb8e0ab90: 00007f78095038e0 00000000006f2b33 00007ffeb8e0aba0: 000000000205e900 0000000002070448 00007ffeb8e0abb0: 00007f78095070b8 0000000000cd8b3d 00007ffeb8e0abc0: 00000000020864c8 0000000000cbffc1 00007ffeb8e0abd0: 0000000002070448 00007f78095070b8 00007ffeb8e0abe0: 0000000c00000000 00007f7809505ef8 00007ffeb8e0abf0: 00007f78095070d8 00007f7809504840 00007ffeb8e0ac00: 7cab467402070d98 0000000000fbfc08 00007ffeb8e0ac10: 0000000002634470 00007f7809507020 00007ffeb8e0ac20: 0000000001873000 00007f78095070d8 00007ffeb8e0ac30: 00007f7809504840 0000000000cc187e 00007ffeb8e0ac40: 0000000000000000 0000000000000000 00007ffeb8e0ac50: 00007f7807c91840 00007f7809504840 00007ffeb8e0ac60: 0000000002070d98 0000000000cc17b9 00007ffeb8e0ac70: 0000000000c848f0 00007f78095038e0 00007ffeb8e0ac80: 0000000002b33a78 0000000100cc4f83 00007ffeb8e0ac90: 0000000000483140 00000000004b5713 00007ffeb8e0aca0: 0000000002070d98 0000000000cdae9a 00007ffeb8e0acb0: 000000000209a600 00007f78095038e0 00007ffeb8e0acc0: 0000000002b33a78 000000000047c576 00007ffeb8e0acd0: 000000000209a600 000000000209a630 00007ffeb8e0ace0: 0000000002a1b8d8 0000000002a1b408 00007ffeb8e0acf0: 000000000209a600 00000000017acc23 00007ffeb8e0ad00: 0000000000000001 0000000000001000 00007ffeb8e0ad10: 0000000000000000 0000000000000000 00007ffeb8e0ad20: 0000000000000000 0000000000000000 00007ffeb8e0ad30: 0000000000000000 0000000000000000 Stacktrace Stage0: RSP 00007ffeb8e0a940 RIP 000000000085b3f6 FrameSize 32 RSP 00007ffeb8e0a960 RIP 000000000085b37c FrameSize 16 RSP 00007ffeb8e0a970 RIP 00000000008454ca FrameSize 80 RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e FrameSize 48 RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 FrameSize 48 RSP 00007ffeb8e0aa20 RIP 0000000000477f69 FrameSize 32 RSP 00007ffeb8e0aa40 RIP 00000000004831b4 FrameSize 320 RSP 00007ffeb8e0ab80 RIP 00000000004830e5 FrameSize 32 RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 FrameSize 256 RSP 00007ffeb8e0aca0 RIP 00000000004b5713 FrameSize 48 RSP 00007ffeb8e0acd0 RIP 000000000047c576 FrameSize 160 RSP 00007ffeb8e0ad70 RIP 000000000047c285 FrameSize 32 RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 FrameSize 256 RSP 00007ffeb8e0ae90 RIP 000000000048f162 FrameSize 32 RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c FrameSize 1 Stacktrace Stage1: RSP 00007ffeb8e0a940 RIP 000000000085b3f6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a960 RIP 000000000085b37c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a970 RIP 00000000008454ca com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aa20 RIP 0000000000477f69 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aa40 RIP 00000000004831b4 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ab80 RIP 00000000004830e5 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aca0 RIP 00000000004b5713 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0acd0 RIP 000000000047c576 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ad70 RIP 000000000047c285 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0ae90 RIP 000000000048f162 com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c com.oracle.svm.core.code.CodeInfo@0x2618c70 name = image code Full Stacktrace: RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.jdk.VMErrorSubstitutions.shutdown(VMErrorSubstitutions.java:111) RSP 00007ffeb8e0a940 RIP 000000000085b3f6 [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:74) RSP 00007ffeb8e0a960 RIP 000000000085b37c [image code] com.oracle.svm.core.util.VMError.shouldNotReachHere(VMError.java:59) RSP 00007ffeb8e0a970 RIP 00000000008454ca [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.ensureInitialized(SunMiscSubstitutions.java:176) RSP 00007ffeb8e0a9c0 RIP 0000000000845f6e [image code] com.oracle.svm.core.posix.Util_jdk_internal_misc_Signal.numberFromName(SunMiscSubstitutions.java:223) RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.findSignal(Signal.java:78) RSP 00007ffeb8e0a9f0 RIP 00000000012b77e6 [image code] sun.misc.Signal.\u0026lt;init\u0026gt;(Signal.java:140) RSP 00007ffeb8e0aa20 RIP 0000000000477f69 [image code] babashka.impl.pipe_signal_handler$handle_pipe_BANG_.invokeStatic(pipe_signal_handler.clj:11) RSP 00007ffeb8e0aa40 RIP 00000000004831b4 [image code] babashka.main$main.invokeStatic(main.clj:282) RSP 00007ffeb8e0ab80 RIP 00000000004830e5 [image code] babashka.main$main.doInvoke(main.clj:282) RSP 00007ffeb8e0aba0 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137) RSP 00007ffeb8e0aca0 RIP 00000000004b5713 [image code] clojure.core$apply.invokeStatic(core.clj:665) RSP 00007ffeb8e0acd0 RIP 000000000047c576 [image code] babashka.main$_main.invokeStatic(main.clj:442) RSP 00007ffeb8e0ad70 RIP 000000000047c285 [image code] babashka.main$_main.doInvoke(main.clj:437) RSP 00007ffeb8e0ad90 RIP 00000000006f2b33 [image code] clojure.lang.RestFn.applyTo(RestFn.java:137) RSP 00007ffeb8e0ae90 RIP 000000000048f162 [image code] babashka.main.main(Unknown Source) RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.runCore(JavaMainWrapper.java:151) RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.JavaMainWrapper.run(JavaMainWrapper.java:186) RSP 00007ffeb8e0aeb0 RIP 00000000007fb05c [image code] com.oracle.svm.core.code.IsolateEnterStub.JavaMainWrapper_run_5087f5482cc9a6abc971913ece43acb471d2631b(IsolateEnterStub.java:0) [Native image heap boundaries: ReadOnly Primitives: 0x1873008 .. 0x206f048 ReadOnly References: 0x206ff78 .. 0x24fc9f8 Writable Primitives: 0x24fd000 .. 0x26343e0 Writable References: 0x2634470 .. 0x2ba42c0] [Heap: [Young generation: [youngSpace: aligned: 0/0 unaligned: 0/0]] [Old generation: [fromSpace: aligned: 0/0 unaligned: 0/0] [toSpace: aligned: 0/0 unaligned: 0/0] ] [Unused: aligned: 0/0]] Fatal error: Util_sun_misc_Signal.ensureInitialized: CSunMiscSignal.open() failed. RequestId: 263ff1be-425d-4dcb-9ea5-67020dc3041b Error: Runtime exited with error: exit status 99 Runtime.ExitError  The fight After some Googling I\u0026rsquo;ve discovered several related clues here and here. They say that signals are not supported in AWS lambda. So, why not to disable signals for babashka and see what happens? I\u0026rsquo;ve forked the repo, made a flag that disables PIPE signal handling, deployed babashka to the docker hub and tried to deploy lambda once again.\nAnd? It worked:\nmake function-name=$(make get-function-name) invoke-function =\u0026gt; {\u0026quot;test\u0026quot;:\u0026quot;test914\u0026quot;}{ \u0026quot;StatusCode\u0026quot;: 200, \u0026quot;ExecutedVersion\u0026quot;: \u0026quot;$LATEST\u0026quot; }  Summary  Here is the example of babashka script that can be deployed to AWS Lambda.\n The function.zip weights just 18MB. The cold startup of the Lambda that is given 128MB of RAM is ~400ms. Subsequent calls ranges from 4ms and 120ms. The more RAM you give the faster lambda gets. I can develop the code in Cursive as the structure is like of an ordinary Clojure deps.edn project (and it can be used on the JVM). I made a PR to babashka and I\u0026rsquo;ve got accepted.  Next Steps  Fix Problem building on macos (/tmp dir is not writable). Get rid of AWS CloudFormation part. Work a bit more to support AWS API Gateway. Create a template for such projects.  ","date":1584748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584748800,"objectID":"2d26caeb6ab5a7424a8d00ad7870fe3d","permalink":"https://www.jocas.lt/blog/post/babashka-aws-lambda/","publishdate":"2020-03-21T00:00:00Z","relpermalink":"/blog/post/babashka-aws-lambda/","section":"post","summary":"Adventures with babashka and AWS Lambda.","tags":["AWS Lambda","babashka","clojure","GraalVM"],"title":"Deploy babashka script to AWS Lambda","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch"],"content":"I want to take a look at Search Templates for Elasticsearch. Let\u0026rsquo;s apply them to examples from previous post on Synonym Graphs.\nSetup I\u0026rsquo;m using Elasticsearch 7.5.1.\nIndex configuration:\nDELETE test_index-1 PUT /test_index-1 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;descrition\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;entity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;lowercase_keyword_analyzer\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;synonym_graph_analyzer\u0026quot; } } }, \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;synonym_graph_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_synonym_graph\u0026quot; ] }, \u0026quot;lowercase_keyword_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot; ], \u0026quot;char_filter\u0026quot;: [ \u0026quot;spaces_to_undescores_filter\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;spaces_to_undescores_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mapping\u0026quot;, \u0026quot;mappings\u0026quot;: [ \u0026quot; \\\\u0020 =\u0026gt; _\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;my_synonym_graph\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym_graph\u0026quot;, \u0026quot;lenient\u0026quot;: true, \u0026quot;synonyms\u0026quot;: [ \u0026quot;very important thing =\u0026gt; very_important_thing\u0026quot; ] } } } } } }  Index the document:\nPUT test_index-1/_doc/1 { \u0026quot;description\u0026quot;: \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot;: \u0026quot;Very Important Thing\u0026quot; }  Search queries:\n prefix very important thing suffix  Templates I\u0026rsquo;m very interested in one particular use of the search templates: how flexible is the management of stored seach templates? Can I update a search template while receiving queries?\nAdd a template:\nPOST _scripts/synonym-graph-search { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;mustache\u0026quot;, \u0026quot;source\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;{{query_string}}\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } } } }  Try to run the search:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: \u0026quot;suffix very important thing prefix\u0026quot; } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.5753642, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.5753642, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Exactly as expected.\nWhen using a stored search template the Elasticsearch client doesn\u0026rsquo;t need to handle the complex query construction.\nTemplates are updateable Let\u0026rsquo;s try to update the template with a higher boost value:\nPOST _scripts/synonym-graph-search { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;mustache\u0026quot;, \u0026quot;source\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;{{query_string}}\u0026quot;, \u0026quot;boost\u0026quot;: 5 } } } } } }  Works.\nNow let\u0026rsquo;s run the same query:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: \u0026quot;suffix very important thing prefix\u0026quot; } }  This yields:\n{ \u0026quot;took\u0026quot; : 4, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.4384103, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.4384103, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  The scores are 0.5753642 and 1.4384103 that is ~2/5. Cool! This means that without changing (and redeploying) the Elasticsearch client we can change the querying logic, making the query an more dynamic.\nCorner Cases What if we run query has more attributes, e.g.:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: \u0026quot;suffix very important thing prefix\u0026quot;, \u0026quot;new_attr\u0026quot;: \u0026quot;123\u0026quot; } }  Works as expected!\nWhen query_string is null:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: null } }  Works!\nWhat if the param is not provided:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;new_attr\u0026quot;: \u0026quot;value\u0026quot; } }  No error!\nWhat if we provide a list instead of a string:\nGET test_index-1/_search/template { \u0026quot;id\u0026quot;: \u0026quot;synonym-graph-search\u0026quot;, \u0026quot;params\u0026quot;: { \u0026quot;query_string\u0026quot;: [\u0026quot;this\u0026quot;, \u0026quot;Very Important Thing\u0026quot;] } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.4384103, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.4384103, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Instead of profiding one value we can replace it with a list. Good!\nMetadata of the search template It would be great to be able to store some metadata with the search template script, e.g. Git commit SHA of the query. I couldn\u0026rsquo;t find a way to do this. A workaround might be to _name attribute of the query. E.g.:\nPOST _scripts/synonym-graph-search { \u0026quot;script\u0026quot;: { \u0026quot;lang\u0026quot;: \u0026quot;mustache\u0026quot;, \u0026quot;source\u0026quot;: { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;_name\u0026quot;: \u0026quot;GIT COMMIT SHA\u0026quot;, \u0026quot;query\u0026quot;: \u0026quot;{{query_string}}\u0026quot;, \u0026quot;boost\u0026quot;: 5 } } } } } }  The response:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.4384103, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.4384103, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; }, \u0026quot;matched_queries\u0026quot; : [ \u0026quot;GIT COMMIT SHA\u0026quot; ] } ] } }  Not great but might be useful.\nDiscussion  Templates doesn\u0026rsquo;t support search index specification. Field names can be parameterized, this feature alows to start/stop using a new/old field. Search template can be tested in (even in production cluster) independently. We can run our query against multiple search templates. Combine this with the Profile API and performance can be compared. Explain API also is supported.  ","date":1577059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577059200,"objectID":"dae7e72b85246f090d3d6b15c05e133d","permalink":"https://www.jocas.lt/blog/post/on-search-templates/","publishdate":"2019-12-23T00:00:00Z","relpermalink":"/blog/post/on-search-templates/","section":"post","summary":"A couple of examples and notes on using Elasticsearch search templates","tags":["elasticsearch"],"title":"Using Search Templates in Elasticsearch","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch"],"content":"I\u0026rsquo;ve written that if you google for How can you match a long query text to a short text field? you\u0026rsquo;re advised to use Elasticsearch Percolator. Today I\u0026rsquo;ll show an alternative way of solving the same problem with Elasticsearch.\nThe main idea is to use Synonym Graph Token Filter with some data preparation.\nProblem Statement Say that we learned how extract some entity from free form text with techniques such as NER, dictionary annotations, or some fancy Machine Learning. And when this entity is mentioned in the search query we want to boost documents that mention this entity. Also, say you\u0026rsquo;ve ruled out using Elasticsearch Percolator because it increases network latency because it requires additional call to Elasticsearch.\nFor further discussion our unstructured text is going to be This description is about a Very Important Thing and something else. and the extracted entity Very Important Thing. Our test document looks like :\n{ \u0026quot;description\u0026quot;: \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot;: \u0026quot;Very Important Thing\u0026quot; }  Search queries:\n prefix very important thing suffix prefix very important another thing suffix prefix thing suffix  All examples are tested on Elasticsearch 7.5.1.\nNaive Setup Let\u0026rsquo;s create an index for our documents:\nPUT /test_index-2 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;description\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;entity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; } } } }  Entity field is of type text because we want it to be searchable. keyword type won\u0026rsquo;t work because it does only exact matches and out query most likely will be longer than our entity string.\nIndex our document:\nPUT test_index-2/_doc/1 { \u0026quot;description\u0026quot;: \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot;: \u0026quot;Very Important Thing\u0026quot; }  Search the index with the query that mentions our very important thing:\nGET test_index-2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.7260926, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-2\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.7260926, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Cool, we found what we we looking for.\nLet\u0026rsquo;s try another query, this time with a mention of very important another thing:\nGET test_index-2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important another thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 1.7260926, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-2\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 1.7260926, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Oh, the results are the same as with the previous query despite the fact that we mention Another Thing here. But it still might be OK because we matched all the terms of the entity.\nLet\u0026rsquo;s try another query:\nGET test_index-2/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.5753642, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-2\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.5753642, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  Oh no, we still matched our Very Important Thing while only thing term is present in the query. But at least this time the score is lower than with previous twoqueries, 0.5753642 vs. 1.7260926. Here we clearly see the problem: we are matching short strings with long strings and partial matches raises problems.\nProposed Solution Let\u0026rsquo;s leverage Synonym Graph Token Filter to solve our problem.\nPUT /test_index-1 { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;descrition\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot; }, \u0026quot;entity\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;lowercase_keyword_analyzer\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;synonym_graph_analyzer\u0026quot; } } }, \u0026quot;settings\u0026quot;: { \u0026quot;index\u0026quot;: { \u0026quot;analysis\u0026quot;: { \u0026quot;analyzer\u0026quot;: { \u0026quot;synonym_graph_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot;, \u0026quot;my_synonym_graph\u0026quot; ] }, \u0026quot;lowercase_keyword_analyzer\u0026quot;: { \u0026quot;tokenizer\u0026quot;: \u0026quot;keyword\u0026quot;, \u0026quot;filter\u0026quot;: [ \u0026quot;lowercase\u0026quot; ], \u0026quot;char_filter\u0026quot;: [ \u0026quot;spaces_to_undescores_filter\u0026quot; ] } }, \u0026quot;char_filter\u0026quot;: { \u0026quot;spaces_to_undescores_filter\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;mapping\u0026quot;, \u0026quot;mappings\u0026quot;: [ \u0026quot; \\\\u0020 =\u0026gt; _\u0026quot; ] } }, \u0026quot;filter\u0026quot;: { \u0026quot;my_synonym_graph\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;synonym_graph\u0026quot;, \u0026quot;lenient\u0026quot;: true, \u0026quot;synonyms\u0026quot;: [ \u0026quot;very important thing =\u0026gt; very_important_thing\u0026quot; ] } } } } } }  Let\u0026rsquo;s decompose this large index configuration piece by piece:\n The entity attribute now has separate analyzers for both index and search phases. The lowercase_keyword_analyzer uses keyword tokenizer which means that tokenization will result in the sequence of token of size 1, then it normalizes tokens by lowercasing them and finally spaces_to_undescores_filter, replaces spaces to underscores. E.g. a string \u0026quot;Very Important Thing\u0026quot; is transformed into list of tokens [\u0026quot;very_important_thing\u0026quot;]. Or use out friend _analyze API:  POST test_index-1/_analyze { \u0026quot;text\u0026quot;: [\u0026quot;Very Important Thing\u0026quot;], \u0026quot;analyzer\u0026quot;: \u0026quot;lowercase_keyword_analyzer\u0026quot; }  This yields:\n{ \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;very_important_thing\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 20, \u0026quot;type\u0026quot; : \u0026quot;word\u0026quot;, \u0026quot;position\u0026quot; : 0 } ] }  The synonym_graph_analyzer use standard tokenizer, which is followed by the lowercase filter, and then the my_synonym_graph token filter is applied. We\u0026rsquo;ve set up one synonym \u0026quot;very important thing =\u0026gt; very_important_thing\u0026quot;. E.g.  POST test_index-1/_analyze { \u0026quot;text\u0026quot;: [\u0026quot;prefix very important thing suffix\u0026quot;], \u0026quot;analyzer\u0026quot;: \u0026quot;synonym_graph_analyzer\u0026quot; }  This yields:\n{ \u0026quot;tokens\u0026quot; : [ { \u0026quot;token\u0026quot; : \u0026quot;prefix\u0026quot;, \u0026quot;start_offset\u0026quot; : 0, \u0026quot;end_offset\u0026quot; : 6, \u0026quot;type\u0026quot; : \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot; : 0 }, { \u0026quot;token\u0026quot; : \u0026quot;very_important_thing\u0026quot;, \u0026quot;start_offset\u0026quot; : 7, \u0026quot;end_offset\u0026quot; : 27, \u0026quot;type\u0026quot; : \u0026quot;SYNONYM\u0026quot;, \u0026quot;position\u0026quot; : 1 }, { \u0026quot;token\u0026quot; : \u0026quot;suffix\u0026quot;, \u0026quot;start_offset\u0026quot; : 28, \u0026quot;end_offset\u0026quot; : 34, \u0026quot;type\u0026quot; : \u0026quot;\u0026lt;ALPHANUM\u0026gt;\u0026quot;, \u0026quot;position\u0026quot; : 2 } ] }  After analysis we have 3 tokens [\u0026quot;prefix\u0026quot;, \u0026quot;very_important_thing\u0026quot;, \u0026quot;suffix\u0026quot;]. Notice \u0026quot;very_important_thing\u0026quot; token: this is equal to the right-hand-side from our synonym definitions. Now let\u0026rsquo;s run queries from the previous section:\nGET test_index-1/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.5753642, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;test_index-1\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.5753642, \u0026quot;_source\u0026quot; : { \u0026quot;description\u0026quot; : \u0026quot;This description is about a Very Important Thing and something else.\u0026quot;, \u0026quot;entity\u0026quot; : \u0026quot;Very Important Thing\u0026quot; } } ] } }  As expected: exact match -\u0026gt; hit.\nAnother query:\nGET test_index-1/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix very important another thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 0, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No hits! Good! The document is not going to be boosted despite the fact that all tokens match.\nAnd the last one:\nGET test_index-1/_search { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;entity\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;prefix thing suffix\u0026quot;, \u0026quot;boost\u0026quot;: 2 } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No hits! Good. This means that also substring doesn\u0026rsquo;t match.\nDiscussion Synonym Graph Token Filter can \u0026ldquo;replace\u0026rdquo; a sequence of tokens (e.g. a phrase) with another sequence of tokens. In this particular example: many tokens were replaced with one token.\n One field can have only one analyzer pair for index and search phases. If we want another analysis pipeline for the entity attribute we have to create another field with the analyzers specified, e.g. stemmed phrase with lower boost. The synonym list must be prepared before the index creation. Management of the synonym list might complicate index management, e.g. you use templates for your index management. The overal solution in general might look a bit too complicated.  ","date":1576972800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576972800,"objectID":"bbf1ba4712ba08644677c25dbacd8b0b","permalink":"https://www.jocas.lt/blog/post/synonym-graph-phrase-search/","publishdate":"2019-12-22T00:00:00Z","relpermalink":"/blog/post/synonym-graph-phrase-search/","section":"post","summary":"An idea on how to search for short string with a long query string","tags":["elasticsearch"],"title":"Phrase Search with Synonym Graph Token Filter in Elasticsearch","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch","percolator"],"content":"This time I need to percolate texts with different analyzers for index and search analyzers.\nLet\u0026rsquo;s elaborate a bit on previous article and explicitly declare analyzers to use.\nDefine index:\nPUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } }  Then define 2 slightly different percolator queries (notice the difference between \u0026quot;bonsai tree\u0026quot; and \u0026quot;bonsai, tree\u0026quot;).\nPUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai tree\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot; } } } } PUT /my-index/_doc/2?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai, tree\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot; } } } }  Let\u0026rsquo;s percolate:\nGET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 80, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 2, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;2\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai, tree\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;standard\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } }, { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai tree\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;standard\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  As expected: 2 documents matched.\nBut now lets change the analyzer of the second percolation query to whitespace:\nPUT /my-index/_doc/2?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai, tree\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;whitespace\u0026quot; } } } }  Run the percolator:\n GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 5, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai tree\u0026quot;, \u0026quot;analyzer\u0026quot; : \u0026quot;standard\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  As expected: only 1 percolator query matched our input.\nPhrases with Stopwords Say, we have a phrase \u0026quot;bonsai is tree\u0026quot; and we percolate text A new bonsai in tree in the office with the standard analyzer for indexing and english for search analyzer. There should be no matches. Let\u0026rsquo;s try:\nDELETE my-index PUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;english\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } } PUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai is tree\u0026quot; } } } } GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai in tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  And, surprisingly, this yields:\n{ \u0026quot;took\u0026quot; : 2, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai is tree\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] } } ] } }  We have a match! Also notice that the highlighter is broken!\nThe problem that these two analyzers have different stopword lists (no stopwords for standard and several English stopwords for english analyzer) and the phrase contains a stopword that is not shared between analyzers.\nLet\u0026rsquo;s fix this surprise with search_quote_analyzer.\nDELETE my-index PUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;english\u0026quot;, \u0026quot;search_quote_analyzer\u0026quot;: \u0026quot;standard\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } } PUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;bonsai is tree\u0026quot; } } } } GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai in tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 1, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 0, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : null, \u0026quot;hits\u0026quot; : [ ] } }  No hits, as expected.\nLet\u0026rsquo;s check if the expected behaviour is still there:\nGET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai is tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This yields:\n{ \u0026quot;took\u0026quot; : 4, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.39229375, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.39229375, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : { \u0026quot;query\u0026quot; : \u0026quot;bonsai is tree\u0026quot; } } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai is tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  Good. Even the highlighting works.\n","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"e3f59a2e678c8500ecbac9f71472142b","permalink":"https://www.jocas.lt/blog/post/percolator-phrase-analyzers/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/blog/post/percolator-phrase-analyzers/","section":"post","summary":"Example on how to use analyzers with the","tags":["elasticsearch","percolator"],"title":"Elasticsearch Percolator and Text Analyzers","type":"post"},{"authors":["Dainius Jocas"],"categories":["elasticsearch","percolator"],"content":"If you google How can you match a long query text to a short text field? it will point you to the Stack Overflow page or here where the answer is to use Elasticsearch Percolator.\nMy search items are phrases meaning that it should match all terms in order. Let\u0026rsquo;s create a sample setup in Kibana (v7.5) Dev dashboard.\n Create an index for percolation:  PUT /my-index { \u0026quot;mappings\u0026quot;: { \u0026quot;properties\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot; }, \u0026quot;query\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;percolator\u0026quot; } } } }  Note on \u0026quot;term_vector\u0026quot;: \u0026quot;with_positions_offsets\u0026quot;: this allows Fast Vector Highlighter to highlight combined phrase not just separate qeury terms.\nStore one phrase query:  PUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;bonsai tree\u0026quot; } } }  Percolate a document:  GET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  Note on \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot;: this instructs Elasticsearch to use the Fast Vector Highlighter.\nThe query yields:\n{ \u0026quot;took\u0026quot; : 23, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : \u0026quot;bonsai tree\u0026quot; } } }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  As we see highlighter correctly marker the search phrase.\nStoring additional data with percolator queries Percolation result can be used to connect pieces of information in your system, e.g. store a subscriber_email attribute of the user that wants to be notified when the query matches along with the percolator query.\nPUT /my-index/_doc/1?refresh { \u0026quot;query\u0026quot;: { \u0026quot;match_phrase\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;bonsai tree\u0026quot; } }, \u0026quot;subscriber_email\u0026quot;: \u0026quot;subscriber_email@example.com\u0026quot; }  Then query:\nGET /my-index/_search? { \u0026quot;query\u0026quot;: { \u0026quot;percolate\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;query\u0026quot;, \u0026quot;document\u0026quot;: { \u0026quot;message\u0026quot;: \u0026quot;A new bonsai tree in the office\u0026quot; } } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot;: { \u0026quot;message\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;fvh\u0026quot; } } } }  This query yields:\n{ \u0026quot;took\u0026quot; : 10, \u0026quot;timed_out\u0026quot; : false, \u0026quot;_shards\u0026quot; : { \u0026quot;total\u0026quot; : 1, \u0026quot;successful\u0026quot; : 1, \u0026quot;skipped\u0026quot; : 0, \u0026quot;failed\u0026quot; : 0 }, \u0026quot;hits\u0026quot; : { \u0026quot;total\u0026quot; : { \u0026quot;value\u0026quot; : 1, \u0026quot;relation\u0026quot; : \u0026quot;eq\u0026quot; }, \u0026quot;max_score\u0026quot; : 0.26152915, \u0026quot;hits\u0026quot; : [ { \u0026quot;_index\u0026quot; : \u0026quot;my-index\u0026quot;, \u0026quot;_type\u0026quot; : \u0026quot;_doc\u0026quot;, \u0026quot;_id\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot; : 0.26152915, \u0026quot;_source\u0026quot; : { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;message\u0026quot; : \u0026quot;bonsai tree\u0026quot; } }, \u0026quot;subscriber_email\u0026quot; : \u0026quot;subscriber_email@example.com\u0026quot; }, \u0026quot;fields\u0026quot; : { \u0026quot;_percolator_document_slot\u0026quot; : [ 0 ] }, \u0026quot;highlight\u0026quot; : { \u0026quot;message\u0026quot; : [ \u0026quot;A new \u0026lt;em\u0026gt;bonsai tree\u0026lt;/em\u0026gt; in the office\u0026quot; ] } } ] } }  Now, take the email under the \u0026quot;subscriber_email\u0026quot; from the response and send an email with the highlight.\n","date":1576627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576627200,"objectID":"8e98378fbfbaafa1f97ba3a46fd35817","permalink":"https://www.jocas.lt/blog/post/es-percolator-phrase-highlight/","publishdate":"2019-12-18T00:00:00Z","relpermalink":"/blog/post/es-percolator-phrase-highlight/","section":"post","summary":"Investigating the Elasticsearch percolator.","tags":["elasticsearch","search"],"title":"Phrase Highlighting with the Elasticsearch Percolator","type":"post"},{"authors":["Dainius Jocas"],"categories":["clojure","aws","lambda","devops"],"content":"I was writing a Clojure application and the plan was to deploy it as a AWS Lambda. The question I\u0026rsquo;m going to answer in this blog post is: how to build an uberjar for AWS Lambda with Uberdeps?\nTL;DR Add an alias to the deps.edn for uberjar building:\n{:aliases {:uberjar {:extra-deps {uberdeps {:mvn/version \u0026quot;0.1.6\u0026quot;}} :main-opts [\u0026quot;-m\u0026quot; \u0026quot;uberdeps.uberjar\u0026quot;]}}}  Create an executable file compile.clj in the project root folder:\ntouch compile.clj chmod +x compile.clj  Put this code in the compile.clj file:\n Run:\n(rm -rf classes \u0026amp;\u0026amp; \\ mkdir classes \u0026amp;\u0026amp; \\ ./compile.clj \u0026amp;\u0026amp; \\ clojure -A:uberjar --target target/UBERJAR_NAME.jar)  I\u0026rsquo;d advise put that last script into a Makefile ;)\n Introduction To deploy your Clojure code to AWS Lambda you need to package it as an uberjar. If your project is managed with deps.edn, basically you\u0026rsquo;re on your own to find a suitable library to package your code.\nFor some time to build uberjars for deps.edn projects I was using Cambada. It did the job but I was not entirely happy with the library for a couple of reasons:\n the library seems to be no longer maintained; it has various bugs with transitive Git dependencies. I\u0026rsquo;ve found out that these bugs are fixed in a fork of the Cambada and I used it as a git dependency.  Because building an uberjar for deps.edn boils down to just finding a library there is always temptation to try something new.\nEnter Uberdeps For my toy project I wanted to try out Uberdeps. The introduction blog post got me interested and I really liked the main idea:\n Takes deps.edn and packs an uberjar out of it.\n Sounds like exactly what I need.\nTrouble I\u0026rsquo;ve written my application, added all the things needed to deploy it as an AWS Lambda, build an uberjar with Uberdeps, deployed the app with the AWS CloudFormation, but when I\u0026rsquo;ve invoked the Lambda I\u0026rsquo;ve received an error:\n{ \u0026quot;message\u0026quot; : \u0026quot;Internal server error\u0026quot; }  After searching through the AWS CloudWatch logs I\u0026rsquo;ve found:\nClass not found: my.Lambda: java.lang.ClassNotFoundException java.lang.ClassNotFoundException: my.Lambda at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348)  The my.Lambda class was not found.\nAfter taking a look at the contents of the uberjar I\u0026rsquo;ve noticed that the my.Lambda class is indeed not inside the Uberjar. Ah, it seems that AOT (Ahead-of-Time) is not done out of the box. After searching and not finding a flag or some parameter that I need to pass to force the AOT compilation in the Uberdeps README, I\u0026rsquo;ve discovered an already closed pull request: the AOT compilation functionality is not implemented.\nI was in trouble.\nSolution The solution was to manually perform AOT compilation of the relevant namespaces right before building an uberjar and then instruct Uberdeps to put the resulting class files into the uberjar.\nTo do AOT compilation I\u0026rsquo;ve written a Clojure script compile.clj:\n Inspiration on how to write the script was taken from here and here.\nTo instruct Uberdeps to put class files to the uberjar I\u0026rsquo;ve added classes directory to the :paths vector in deps.edn.\nJust for the convenience, in the Makefile I\u0026rsquo;ve put commands for AOT compilation right before the command to build an uberjar:\nuberjar: rm -rf classes mkdir classes ./compile.clj clojure -A:uberjar --target target/my-jar-name.jar  And that is it! I have an uberjar with my.Lambda class and the AWS Lambda runtime is happy.\nDiscussion The solution is not bullet proof because:\n it assumes that the main deps.end file is called deps.edn; compiled classes are put in the classes directory; the alias for which namespaces should be AOT compiled is the default alias.  I hope that when a more generic solution will be needed either the Uberdeps will have an option for AOT compilatoin or I\u0026rsquo;ll be clever enough to deal with the situation and write a follow up blog post with the workaround.\n","date":1573776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573776000,"objectID":"bc08dd6e0a5cfada2a44f4a636a8db0f","permalink":"https://www.jocas.lt/blog/post/uberdeps-for-aws-lambda/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/blog/post/uberdeps-for-aws-lambda/","section":"post","summary":"A guide on how to build an Uberjar for AWS Lambda with `tonsky/uberdeps`","tags":["clojure","devops","aws","lambda"],"title":"Using Uberdeps to Build AWS Lambda Uberjar","type":"post"},{"authors":["Dainius Jocas"],"categories":["clojure","gitlab","ci","devops"],"content":"I want to share my hard-won lessons on how to setup the Gitlab CI for Clojure projects based on tools.deps. I think that the Gitlab CI is a wonderful tool for CI workloads. But when you\u0026rsquo;re going a bit sideways from the documented ways of doing things you have to do a bit of discovery for yourself.\nGitlab CI Cache Setup Usually I want to cache dependencies between all build and all branches. To achieve this I hard-code the cache key at the root of the .gitlab-ci.yml file e.g.:\ncache: key: one-key-to-rule-them-all  When it comes to caching Clojure dependencies we have to be aware that there different types of dependencies. Two most common ones are: Maven and gitlibs.\nThe Gitlab CI cache works only with directories inside the project directory. While local repositories (i.e. cache) for Clojure dependencies by default are stored outside the project directory (~/.m2 and ~/.gitlibs). Therefore, we have to provide parameters for our build tool to change the default directories for storing the dependencies.\nTo specify Maven local repository we can provide :mvn/local-repo parameter e.g.:\nclojure -Sdeps '{:mvn/local-repo \u0026quot;./.m2/repository\u0026quot;}' -A:test  Having configured local maven repository in our gitlab-ci.yml we can specify:\ncache: key: one-key-to-rule-them-all paths: - ./.m2/repository  When it comes to gitlibs there is no public API for changing the default directory in tools.deps. But the underlying tools.gitlibs uses an environment variable to set where to store the gitlibs conveniently named GITLIBS. E.g.\n$ (export GITLIBS=\u0026quot;.gitlibs/\u0026quot; \u0026amp;\u0026amp; clojure -A:test)  Of course, we should not forget to configure the cache:\ncache: key: one-key-to-rule-them-all paths: - ./.gitlibs  To use caching for both types of dependencies:\n(export GITLIBS=\u0026quot;.gitlibs/\u0026quot; \u0026amp;\u0026amp; clojure -Sdeps '{:mvn/local-repo \u0026quot;./.m2/repository\u0026quot;}' -A:test)  And setup the cache:\ncache: key: one-key-to-rule-them-all paths: - ./.m2/repository - ./.gitlibs  If you want to disable cache for a particular job (e.g. you\u0026rsquo;re linting with clj-kondo, which is delivered as a GraalVM compiled native image), just give an empty map for a job\u0026rsquo;s cache setup, e.g.:\nlint: stage: test image: borkdude/clj-kondo cache: {} when: always script: - clj-kondo --lint src test  I\u0026rsquo;ve used the Gitlab CI cache while working on a streaming-text search library Beagle. A full .gitlab-ci.yml file example of the setup can be found here.\nHope this helps!\n","date":1573430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573430400,"objectID":"10dfc9aba13fb40f8ec0be74224e4205","permalink":"https://www.jocas.lt/blog/post/gitlab-ci-clojure-dependencies/","publishdate":"2019-11-11T00:00:00Z","relpermalink":"/blog/post/gitlab-ci-clojure-dependencies/","section":"post","summary":"A guide on how to use Gitlab CI Cache for Clojure Dependencies","tags":["clojure","devops"],"title":"Using Gitlab CI Cache for Clojure Dependencies","type":"post"},{"authors":["Dainius Jocas"],"categories":null,"content":"  Clojure workflow @ TokenMill by Dainius Jocas  The source code of the demo project can be found here.\n","date":1559242800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559242800,"objectID":"86663215daf06a1fb32613517d99226f","permalink":"https://www.jocas.lt/blog/talk/vilnius-clojure-meetup/","publishdate":"2019-11-11T19:00:00Z","relpermalink":"/blog/talk/vilnius-clojure-meetup/","section":"talk","summary":"Sharing Joy of Clojure Programming","tags":["meetup","clojure","lambda","aws","graalvm"],"title":"Clojure Workflow @ TokenMill","type":"talk"}]